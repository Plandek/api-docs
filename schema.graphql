schema {
  query: Query
  mutation: Mutation
}

directive @cacheControl(maxAge: Int, scope: CacheControlScope) on OBJECT | FIELD_DEFINITION

directive @notNormalised on OBJECT

directive @queryMethodFor(metricGroups: [String!]!) on FIELD_DEFINITION

type AverageNumberDeltaSummary @notNormalised {
  """identification of the summary (breakdown's bucket key, or _general_)"""
  key: String!
  """optional label to show in the UI for this Summary bucket (blank for _general_)"""
  label: String!
  """delta value of the summary"""
  value: DeltaValue!
  """delta of value taken into account (numerator to calculate the average)"""
  numerator: DeltaValue!
  """delta of number of items taken into account in the summary (denominator to calculate the average)"""
  countItems: DeltaValue!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query) for the present response"""
  overrideFilters: OverrideFilters!
}

type AverageNumberSummary @notNormalised {
  """identification of the summary (breakdown's bucket key, or _general_)"""
  key: String!
  """label to show in the UI for this Summary bucket (blank for _general_)"""
  label: String!
  """value of the summary (average -> numerator / countItems)"""
  value: Float!
  """value taken into account (numerator to calculate the average)"""
  numerator: Float!
  """optionally return the ids of the items of countItems, useful for the Detail."""
  itemIDs: [KeyString!]
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

"""represents a single point of a TimeSeries with a single numeric value"""
type AverageNumberTimePoint implements TimePoint @notNormalised {
  """date-time of the point"""
  x: JSDate!
  """date-time of the initial moment of the time range represented by this point"""
  fromDate: UTCStartOfDay!
  """date-time of the final moment of the time range represented by this point"""
  toDate: UTCEndOfDay!
  """value of the point (average -> numerator / countItems)"""
  y: Float!
  """value taken into account (numerator to calculate the average)"""
  numerator: Float!
  """optionally return the ids of the items of countItems, useful for the Detail."""
  itemIDs: [KeyString!]
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

type AverageNumberTimeSeries implements TimeSeries @notNormalised {
  """identification of the Time Series"""
  key: String!
  """optional label to show in the UI for this Time Series (it should override any regular translation-based label)"""
  label: String!
  """sorted list of points"""
  data: [AverageNumberTimePoint!]!
}

enum AverageType {
  AVERAGE
  ROLLING_AVERAGE
}

"""aka Project"""
type Board {
  """clientKey--boardKey"""
  id: ID!
  """client_key that this board belongs to"""
  clientKey: KeyString!
  """(aka project_key)"""
  boardKey: KeyString!
  name: String!
  status: BoardStatus!
}

enum BoardStatus {
  NOT_READY
  ONBOARDING
  CONFIGURED
  READY
}

"""(TYPE) common type to represent a MultiFilter (key -> array of values)"""
type BoolFilter @notNormalised {
  """
  key (same in FilterConfig.key) of the filter to apply"
  """
  key: KeyString!
  """value of the flag"""
  value: Boolean!
}

"""(INPUT) common type to represent a BoolFilter (key -> bool value)"""
input BoolFilterInput {
  """key (same in FilterConfig.key) of the filter to apply"""
  key: KeyString!
  """value of the flag"""
  value: Boolean!
}

"""common type to represent a BoolFilter with labels"""
type BoolFilterWithLabels @notNormalised {
  """key (same in FilterConfig.key) of the filter to apply"""
  key: KeyString!
  """label (same in FilterConfig.label) of the filter to apply"""
  label: String!
  """value type (same in FilterConfig.valueType) of the filter to apply"""
  valueType: FilterValueType!
  """filter type (same in FilterConfig.filterType) of the filter to apply"""
  filterType: FilterType!
  """value with label"""
  valueLabel: BoolLabel!
}

type BoolLabel @notNormalised {
  """value of the flag (true or false)"""
  value: Boolean!
  """how to print this in the UI when the value is true/false (depends on flag)"""
  label: String!
}

type BreakdownInfo @notNormalised {
  key: KeyString!
  label: String!
}

enum CacheControlScope {
  PUBLIC
  PRIVATE
}

union Card = TextCard | MetricCard

input CardDashboardSettingsInput {
  breakdown: KeyString
  chartType: KeyString
  chartTab: ChartTab!
}

input CardFiltersInput {
  entitySetRefinement: EntitySetInput!
  multiFilters: [MultiFilterInput!]!
  boolFilters: [BoolFilterInput!]!
  rangeFilters: [RangeFilterInput!]!
}

type CardList {
  id: ID!
  """hasMany"""
  cards: [Card!]!
}

enum CardSizeType {
  SMALL
  MEDIUM
  LARGE
  XLARGE
}

enum ChartTab {
  DEFAULT
  SCATTER
  SUM
  TIME
}

type CiBuildDetail {
  id: ID!
  """determines the field used to calculate `value`"""
  valueConfigKey: KeyString!
  """value to be used"""
  value: Float
  clientKey: KeyString!
  buildId: String!
  project: String!
  projectUrl: String
  branch: String
  commit: KeyString
  status: String!
  buildStartedAt: UTCDate!
  buildStoppedAt: UTCDate
  durationSeconds: Float
  credits: Float
  jobs: [CiJobDetail!]!
  workflowUrl: String
  workflowName: String
  tag: String
  timeSinceLastFailureSeconds: Float
  timeSinceLastSuccessSeconds: Float
  outageDurationSeconds: Float
  outageStartedAt: UTCDate
  outageStoppedAt: UTCDate
  outageSuccessfulBuild: String
  outageFailedBuilds: [CiFailedBuildDetail!]!
}

type CiBuildDetailList {
  id: ID!
  """total quantity of details"""
  count: Int!
  """list of item details, all sharing the same valueConfig to calculate the `value` field"""
  details: [CiBuildDetail!]!
  """key of the valueConfig"""
  valueConfigKey: KeyString!
  """valueConfig used for the `value` field of each detail"""
  valueConfig: ValueConfig!
}

enum CiBuildValueField {
  durationSeconds
  outageDurationSeconds
}

input CiBuildsDetailInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: CiMetric!
}

type CiFailedBuildDetail {
  id: ID!
  clientKey: KeyString!
  buildId: ID!
  status: String!
  buildStartedAt: UTCDate!
  buildStoppedAt: UTCDate!
  durationSeconds: Float!
}

type CiJobDetail {
  id: ID!
  name: String!
  type: String!
  status: String!
  jobNumber: String!
  startedAt: UTCDate
  stoppedAt: UTCDate
  creditsUsed: Int!
  jobUrl: String
  durationSeconds: Float!
}

"""list of available ci metrics with a single value"""
enum CiMetric {
  """
  Percentage of builds whose status is not "success"
  """
  BUILD_FAILURE_RATE
  """Build Count"""
  BUILD_COUNT
  """Total Build Cost"""
  TOTAL_BUILD_COST
  """Mean Build Cost"""
  MEAN_BUILD_COST
  """Mean build duration in seconds (average of project build time)"""
  MEAN_BUILD_TIME_BY_PROJECT
  """Mean time between build failures (summary value is an average of project values)"""
  MEAN_TIME_BETWEEN_BUILD_FAILURES_BY_PROJECT
  """Mean time between build failures and first successful build (summary value is an average of project values)"""
  MEAN_TIME_TO_RECOVER_BUILD_FAILURES_BY_PROJECT
  """Mean time between PR creation and closure (or current time)"""
  MEAN_PULL_REQUEST_TIME_TO_RESOLVE_BY_REPO
}

input CiMetricFacetsInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: CiMetric!
}

input CiMetricInput {
  metricKey: CiMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  """granularity for the time series (default to `day` if not specified)"""
  granularity: Granularity
  """
  can be null, to ask for the general result.
  
  If it is not null, it has to be one of the keys of the valid FilterConfig (see FilterConfig)
  
  note that this cannot be an enum because it will depend on the data (custom fields et al)
  """
  breakdown: KeyString
}

type CiMetricInputType @notNormalised {
  metricKey: CiMetric!
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  breakdown: KeyString
}

input CiMetricInsightInput {
  metricKey: CiMetric!
  filters: FiltersInput!
}

type CiMetricInsightResponse {
  id: ID!
  summary: MetricInsightSummary
  driversList: [MetricInsightDriver!]!
}

type CiMetricResponse {
  id: ID!
  metricKey: CiMetric!
  input: CiMetricInputType!
  """summary with breakdown blank (key = _general_)"""
  generalSummary: NumberSummary
  """delta of the generalSummary"""
  generalDelta: NumberDeltaSummary
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one summary per breakdown bucket
  """
  summariesList: [NumberSummary!]!
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one delta summary per breakdown bucket
  if there are no deltas configured, it will be NULL
  """
  deltasList: [NumberDeltaSummary!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value, depending on `metric`
  """
  timeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates. The value of each point will be the cumulative average.
  
  e.g.
  
  ```
  TS0---TS1---TS2---TS3---TS4
  
  ATS0 = TS0
  ATS1 = (TS1 + TS0) / 2
  ATS2 = (TS2 + TS1 + TS0) / 3
  ATS3 = (TS3 + TS2 + TS1 + TS0) / 4
  ATS4 = (TS4 + TS3 + TS2 + TS1 + TS0) / 5
  ```
  """
  averageTimeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates.
  The value of each point will be the cumulative average taking into account only the current value + the previous `n` values (where n is the windowSize)
  It will load the TS of the past filters (same filters used for delta calculation) to get the "previous values" of the initial points
  
  e.g time series with 5 points (TS0 to TS4), windowSize = 2
  
  ```
  PTS0---PTS1---PTS2---PTS3---PTS4---TS0---TS1---TS2---TS3---TS4
  
  RATS0 = (TS0 + PTS4 + PTS3) / 3
  RATS1 = (TS1 + TS0 + PTS4) / 3
  RATS2 = (TS2 + TS1 + TS0) / 3
  RATS3 = (TS3 + TS2 + TS1) / 3
  RATS4 = (TS4 + TS3 + TS2) / 3
  ```
  """
  rollingAverageTimeSeriesList(windowSize: Int, windowSizePercentage: Float): [NumberTimeSeriesWithWindowSize!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value taken from each CiBuildDetail
  The `size` will be the number of builds that the point represents
  The `singleItem` is populated only when size is 1. It contains key, label, link
  """
  scatterTimeSeriesList: [ScatterTimeSeries!]!
}

type CiPipeline {
  id: ID!
  clientKey: KeyString!
  pipelineUri: ResourceURI!
  name: String!
}

"""
Different types of entities, used for the Config for labels and allowed entities.

The elements of this enum must be compatible with the `label_entity_set` in ClientService.
"""
enum ClassicEntities {
  project
  person
  programme
  team
}

type Client {
  """same as clientKey"""
  id: ID!
  clientKey: KeyString!
  name: String!
  status: ClientStatus!
  toggles: ClientFeatureToggles!
}

"""For the given client and the given user, this models the default DataSet to use in the UI as landing page. If it is null, an index page will be shown instead."""
type ClientDefaultDataSet {
  """clientkey---userid"""
  id: String!
  """user id, part of PK"""
  userId: String!
  """clientKey, part of PK"""
  clientKey: String!
  """if present, dataSetKey of the default DataSet"""
  dataSetKey: String
  """if present, the default DataSet"""
  dataSet: DataSet
}

type ClientFeatureToggles @notNormalised {
  withSidebarFocusList: Boolean!
  disablePeopleBreakdown: Boolean!
  disablePeopleFilter: Boolean!
  metricSprintValuePerTeamSize: Boolean!
  sprintDateRangeField: SprintDateRangeField!
}

type ClientMetricConfig {
  id: ID!
  enabled: Boolean!
  metricKey: MetricKey!
  clientKey: KeyString!
  filterConfigs: [FilterConfig!]!
  facetConfigs: [FilterConfig!]!
  breakdownConfigs: [FilterConfig!]!
  defaultBreakdownConfigForDetails: FilterConfig
  valueConfigs: [ValueConfig!]!
  defaultValueConfig: ValueConfig!
  canConfigureValue: Boolean!
  supportsOutliers: Boolean!
  dataSetEntities: [EntityKey!]!
}

enum ClientStatus {
  NOT_READY
  ONBOARDING
  CONFIGURED
  READY
}

input CodeCycleTimeByStatusInput {
  metricKey: CodeCycleTimeMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  breakdown: KeyString!
}

type CodeCycleTimeByStatusInputType @notNormalised {
  metricKey: CodeCycleTimeMetric!
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  breakdown: KeyString!
}

type CodeCycleTimeByStatusResponse {
  id: ID!
  metricKey: CodeCycleTimeMetric!
  input: CodeCycleTimeByStatusInputType!
  """summary with breakdown blank (key = _general_)"""
  generalSummary: AverageNumberSummary
  """delta of the generalSummary"""
  generalDelta: AverageNumberDeltaSummary
  """
  one summary per status
  
  NOTE: value is the AVG of HOURS spent in the filtered stages for all the filtered tickets
  (stages include Deployment if it is one of the filtered ones or if we are not filtering by any
  """
  summariesList: [PRStatusSummary!]!
  """one delta summary per status"""
  deltasList: [PRStatusDeltaSummary!]!
  """
  one time series per status
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value, depending on `metric`
  """
  timeSeriesList: [PullRequestStatusTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates. The value of each point will be the cumulative average.
  
  e.g.
  
  ```
  TS0---TS1---TS2---TS3---TS4
  
  ATS0 = TS0
  ATS1 = (TS1 + TS0) / 2
  ATS2 = (TS2 + TS1 + TS0) / 3
  ATS3 = (TS3 + TS2 + TS1 + TS0) / 4
  ATS4 = (TS4 + TS3 + TS2 + TS1 + TS0) / 5
  ```
  """
  averageTimeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates.
  The value of each point will be the cumulative average taking into account only the current value + the previous `n` values (where n is the windowSize)
  It will load the TS of the past filters (same filters used for delta calculation) to get the "previous values" of the initial points
  
  e.g time series with 5 points (TS0 to TS4), windowSize = 2
  
  ```
  PTS0---PTS1---PTS2---PTS3---PTS4---TS0---TS1---TS2---TS3---TS4
  
  RATS0 = (TS0 + PTS4 + PTS3) / 3
  RATS1 = (TS1 + TS0 + PTS4) / 3
  RATS2 = (TS2 + TS1 + TS0) / 3
  RATS3 = (TS3 + TS2 + TS1) / 3
  RATS4 = (TS4 + TS3 + TS2) / 3
  ```
  """
  rollingAverageTimeSeriesList(windowSize: Int, windowSizePercentage: Float): [NumberTimeSeriesWithWindowSize!]!
}

input CodeCycleTimeDetailInput {
  metricKey: CodeCycleTimeMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  """optionally state which PRs you want as a subset"""
  prIDs: [KeyString!]
}

input CodeCycleTimeFacetsInput {
  metricKey: CodeCycleTimeMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
}

input CodeCycleTimeInput {
  metricKey: CodeCycleTimeMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  breakdown: KeyString
}

type CodeCycleTimeInputType @notNormalised {
  metricKey: CodeCycleTimeMetric!
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  breakdown: KeyString
}

enum CodeCycleTimeMetric {
  """code cycle time avg by hours based on original statuses"""
  CODE_CYCLE_TIME
}

type CodeCycleTimeResponse {
  id: ID!
  metricKey: CodeCycleTimeMetric!
  input: CodeCycleTimeInputType!
  """same as codeCycleTimeMetric.generalSummary"""
  generalSummary: AverageNumberSummary
  """same as codeCycleTimeMetric.generalDelta"""
  generalDelta: AverageNumberDeltaSummary
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one summary per breakdown bucket
  
  NOTE: value is the AVG of HOURS spent in the filtered stages for all the filtered tickets
  (stages include Deployment if it is one of the filtered ones or if we are not filtering by any
  """
  summariesList: [AverageNumberSummary!]!
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one delta summary per breakdown bucket
  if there are no deltas configured, it will be NULL
  """
  deltasList: [AverageNumberDeltaSummary!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value, depending on `metric`
  """
  timeSeriesList: [AverageNumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates. The value of each point will be the cumulative average.
  
  e.g.
  
  ```
  TS0---TS1---TS2---TS3---TS4
  
  ATS0 = TS0
  ATS1 = (TS1 + TS0) / 2
  ATS2 = (TS2 + TS1 + TS0) / 3
  ATS3 = (TS3 + TS2 + TS1 + TS0) / 4
  ATS4 = (TS4 + TS3 + TS2 + TS1 + TS0) / 5
  ```
  """
  averageTimeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates.
  The value of each point will be the cumulative average taking into account only the current value + the previous `n` values (where n is the windowSize)
  It will load the TS of the past filters (same filters used for delta calculation) to get the "previous values" of the initial points
  
  e.g time series with 5 points (TS0 to TS4), windowSize = 2
  
  ```
  PTS0---PTS1---PTS2---PTS3---PTS4---TS0---TS1---TS2---TS3---TS4
  
  RATS0 = (TS0 + PTS4 + PTS3) / 3
  RATS1 = (TS1 + TS0 + PTS4) / 3
  RATS2 = (TS2 + TS1 + TS0) / 3
  RATS3 = (TS3 + TS2 + TS1) / 3
  RATS4 = (TS4 + TS3 + TS2) / 3
  ```
  """
  rollingAverageTimeSeriesList(windowSize: Int, windowSizePercentage: Float): [NumberTimeSeriesWithWindowSize!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the duration in hours
  The `size` will be the number of tickets that the point represents
  The `singleItem` is populated only when size is 1. It contains key, label, link
  """
  scatterTimeSeriesList: [PullRequestWithKeysNumberTimeSeries!]!
}

enum CodeKnowledgeMetric {
  """Knowledge Graph Visualisation for PR reviews"""
  CODE_KNOWLEDGE
}

input CodeKnowledgeMetricDetailsInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: CodeKnowledgeMetric!
}

input CodeKnowledgeMetricFacetsInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: CodeKnowledgeMetric!
}

input CodeKnowledgeMetricInput {
  metricKey: CodeKnowledgeMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
}

type CodeKnowledgeMetricInputType @notNormalised {
  metricKey: CodeKnowledgeMetric!
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
}

type CodeKnowledgeMetricResponse {
  id: ID!
  input: CodeKnowledgeMetricInputType!
  metricKey: CodeKnowledgeMetric!
  """
  provide a list of PR's authors with their different interactions
  I.E:
  Sigmund Freud -> EGO with 6 PRs
  Carl Jung -> Robert Aziz, with 9 PRs
  """
  summariesList: [PersonRelationshipSummary!]!
  """filter config to use for filtering a specific Knowledge relationship (FROM person -> TO person)"""
  peoplePairFilterConfig: FilterConfig!
}

type CommitDetail {
  """system-wide ID for the commit detail"""
  id: ID!
  """determines the field used to calculate `value`"""
  valueConfigKey: KeyString!
  """value to be used"""
  value: Float
  """hit ID"""
  hitID: ID!
  """list of board ids asociated to the commit"""
  boardIds: [KeyString!]!
  """commit sha name"""
  commitHash: KeyString!
  """person ID who committed"""
  committerPersonId: ID
  """person who committed"""
  committer: Person
  """number of inserted lines"""
  insertedLines: Int!
  """number of deleted lines"""
  deletedLines: Int!
  """number of changed files"""
  filesChanged: Int!
  """list of tickets associated to the commit"""
  matchedTicketKeys: [KeyString!]!
  """number of tickets associated with the commit"""
  matchedTicketKeysCount: Int!
  """UTC date when commit occured"""
  time: UTCDate!
  """link to the commit url"""
  commitUrl: String
  """the default branch the repo is using (Default master)"""
  inDefaultBranch: Boolean!
  """if the commit seen in the pull requests"""
  seenInPullRequest: Boolean!
  """repository the commit belongs to"""
  repository: Repository
  """repository name the commit belongs to"""
  repoName: String
  """Client key associated to the commit"""
  clientKey: KeyString!
}

type CommitDetailList {
  id: ID!
  """total quantity of details"""
  count: Int!
  """list of item details, all sharing the same valueConfig to calculate the `value` field"""
  details: [CommitDetail!]!
  """key of the valueConfig"""
  valueConfigKey: KeyString!
  """valueConfig used for the `value` field of each detail"""
  valueConfig: ValueConfig!
}

enum CommitDetailValueField {
  insertions
  deletions
  filesChanged
}

enum CommitPercentageMetric {
  """percentage of the commits without a pull request (100 * commitsWithoutPR / allCommits). Value is percentage of commits count"""
  COMMIT_WITHOUT_PULL_REQUEST_PERCENTAGE_COUNT
  """percentage of the commits without ticket reference (100 * commitsWithoutTR / allCommits). Value is percentage of commits count"""
  COMMIT_WITHOUT_TICKET_REF_PERCENTAGE_COUNT
}

input CommitPercentageMetricDetailsInput {
  metricKey: CommitPercentageMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
}

input CommitPercentageMetricFacetsInput {
  metricKey: CommitPercentageMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
}

input CommitPercentageMetricInput {
  metricKey: CommitPercentageMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  breakdown: KeyString
}

"""CommitPercentageMetricInput object to be returned as part of the CommitPercentageMetricResponse"""
type CommitPercentageMetricInputType @notNormalised {
  metricKey: CommitPercentageMetric!
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  breakdown: KeyString
}

type CommitPercentageMetricResponse {
  id: ID!
  metricKey: CommitPercentageMetric!
  input: CommitPercentageMetricInputType!
  """summary with breakdown blank (key = _general_)"""
  generalSummary: PercentageSummary
  """delta of the generalSummary"""
  generalDelta: NumberDeltaSummary
  summariesList: [PercentageSummary!]!
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one delta summary per breakdown bucket
  if there are no deltas configured, it will be NULL
  """
  deltasList: [NumberDeltaSummary!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _global_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value, depending on `metric`
  """
  timeSeriesList: [PercentageTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates. The value of each point will be the cumulative average.
  
  e.g.
  
  ```
  TS0---TS1---TS2---TS3---TS4
  
  ATS0 = TS0
  ATS1 = (TS1 + TS0) / 2
  ATS2 = (TS2 + TS1 + TS0) / 3
  ATS3 = (TS3 + TS2 + TS1 + TS0) / 4
  ATS4 = (TS4 + TS3 + TS2 + TS1 + TS0) / 5
  ```
  """
  averageTimeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates.
  The value of each point will be the cumulative average taking into account only the current value + the previous `n` values (where n is the windowSize)
  It will load the TS of the past filters (same filters used for delta calculation) to get the "previous values" of the initial points
  
  e.g time series with 5 points (TS0 to TS4), windowSize = 2
  
  ```
  PTS0---PTS1---PTS2---PTS3---PTS4---TS0---TS1---TS2---TS3---TS4
  
  RATS0 = (TS0 + PTS4 + PTS3) / 3
  RATS1 = (TS1 + TS0 + PTS4) / 3
  RATS2 = (TS2 + TS1 + TS0) / 3
  RATS3 = (TS3 + TS2 + TS1) / 3
  RATS4 = (TS4 + TS3 + TS2) / 3
  ```
  """
  rollingAverageTimeSeriesList(windowSize: Int, windowSizePercentage: Float): [NumberTimeSeriesWithWindowSize!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value of commit size
  The `size` will be the number of commits that the point represents
  The `items` list will be the CommitDetails of the commit that the point represents
  """
  scatterTimeSeriesList: [ScatterTimeSeries!]!
}

"""represents a single point of a TimeSeries with a list of commits for each x"""
type CommitTimePoint implements TimePoint @notNormalised {
  """date-time of the point"""
  x: JSDate!
  """date-time of the initial moment of the time range represented by this point"""
  fromDate: UTCStartOfDay!
  """date-time of the final moment of the time range represented by this point"""
  toDate: UTCEndOfDay!
  """number of items that this point represents (commits or commits)"""
  size: Int!
  """info about the single item if this point represents a single item (only 1 commit). If size > 1 singleItem will be blank."""
  singleItem: ItemWithLink
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

type CommitTimeSeries implements TimeSeries @notNormalised {
  """identification of the Time Series"""
  key: String!
  """optional label to show in the UI for this Time Series (it should override any regular translation-based label)"""
  label: String!
  """sorted list of points"""
  data: [CommitTimePoint!]!
}

"""list of available commit metric with a single value"""
enum CommitsMetric {
  """completed commits -> value = count of commits"""
  COMMITS_COUNT
}

input CommitsMetricDetailInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: CommitsMetric!
}

input CommitsMetricFacetsInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: CommitsMetric!
}

input CommitsMetricInput {
  metricKey: CommitsMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  """granularity for the time series (default to `day` if not specified)"""
  granularity: Granularity
  """
  can be null, to ask for the general result.
  
  If it is not null, it has to be one of the keys of the valid FilterConfig (see FilterConfig)
  
  note that this cannot be an enum because it will depend on the data (custom fields et al)
  """
  breakdown: KeyString
}

type CommitsMetricInputType @notNormalised {
  metricKey: CommitsMetric!
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  breakdown: KeyString
}

type CommitsMetricResponse {
  id: ID!
  input: CommitsMetricInputType!
  metricKey: CommitsMetric!
  """summary with breakdown blank (key = _general_)"""
  generalSummary: NumberSummary
  """delta of the generalSummary"""
  generalDelta: NumberDeltaSummary
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one summary per breakdown bucket
  """
  summariesList: [NumberSummary!]!
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one delta summary per breakdown bucket
  if there are no deltas configured, it will be NULL
  """
  deltasList: [NumberDeltaSummary!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value, depending on `metric`
  """
  timeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates. The value of each point will be the cumulative average.
  
  e.g.
  
  ```
  TS0---TS1---TS2---TS3---TS4
  
  ATS0 = TS0
  ATS1 = (TS1 + TS0) / 2
  ATS2 = (TS2 + TS1 + TS0) / 3
  ATS3 = (TS3 + TS2 + TS1 + TS0) / 4
  ATS4 = (TS4 + TS3 + TS2 + TS1 + TS0) / 5
  ```
  """
  averageTimeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates.
  The value of each point will be the cumulative average taking into account only the current value + the previous `n` values (where n is the windowSize)
  It will load the TS of the past filters (same filters used for delta calculation) to get the "previous values" of the initial points
  
  e.g time series with 5 points (TS0 to TS4), windowSize = 2
  
  ```
  PTS0---PTS1---PTS2---PTS3---PTS4---TS0---TS1---TS2---TS3---TS4
  
  RATS0 = (TS0 + PTS4 + PTS3) / 3
  RATS1 = (TS1 + TS0 + PTS4) / 3
  RATS2 = (TS2 + TS1 + TS0) / 3
  RATS3 = (TS3 + TS2 + TS1) / 3
  RATS4 = (TS4 + TS3 + TS2) / 3
  ```
  """
  rollingAverageTimeSeriesList(windowSize: Int, windowSizePercentage: Float): [NumberTimeSeriesWithWindowSize!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value taken from each CommitDetail
  The `size` will be the number of commits that the point represents
  The `commits` list will be the CommitDetail of the commits that the point represents
  """
  scatterTimeSeriesList: [ScatterTimeSeries!]!
}

enum DashboardTemplateScope {
  GLOBAL
  CLIENT
}

enum DashboardVisibility {
  HIDDEN
  VISIBLE
}

"""represents a Complete Data Set (with children, with entitySet, or with both)"""
type DataSet {
  """clientKey--dataSetKey"""
  id: ID!
  clientKey: KeyString!
  """to be used in URL if it needs to be in the URL, unique per client"""
  dataSetKey: KeyString!
  """the default permission role to be added to every"""
  defaultPermissionRole: DataSetPermissionRole
  """public name to use in the UI"""
  label: String!
  """FK to category (null if it is data set `all`)"""
  categoryKey: KeyString
  """dataSetKey of the ancestor data sets"""
  ancestorsKeys: [KeyString!]!
  """dataSetKey of the descendants data sets"""
  descendantsKeys: [KeyString!]!
  """It will have a hash based on the entitySet"""
  dataSetHash: KeyString!
  """dataSetKey of the children data sets"""
  childrenKeys: [KeyString!]!
  """children data sets"""
  children: [DataSet!]!
  """entities of this DataSet, noop if not present"""
  entitySet: EntitySet
  entityLabelSet: EntityLabelSet
  """list of entities for which this Data Set has a NONE"""
  forbiddenEntities: [EntityType!]!
  dataSetType: DataSetType!
}

type DataSetCategory {
  """clientKey--categoryKey"""
  id: ID!
  clientKey: KeyString!
  """to be used in URL if it needs to be in the URL"""
  categoryKey: KeyString!
  label: String!
  dataSets: [DataSet!]!
}

"""input for creating or upserting a Data Set. It can have either children or entityset"""
input DataSetInput {
  categoryKey: KeyString!
  label: String!
  """required if the DataSet is a SuperDataSet. dataSetKey of the children"""
  childrenKeys: [KeyString!]
  """required if the DataSet is an EntityDataSet. dataSetKey of the children"""
  entitySetInput: EntitySetInput
}

enum DataSetPermissionRole {
  EDIT
  MANAGE
  VIEW
}

type DataSetTree {
  """clientKey"""
  id: ID!
  allDataSet: DataSet
  categories: [DataSetCategory!]!
}

enum DataSetType {
  ALL_DATA_SET
  ENTITY_DATA_SET
  SUPER_DATA_SET
  SUPER_ENTITY_DATA_SET
}

type DateRange @notNormalised {
  from: UTCStartOfDay
  to: UTCEndOfDay
}

input DateRangeInput {
  from: UTCStartOfDay
  to: UTCEndOfDay
}

type DefectDetail {
  id: ID!
  clientKey: KeyString!
  ticketKey: KeyString!
  issueType: String
  summary: String
  returnTransitionsCount: Int!
  returnTransitions: [ReturnedEvent!]!
}

input DeliveryTimeByStatusBucketInput {
  metricKey: DeliveryTimeMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  """the breakdown to use"""
  breakdown: KeyString!
  valueConfigKey: KeyString
}

type DeliveryTimeByStatusBucketInputType @notNormalised {
  metricKey: DeliveryTimeMetric!
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  breakdown: KeyString!
  valueConfigKey: KeyString
}

type DeliveryTimeByStatusBucketResponse {
  id: ID!
  metricKey: DeliveryTimeMetric!
  input: DeliveryTimeByStatusBucketInputType!
  """deliveryTimeMetric.generalSummary"""
  generalSummary: AverageNumberSummary
  """deliveryTimeMetric.generalDelta"""
  generalDelta: AverageNumberDeltaSummary
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one summary per breakdown bucket
  
  NOTE: value is the AVG of DAYS spent in the filtered stages for all the filtered tickets
  (stages include Deployment if it is one of the filtered ones or if we are not filtering by any
  """
  summariesList: [StatusBucketSummary!]!
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one delta summary per breakdown bucket
  if there are no deltas configured, it will be NULL
  """
  deltasList: [AverageNumberDeltaSummary!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value, depending on `metric`
  """
  timeSeriesList: [StatusBucketTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates. The value of each point will be the cumulative average.
  
  e.g.
  
  ```
  TS0---TS1---TS2---TS3---TS4
  
  ATS0 = TS0
  ATS1 = (TS1 + TS0) / 2
  ATS2 = (TS2 + TS1 + TS0) / 3
  ATS3 = (TS3 + TS2 + TS1 + TS0) / 4
  ATS4 = (TS4 + TS3 + TS2 + TS1 + TS0) / 5
  ```
  """
  averageTimeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates.
  The value of each point will be the cumulative average taking into account only the current value + the previous `n` values (where n is the windowSize)
  It will load the TS of the past filters (same filters used for delta calculation) to get the "previous values" of the initial points
  
  e.g time series with 5 points (TS0 to TS4), windowSize = 2
  
  ```
  PTS0---PTS1---PTS2---PTS3---PTS4---TS0---TS1---TS2---TS3---TS4
  
  RATS0 = (TS0 + PTS4 + PTS3) / 3
  RATS1 = (TS1 + TS0 + PTS4) / 3
  RATS2 = (TS2 + TS1 + TS0) / 3
  RATS3 = (TS3 + TS2 + TS1) / 3
  RATS4 = (TS4 + TS3 + TS2) / 3
  ```
  """
  rollingAverageTimeSeriesList(windowSize: Int, windowSizePercentage: Float): [NumberTimeSeriesWithWindowSize!]!
}

input DeliveryTimeByStatusInput {
  metricKey: DeliveryTimeMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  """the breakdown to use"""
  breakdown: KeyString!
  valueConfigKey: KeyString
}

type DeliveryTimeByStatusInputType @notNormalised {
  metricKey: DeliveryTimeMetric!
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  breakdown: KeyString!
  valueConfigKey: KeyString
}

type DeliveryTimeByStatusResponse {
  id: ID!
  metricKey: DeliveryTimeMetric!
  input: DeliveryTimeByStatusInputType!
  """deliveryTimeMetric.generalSummary"""
  generalSummary: AverageNumberSummary
  """deliveryTimeMetric.generalDelta"""
  generalDelta: AverageNumberDeltaSummary
  """
  one summary per status
  
  NOTE: value is the AVG of DAYS spent in the filtered stages for all the filtered tickets
  (stages include Deployment if it is one of the filtered ones or if we are not filtering by any
  """
  summariesList: [StatusSummary!]!
  """one delta summary per status"""
  deltasList: [StatusDeltaSummary!]!
  """
  one time series per status
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value, depending on `metric`
  """
  timeSeriesList: [StatusTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates. The value of each point will be the cumulative average.
  
  e.g.
  
  ```
  TS0---TS1---TS2---TS3---TS4
  
  ATS0 = TS0
  ATS1 = (TS1 + TS0) / 2
  ATS2 = (TS2 + TS1 + TS0) / 3
  ATS3 = (TS3 + TS2 + TS1 + TS0) / 4
  ATS4 = (TS4 + TS3 + TS2 + TS1 + TS0) / 5
  ```
  """
  averageTimeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates.
  The value of each point will be the cumulative average taking into account only the current value + the previous `n` values (where n is the windowSize)
  It will load the TS of the past filters (same filters used for delta calculation) to get the "previous values" of the initial points
  
  e.g time series with 5 points (TS0 to TS4), windowSize = 2
  
  ```
  PTS0---PTS1---PTS2---PTS3---PTS4---TS0---TS1---TS2---TS3---TS4
  
  RATS0 = (TS0 + PTS4 + PTS3) / 3
  RATS1 = (TS1 + TS0 + PTS4) / 3
  RATS2 = (TS2 + TS1 + TS0) / 3
  RATS3 = (TS3 + TS2 + TS1) / 3
  RATS4 = (TS4 + TS3 + TS2) / 3
  ```
  """
  rollingAverageTimeSeriesList(windowSize: Int, windowSizePercentage: Float): [NumberTimeSeriesWithWindowSize!]!
}

input DeliveryTimeDetailInput {
  metricKey: DeliveryTimeMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  """optionally state which tickets you want as a subset, identified with ticketKey"""
  ticketKeys: [KeyString!]
  """optionally state which tickets you want as a subset, identified with hitID"""
  itemIDs: [KeyString!]
  valueConfigKey: KeyString
}

input DeliveryTimeFacetsInput {
  metricKey: DeliveryTimeMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
}

input DeliveryTimeInput {
  metricKey: DeliveryTimeMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  breakdown: KeyString
  valueConfigKey: KeyString
}

type DeliveryTimeInputType @notNormalised {
  metricKey: DeliveryTimeMetric!
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  breakdown: KeyString
  valueConfigKey: KeyString
}

enum DeliveryTimeMetric {
  """delivery time avg by days based on original statuses"""
  DELIVERY_TIME
}

type DeliveryTimeResponse {
  id: ID!
  metricKey: DeliveryTimeMetric!
  input: DeliveryTimeInputType!
  """summary with breakdown blank (key = _general_)"""
  generalSummary: AverageNumberSummary
  """delta of the generalSummary"""
  generalDelta: AverageNumberDeltaSummary
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one summary per breakdown bucket
  
  NOTE: value is the AVG of DAYS spent in the filtered stages for all the filtered tickets
  (stages include Deployment if it is one of the filtered ones or if we are not filtering by any
  """
  summariesList: [AverageNumberSummary!]!
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one delta summary per breakdown bucket
  if there are no deltas configured, it will be NULL
  """
  deltasList: [AverageNumberDeltaSummary!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value, depending on `metric`
  """
  timeSeriesList: [AverageNumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates. The value of each point will be the cumulative average.
  
  e.g.
  
  ```
  TS0---TS1---TS2---TS3---TS4
  
  ATS0 = TS0
  ATS1 = (TS1 + TS0) / 2
  ATS2 = (TS2 + TS1 + TS0) / 3
  ATS3 = (TS3 + TS2 + TS1 + TS0) / 4
  ATS4 = (TS4 + TS3 + TS2 + TS1 + TS0) / 5
  ```
  """
  averageTimeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates.
  The value of each point will be the cumulative average taking into account only the current value + the previous `n` values (where n is the windowSize)
  It will load the TS of the past filters (same filters used for delta calculation) to get the "previous values" of the initial points
  
  e.g time series with 5 points (TS0 to TS4), windowSize = 2
  
  ```
  PTS0---PTS1---PTS2---PTS3---PTS4---TS0---TS1---TS2---TS3---TS4
  
  RATS0 = (TS0 + PTS4 + PTS3) / 3
  RATS1 = (TS1 + TS0 + PTS4) / 3
  RATS2 = (TS2 + TS1 + TS0) / 3
  RATS3 = (TS3 + TS2 + TS1) / 3
  RATS4 = (TS4 + TS3 + TS2) / 3
  ```
  """
  rollingAverageTimeSeriesList(windowSize: Int, windowSizePercentage: Float): [NumberTimeSeriesWithWindowSize!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the duration in days
  The `size` will be the number of tickets that the point represents
  The `singleItem` is populated only when size is 1. It contains key, label, link
  """
  scatterTimeSeriesList: [TicketWithKeysNumberTimeSeries!]!
}

"""determines if the currentValue is > (INCREASE), < (DECREASE) or = (SAME) than the pastValue"""
enum DeltaIncrease {
  """currentValue = pastValue"""
  SAME
  """currentValue > pastValue"""
  INCREASE
  """currentValue < pastValue"""
  DECREASE
}

"""determines if the delta is supposed to be considered an improvement, a deterioration, or neutral"""
enum DeltaQuality {
  """no quality judgement. Could be that the values are the same or that there is no judgement to be passed."""
  NEUTRAL
  """an improvement"""
  BETTER
  """a deterioriation"""
  WORSE
}

"""determines how the difference between currentValue and pastValue determines the `quality`"""
enum DeltaQualityMode {
  """uses DeltaQuality.NEUTRAL no matter what"""
  NEUTRAL
  """if currentValue > pastValue => BETTER, if the same NEUTRAL, otherwise WORSE"""
  DIRECT
  """if currentValue > pastValue => WORSE, if the same NEUTRAL, otherwise BETTER"""
  INVERSE
}

type DeltaValue @notNormalised {
  """value of the past summary"""
  pastValue: Float!
  """value of the current summary"""
  currentValue: Float!
  """filters of the past summary"""
  pastFilters: Filters!
  """filters of the current summary"""
  currentFilters: Filters!
  """`currentValue - pastValue`"""
  deltaAbsolute: FloatRounded2!
  """`100 * ((currentValue - pastValue) / abs(pastValue))`, null if pastValue is 0 or if `canHaveRelative` is false"""
  deltaRelative: FloatRounded2
  """true if we do have deltaRelative calculated"""
  canHaveRelative: Boolean!
  """quality of the Delta (see `DeltaQuality`)"""
  quality: DeltaQuality!
  """increase of the Delta (see `DeltaIncrease`)"""
  increase: DeltaIncrease!
  """mode to calculate the quality of the Delta (see `DeltaQualityMode`)"""
  qualityMode: DeltaQualityMode!
  """mode to check if we calculate deltaRelative and which deltaX to prefer"""
  valueMode: DeltaValueMode!
}

"""determines what kind of delta to use: only Absolute, or Absolute and Relative (preferring Relative if possible)"""
enum DeltaValueMode {
  """canHaveRelative = false, deltaRelative = null, use only deltaAbsolute"""
  ABSOLUTE_ONLY
  """calculate both deltaRelative and deltaAbsolute, and prefer the use of deltaRelative"""
  PREFER_RELATIVE
}

type DeploymentDetail {
  id: ID!
  """determines the field used to calculate `value`"""
  valueConfigKey: KeyString!
  """value to be used. By default Story Points"""
  value: Float
  """Unique deployment ID"""
  deploymentKey: String!
  """Deployment name (label)"""
  deploymentLabel: String!
  """Can be (sucess, failure, etc... )"""
  status: String!
  """Time the current deployments took place"""
  time: JSDate!
  """Deployment build commit"""
  commit: String!
  """Deployment build"""
  build: String!
  """Deployment last sucess minutes"""
  lastSuccessMins: Float!
  """Lead time (duration) in seconds. Optional"""
  leadTimeSeconds: Float
}

type DeploymentDetailList {
  id: ID!
  """total quantity of details"""
  count: Int!
  """list of ticket details, all sharing the same valueConfig to calculate the `value` field"""
  details: [DeploymentDetail!]!
  """key of the valueConfig"""
  valueConfigKey: KeyString!
  """valueConfig used for the `value` field of each detail"""
  valueConfig: ValueConfig!
}

enum EntityKey {
  """indicates that the value is a person_id, and the label to show in the UI should be the person's name"""
  PERSON
  """indicates that the value is a board_key, and the label to show in the UI should be the board's name/title"""
  BOARD
  """indicates that the value is a repositoryUri, and the label to show in the UI should be the repository's name"""
  REPOSITORY
  """indicates that the value is a pipelineUri, and the label to show in the UI should be the pipeline's name"""
  CI_PIPELINE
  """indicates that the value is a monitoredServiceUri, and the label to show in the UI should be the service's name"""
  MONITORED_SERVICE
}

"""KeyLabelSets of entities"""
type EntityLabelSet @notNormalised {
  boardsKeyLabelSet: KeyLabelSet!
  peopleKeyLabelSet: KeyLabelSet!
  repositoriesKeyLabelSet: KeyLabelSet!
  ciPipelinesKeyLabelSet: KeyLabelSet!
  monitoredServicesKeyLabelSet: KeyLabelSet!
}

"""KeySets of entities"""
type EntitySet @notNormalised {
  boardsKeySet: KeySet!
  peopleKeySet: KeySet!
  repositoriesKeySet: KeySet!
  ciPipelinesKeySet: KeySet!
  monitoredServicesKeySet: KeySet!
}

"""(INPUT) KeySets of entities"""
input EntitySetInput {
  boardsKeySet: KeySet!
  peopleKeySet: KeySet!
  repositoriesKeySet: KeySet!
  ciPipelinesKeySet: KeySet!
  monitoredServicesKeySet: KeySet!
}

enum EntityType {
  BOARD
  CI_PIPELINE
  MONITORED_SERVICE
  PERSON
  REPOSITORY
}

enum ErrorCodes {
  INVALID_DATA_SET
  INVALID_METRIC_DASHBOARD
  SEARCH_API_ERROR
  INVALID_METRIC_INPUT
  DISABLED_METRIC
  INVALID_METRIC_CONFIG
  INVALID_METRIC_CALCULATION
  NO_SPRINT
  ENTITY_NOT_FOUND
}

"""represents each value-count pair"""
type FacetValue @notNormalised {
  key: String!
  label: String!
  count: Int!
}

type FilterConfig {
  """for general configs = key, for custom configs = `clientKey--key`"""
  id: ID!
  """ID of the breakdown / filter / facet"""
  key: KeyString!
  """name to show in the UI for this breakdown / filter / facet"""
  label: String!
  """indicates which type of value are in the field (see FilterValueType)"""
  valueType: FilterValueType!
  """indicates the type of the filter (multi-valued vs boolean)"""
  filterType: FilterType!
}

"""extends FilterConfig, adds all possible facet values"""
type FilterConfigWithFacets {
  id: ID!
  filterConfigKey: KeyString!
  filterConfig: FilterConfig!
  """list of valid values as facet values (with count of items that would be filtered by it)"""
  values: [FacetValue!]!
}

enum FilterType {
  """indicates that the filter can have multiple STRING values"""
  MULTI
  """indicates that the filter can have a single BOOLEAN value"""
  BOOL
  """indicates that the filter can have a RANGE of from/to values"""
  RANGE
  """indicates that the filter is a DataSet (Only to be used as Breakdown Config) -> breakdowns will be the children of the filtered data set"""
  DATA_SET
  """indicates that the filter is a DataSetCategory (Only to be used as Breakdown Config) -> breakdowns will be the data set of the category determined by the FilterConfig"""
  DATA_SET_CATEGORY
}

enum FilterValueType {
  """indicates that the value is a regular value, the label to show in the UI is the same as the value or it will come from a translation"""
  NORMAL
  """indicates that the value is in itself the label to show in the UI for it"""
  SAME
  """indicates that the value is a sprintKey, and the label to show in the UI should be the sprint's name"""
  SPRINT
  """indicates that the value is a person_id, and the label to show in the UI should be the person's name"""
  PERSON
  """indicates that the value is a person_id, and the label to show in the UI should be the person's name, but this will not count as a Person entity for entitySet/DataSet purposes"""
  PERSON_NOT_ENTITY
  """indicates that the value is a board_key, and the label to show in the UI should be the board's name/title"""
  BOARD
  """indicates that the value is a repositoryUri, and the label to show in the UI should be the repository's name"""
  REPOSITORY
  """indicates that the value is a pipelineUri, and the label to show in the UI should be the pipeline's name"""
  CI_PIPELINE
  """indicates that the value is a transitionKey, and the label to show in the UI should be `PreviousStatus => NewStatus` (uses the SEPARATOR constant)"""
  TRANSITION
  """indicates that the value is a ticket issue type key, and the label to show in the UI should be the type name"""
  ISSUE_TYPE
  """indicates that the value is a Pull Request statusKey, and the label to show in the UI should be the status name"""
  PULL_REQUEST_STATUS
  """indicates that the value is a ticket statusKey, and the label to show in the UI should be the status name"""
  STATUS
  """indicates that the value is a ticket plandekStatusKey, and the label to show in the UI should be the plandek status name"""
  PLANDEK_STATUS
  """indicates that the value is a statusBucketKey, and the label to show in the UI should be the status bucket label"""
  STATUS_BUCKET
  """indicates that the value is a monitoredServiceUri, and the label to show in the UI should be the service's name"""
  MONITORED_SERVICE
  """indicates that the value is a dataSet, and the label to show in the UI should be the data set's label"""
  DATA_SET
  """date value"""
  DATE
  """datetime value"""
  DATE_TIME
  """numeric value"""
  NUMERIC
}

type FilteredPullRequestDetailWithStatuses {
  id: ID!
  pullRequestDetail: PullRequestDetail!
  statuses: [PullRequestStatus!]!
  sumDurationInSeconds: FloatRounded2!
  sumDurationInHours: FloatRounded2!
}

type FilteredPullRequestDetailWithStatusesList {
  id: ID!
  """total quantity of details"""
  count: Int!
  """list of ticket details, all sharing the same valueConfig to calculate the `value` field"""
  details: [FilteredPullRequestDetailWithStatuses!]!
  """key of the valueConfig"""
  valueConfigKey: KeyString!
  """valueConfig used for the `value` field of each detail"""
  valueConfig: ValueConfig!
}

type FilteredTicketDetailWithStatuses {
  id: ID!
  ticketDetail: TicketDetail!
  statuses: [Status!]!
  sumDurationInSeconds: FloatRounded1!
  sumDurationInDays: FloatRounded1!
}

type FilteredTicketDetailWithStatusesList {
  id: ID!
  """total quantity of details"""
  count: Int!
  """list of item details, all sharing the same valueConfig to calculate the `value` field"""
  details: [FilteredTicketDetailWithStatuses!]!
  """key of the valueConfig"""
  valueConfigKey: KeyString!
  """valueConfig used for the `value` field of each detail"""
  valueConfig: ValueConfig!
}

"""(TYPE) common set of filters for all metrics"""
type Filters @notNormalised {
  """mandatory clientKey to filter the tickets to take into account"""
  clientKey: KeyString!
  """optional dateRange to filter the hits to take into account (WARNING: could have a different meaning on each metric)"""
  dateRange: DateRange
  """optional percentileRange to filter the hits to take into account (WARNING: could have a different meaning on each metric)"""
  percentileRange: PercentileRange
  """optional sprint key to use in filters. If the metric does not support filtering by sprint and there is no dateRange, it should use the sprint's dates as date range"""
  sprintKey: KeyString
  """optional sprint key to use as pastSprint in filters for deltas. If the metric does not support filtering by sprint and there is no dateRange, it should use the sprint's dates as date range"""
  pastSprintKey: KeyString
  """required dataSetKey for the DataSet to use as filters"""
  dataSetKey: KeyString!
  """optionally removing children data sets from the calculation (e.g. children of the filtered data set)"""
  excludeDataSetKeys: [KeyString!]
  """optional hash of the dataSet, used to bust ui cache, ignored in the API"""
  dataSetHash: KeyString
  """optional entities to use a filters"""
  entitySetRefinement: EntitySet
  """list of extra filters to apply when filtering the hits to take into account (e.g. issue types)"""
  multiFilters: [MultiFilter!]!
  """list of extra flag filters (boolean filters) to apply when filtering the tickets to take into account"""
  boolFilters: [BoolFilter!]!
  """list of extra range filters to apply when filtering the tickets to take into account"""
  rangeFilters: [RangeFilter!]!
  """optional filter, intended to be the value of the `y` axis of the scatter chart."""
  scatterMetricInput: ScatterMetricInputType
  """optionally state which items you want as a subset, identified with hitID. Each metric will know which type of items to load"""
  itemIDs: [KeyString!]
}

"""(INPUT) common set of filters for all metrics"""
input FiltersInput {
  """mandatory clientKey to filter the tickets to take into account"""
  clientKey: KeyString!
  """optional dateRange to filter the tickets to take into account (WARNING: could have a different meaning on each metric)"""
  dateRange: DateRangeInput
  """optional percentileRange to filter the tickets to take into account (WARNING: could have a different meaning on each metric)"""
  percentileRange: PercentileRangeInput
  """optional sprint key to use in filters. If the metric does not support filtering by sprint and there is no dateRange, it should use the sprint's dates as date range"""
  sprintKey: KeyString
  """optional sprint key to use as pastSprint in filters for deltas. If the metric does not support filtering by sprint and there is no dateRange, it should use the sprint's dates as date range"""
  pastSprintKey: KeyString
  """required dataSetKey for the DataSet to use as filters"""
  dataSetKey: KeyString!
  """optionally removing children data sets from the calculation (e.g. children of the filtered data set)"""
  excludeDataSetKeys: [KeyString!]
  """optional hash of the dataSet, used to bust ui cache, ignored in the API"""
  dataSetHash: KeyString
  """optional entities to use a filters"""
  entitySetRefinement: EntitySetInput
  """list of extra filters to apply when filtering the tickets to take into account (e.g. issue types)"""
  multiFilters: [MultiFilterInput!]!
  """list of extra flag filters (boolean filters) to apply when filtering the tickets to take into account"""
  boolFilters: [BoolFilterInput!]!
  """list of extra range filters to apply when filtering the tickets to take into account"""
  rangeFilters: [RangeFilterInput!]!
  """optionally state which items you want as a subset, identified with hitID. Each metric will know which type of items to load"""
  itemIDs: [KeyString!]
  """optional filter, intended to be the value of the `y` axis of the scatter chart."""
  scatterMetricInput: ScatterMetricInput
}

"""(INPUT) FiltersInput excluding the dateRange, sprint and pastSprint"""
input FiltersNoTimeInput {
  """mandatory clientKey to filter the tickets to take into account"""
  clientKey: KeyString!
  """required dataSetKey for the DataSet to use as filters"""
  dataSetKey: KeyString!
  """optionally removing children data sets from the calculation (e.g. children of the filtered data set)"""
  excludeDataSetKeys: [KeyString!]
  """optional hash of the dataSet, used to bust ui cache, ignored in the API"""
  dataSetHash: KeyString
  """optional entities to use a filters"""
  entitySetRefinement: EntitySetInput
  """list of extra filters to apply when filtering the tickets to take into account (e.g. issue types)"""
  multiFilters: [MultiFilterInput!]!
  """list of extra flag filters (boolean filters) to apply when filtering the tickets to take into account"""
  boolFilters: [BoolFilterInput!]!
  """list of extra range filters to apply when filtering the tickets to take into account"""
  rangeFilters: [RangeFilterInput!]!
  """optional percentileRange to filter the hits to take into account (WARNING: could have a different meaning on each metric)"""
  percentileRange: PercentileRangeInput
  """optionally state which items you want as a subset, identified with hitID. Each metric will know which type of items to load"""
  itemIDs: [KeyString!]
  """optional filter, intended to be the value of the `y` axis of the scatter chart."""
  scatterMetricInput: ScatterMetricInput
}

"""(INPUT) minimal set of filters with a DataSet and EntitySetRefinement"""
input FiltersOnlyDataSetAndEntitiesInput {
  """mandatory clientKey to filter the tickets to take into account"""
  clientKey: KeyString!
  """required dataSetKey for the DataSet to use as filters"""
  dataSetKey: KeyString!
  """optionally removing children data sets from the calculation (e.g. children of the filtered data set)"""
  excludeDataSetKeys: [KeyString!]
  """optional hash of the dataSet, used to bust ui cache, ignored in the API"""
  dataSetHash: KeyString
  """optional entities to use a filters"""
  entitySetRefinement: EntitySetInput
}

"""(INPUT) minimal set of filters with a DataSet"""
input FiltersOnlyDataSetInput {
  """mandatory clientKey to filter the tickets to take into account"""
  clientKey: KeyString!
  """required dataSetKey for the DataSet to use as filters"""
  dataSetKey: KeyString!
  """optionally removing children data sets from the calculation (e.g. children of the filtered data set)"""
  excludeDataSetKeys: [KeyString!]
  """optional hash of the dataSet, used to bust ui cache, ignored in the API"""
  dataSetHash: KeyString
}

input FirstTimeYieldRateFacetsInput {
  metricKey: FirstTimeYieldRateMetric!
  filters: FiltersInput!
  """if not specified, we'll look into the stored Config for this client"""
  returnConfig: TicketReturnSettingsInput
}

enum FirstTimeYieldRateMetric {
  """First Time Yield Rate -> value is % of completed tickets that never were returned"""
  FIRST_TIME_YIELD_RATE
}

input FirstTimeYieldRateMetricDetailsInput {
  metricKey: FirstTimeYieldRateMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  """if not specified, we'll look into the stored Config for this client"""
  returnConfig: TicketReturnSettingsInput
}

input FirstTimeYieldRateMetricInput {
  metricKey: FirstTimeYieldRateMetric!
  """if not specified, we'll look into the stored Config for this client"""
  returnConfig: TicketReturnSettingsInput
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  """granularity for the time series (default to `day` if not specified)"""
  granularity: Granularity
  """
  can be null, to ask for the general result.
  
  If it is not null, it has to be one of the keys of the valid FilterConfig (see FilterConfig)
  
  note that this cannot be an enum because it will depend on the data (custom fields et al)
  """
  breakdown: KeyString
}

type FirstTimeYieldRateMetricInputType @notNormalised {
  metricKey: FirstTimeYieldRateMetric!
  returnConfig: TicketReturnSettings
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  breakdown: KeyString
}

type FirstTimeYieldRateMetricResponse {
  id: ID!
  input: FirstTimeYieldRateMetricInputType!
  metricKey: FirstTimeYieldRateMetric!
  """summary with breakdown blank (key = _general_)"""
  generalSummary: PercentageSummary
  """delta of the generalSummary"""
  generalDelta: NumberDeltaSummary
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one summary per breakdown bucket
  """
  summariesList: [PercentageSummary!]!
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one delta summary per breakdown bucket
  if there are no deltas configured, it will be NULL
  """
  deltasList: [NumberDeltaSummary!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value, depending on `metric`
  """
  timeSeriesList: [PercentageTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates. The value of each point will be the cumulative average.
  
  e.g.
  
  ```
  TS0---TS1---TS2---TS3---TS4
  
  ATS0 = TS0
  ATS1 = (TS1 + TS0) / 2
  ATS2 = (TS2 + TS1 + TS0) / 3
  ATS3 = (TS3 + TS2 + TS1 + TS0) / 4
  ATS4 = (TS4 + TS3 + TS2 + TS1 + TS0) / 5
  ```
  """
  averageTimeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates.
  The value of each point will be the cumulative average taking into account only the current value + the previous `n` values (where n is the windowSize)
  It will load the TS of the past filters (same filters used for delta calculation) to get the "previous values" of the initial points
  
  e.g time series with 5 points (TS0 to TS4), windowSize = 2
  
  ```
  PTS0---PTS1---PTS2---PTS3---PTS4---TS0---TS1---TS2---TS3---TS4
  
  RATS0 = (TS0 + PTS4 + PTS3) / 3
  RATS1 = (TS1 + TS0 + PTS4) / 3
  RATS2 = (TS2 + TS1 + TS0) / 3
  RATS3 = (TS3 + TS2 + TS1) / 3
  RATS4 = (TS4 + TS3 + TS2) / 3
  ```
  """
  rollingAverageTimeSeriesList(windowSize: Int, windowSizePercentage: Float): [NumberTimeSeriesWithWindowSize!]!
}

input FirstTimeYieldTicketsFacetsInput {
  metricKey: FirstTimeYieldTicketsMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  """if not specified, we'll look into the stored Config for this client"""
  returnConfig: TicketReturnSettingsInput
}

enum FirstTimeYieldTicketsMetric {
  """First Time Yield Tickets -> value is number of tickets that were returned"""
  FIRST_TIME_YIELD_TICKETS_COUNT
  """First Time Yield Tickets -> value is sum of story points of tickets that were completed and never returned"""
  FIRST_TIME_YIELD_TICKETS_STORY_POINTS
}

input FirstTimeYieldTicketsMetricDetailsInput {
  metricKey: FirstTimeYieldTicketsMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  """if not specified, we'll look into the stored Config for this client"""
  returnConfig: TicketReturnSettingsInput
}

input FirstTimeYieldTicketsMetricInput {
  metricKey: FirstTimeYieldTicketsMetric!
  """if not specified, we'll look into the stored Config for this client"""
  returnConfig: TicketReturnSettingsInput
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  """granularity for the time series (default to `day` if not specified)"""
  granularity: Granularity
  """
  can be null, to ask for the general result.
  
  If it is not null, it has to be one of the keys of the valid FilterConfig (see FilterConfig)
  
  note that this cannot be an enum because it will depend on the data (custom fields et al)
  """
  breakdown: KeyString
}

type FirstTimeYieldTicketsMetricInputType @notNormalised {
  metricKey: FirstTimeYieldTicketsMetric!
  returnConfig: TicketReturnSettings
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  breakdown: KeyString
}

type FirstTimeYieldTicketsMetricResponse {
  id: ID!
  input: FirstTimeYieldTicketsMetricInputType!
  metricKey: FirstTimeYieldTicketsMetric!
  """summary with breakdown blank (key = _general_)"""
  generalSummary: NumberSummary
  """delta of the generalSummary"""
  generalDelta: NumberDeltaSummary
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one summary per breakdown bucket
  """
  summariesList: [NumberSummary!]!
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one delta summary per breakdown bucket
  if there are no deltas configured, it will be NULL
  """
  deltasList: [NumberDeltaSummary!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value, depending on `metric`
  """
  timeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates. The value of each point will be the cumulative average.
  
  e.g.
  
  ```
  TS0---TS1---TS2---TS3---TS4
  
  ATS0 = TS0
  ATS1 = (TS1 + TS0) / 2
  ATS2 = (TS2 + TS1 + TS0) / 3
  ATS3 = (TS3 + TS2 + TS1 + TS0) / 4
  ATS4 = (TS4 + TS3 + TS2 + TS1 + TS0) / 5
  ```
  """
  averageTimeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates.
  The value of each point will be the cumulative average taking into account only the current value + the previous `n` values (where n is the windowSize)
  It will load the TS of the past filters (same filters used for delta calculation) to get the "previous values" of the initial points
  
  e.g time series with 5 points (TS0 to TS4), windowSize = 2
  
  ```
  PTS0---PTS1---PTS2---PTS3---PTS4---TS0---TS1---TS2---TS3---TS4
  
  RATS0 = (TS0 + PTS4 + PTS3) / 3
  RATS1 = (TS1 + TS0 + PTS4) / 3
  RATS2 = (TS2 + TS1 + TS0) / 3
  RATS3 = (TS3 + TS2 + TS1) / 3
  RATS4 = (TS4 + TS3 + TS2) / 3
  ```
  """
  rollingAverageTimeSeriesList(windowSize: Int, windowSizePercentage: Float): [NumberTimeSeriesWithWindowSize!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value taken from each TicketDetail
  The `size` will be the number of tickets that the point represents
  The `singleItem` is populated only when size is 1. It contains key, label, link
  """
  scatterTimeSeriesList: [ScatterTimeSeries!]!
}

"""float rounded to 1 decimal place"""
scalar FloatRounded1

"""float rounded to 2 decimal places"""
scalar FloatRounded2

type FlowEfficiencyByStatusBucketResponse {
  id: ID!
  metricKey: FlowEfficiencyMetric!
  input: FlowEfficiencyInputByStatusBucketType!
  """summary with breakdown blank (key = _general_)"""
  generalSummary: PercentageSummary
  """delta of the generalSummary"""
  generalDelta: NumberDeltaSummary
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one summary per breakdown bucket
  
  NOTE: value is `100 * (sumOfSecondsInActiveStatuses / sumOfSecondsInAllStatuses)`
  """
  summariesList: [StatusBucketPercentageSummary!]!
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one delta summary per breakdown bucket
  if there are no deltas configured, it will be NULL
  """
  deltasList: [StatusBucketNumberDeltaSummary!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value, depending on `metric`
  """
  timeSeriesList: [StatusBucketPercentageTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates. The value of each point will be the cumulative average.
  
  e.g.
  
  ```
  TS0---TS1---TS2---TS3---TS4
  
  ATS0 = TS0
  ATS1 = (TS1 + TS0) / 2
  ATS2 = (TS2 + TS1 + TS0) / 3
  ATS3 = (TS3 + TS2 + TS1 + TS0) / 4
  ATS4 = (TS4 + TS3 + TS2 + TS1 + TS0) / 5
  ```
  """
  averageTimeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates.
  The value of each point will be the cumulative average taking into account only the current value + the previous `n` values (where n is the windowSize)
  It will load the TS of the past filters (same filters used for delta calculation) to get the "previous values" of the initial points
  
  e.g time series with 5 points (TS0 to TS4), windowSize = 2
  
  ```
  PTS0---PTS1---PTS2---PTS3---PTS4---TS0---TS1---TS2---TS3---TS4
  
  RATS0 = (TS0 + PTS4 + PTS3) / 3
  RATS1 = (TS1 + TS0 + PTS4) / 3
  RATS2 = (TS2 + TS1 + TS0) / 3
  RATS3 = (TS3 + TS2 + TS1) / 3
  RATS4 = (TS4 + TS3 + TS2) / 3
  ```
  """
  rollingAverageTimeSeriesList(windowSize: Int, windowSizePercentage: Float): [NumberTimeSeriesWithWindowSize!]!
}

input FlowEfficiencyDetailInput {
  metricKey: FlowEfficiencyMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  """optionally state which tickets you want as a subset, identified with ticketKey"""
  ticketKeys: [KeyString!]
  """optionally state which tickets you want as a subset, identified with hitID"""
  itemIDs: [KeyString!]
  """keys of the statuses that will be counted as Active"""
  activeStatusKeys: [KeyString!]!
}

input FlowEfficiencyFacetsInput {
  metricKey: FlowEfficiencyMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
}

input FlowEfficiencyInput {
  metricKey: FlowEfficiencyMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  breakdown: KeyString
  """keys of the statuses that will be counted as Active (the rest will be inactive)"""
  activeStatusKeys: [KeyString!]!
}

input FlowEfficiencyInputByStatusBucket {
  metricKey: FlowEfficiencyMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  breakdown: KeyString!
  """keys of the statuses that will be counted as Active (the rest will be inactive)"""
  activeStatusKeys: [KeyString!]!
}

type FlowEfficiencyInputByStatusBucketType @notNormalised {
  metricKey: FlowEfficiencyMetric!
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  breakdown: KeyString!
  activeStatusKeys: [KeyString!]!
}

type FlowEfficiencyInputType @notNormalised {
  metricKey: FlowEfficiencyMetric!
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  breakdown: KeyString
  activeStatusKeys: [KeyString!]!
}

enum FlowEfficiencyMetric {
  FLOW_EFFICIENCY
}

type FlowEfficiencyResponse {
  id: ID!
  metricKey: FlowEfficiencyMetric!
  input: FlowEfficiencyInputType!
  """summary with breakdown blank (key = _general_)"""
  generalSummary: PercentageSummary
  """delta of the generalSummary"""
  generalDelta: NumberDeltaSummary
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one summary per breakdown bucket
  
  NOTE: value is `100 * (sumOfSecondsInActiveStatuses / sumOfSecondsInAllStatuses)`
  """
  summariesList: [PercentageSummary!]!
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one delta summary per breakdown bucket
  if there are no deltas configured, it will be NULL
  """
  deltasList: [NumberDeltaSummary!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value, depending on `metric`
  """
  timeSeriesList: [PercentageTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates. The value of each point will be the cumulative average.
  
  e.g.
  
  ```
  TS0---TS1---TS2---TS3---TS4
  
  ATS0 = TS0
  ATS1 = (TS1 + TS0) / 2
  ATS2 = (TS2 + TS1 + TS0) / 3
  ATS3 = (TS3 + TS2 + TS1 + TS0) / 4
  ATS4 = (TS4 + TS3 + TS2 + TS1 + TS0) / 5
  ```
  """
  averageTimeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates.
  The value of each point will be the cumulative average taking into account only the current value + the previous `n` values (where n is the windowSize)
  It will load the TS of the past filters (same filters used for delta calculation) to get the "previous values" of the initial points
  
  e.g time series with 5 points (TS0 to TS4), windowSize = 2
  
  ```
  PTS0---PTS1---PTS2---PTS3---PTS4---TS0---TS1---TS2---TS3---TS4
  
  RATS0 = (TS0 + PTS4 + PTS3) / 3
  RATS1 = (TS1 + TS0 + PTS4) / 3
  RATS2 = (TS2 + TS1 + TS0) / 3
  RATS3 = (TS3 + TS2 + TS1) / 3
  RATS4 = (TS4 + TS3 + TS2) / 3
  ```
  """
  rollingAverageTimeSeriesList(windowSize: Int, windowSizePercentage: Float): [NumberTimeSeriesWithWindowSize!]!
}

"""input for the loadGenericMetric query"""
input GenericMetricInput {
  metricKey: MetricKey!
  filters: FiltersInput!
  granularity: Granularity
  breakdown: KeyString
  valueConfigKey: KeyString
  sprintDateRangeFieldOverride: SprintDateRangeField
  customSettings: MetricCardCustomSettingsInput
}

"""input to be used for getting the response of a metricCard. It uses this input + the information in the metric card to create a `GenericMetricInput`"""
input GenericMetricInputMin {
  """optional dateRange to filter the tickets to take into account (WARNING: could have a different meaning on each metric)"""
  dateRange: DateRangeInput
  """optional sprint key to use in filters. If the metric does not support filtering by sprint and there is no dateRange, it should use the sprint's dates as date range"""
  sprintKey: KeyString
  """optional sprint key to use as pastSprint in filters for deltas. If the metric does not support filtering by sprint and there is no dateRange, it should use the sprint's dates as date range"""
  pastSprintKey: KeyString
  """optional, defaults to DAY"""
  granularity: Granularity
}

"""
Faux-Union of all the responses possible for the genericMetric query

note: it is not an actual union because the different responses have incompatible field types, which would make using this from the client point of view quite hard
"""
type GenericMetricResponse {
  id: ID!
  genericMetricResponseType: GenericMetricResponseType!
  metricKey: MetricKey!
  ciMetric: CiMetricResponse
  codeCycleTime: CodeCycleTimeResponse
  codeCycleTimeByStatus: CodeCycleTimeByStatusResponse
  codeKnowledgeMetric: CodeKnowledgeMetricResponse
  commitPercentageMetric: CommitPercentageMetricResponse
  commitsMetric: CommitsMetricResponse
  deliveryTimeByStatusBucket: DeliveryTimeByStatusBucketResponse
  deliveryTimeByStatus: DeliveryTimeByStatusResponse
  deliveryTime: DeliveryTimeResponse
  firstTimeYieldRateMetric: FirstTimeYieldRateMetricResponse
  firstTimeYieldTicketsMetric: FirstTimeYieldTicketsMetricResponse
  flowEfficiencyByStatusBucket: FlowEfficiencyByStatusBucketResponse
  flowEfficiency: FlowEfficiencyResponse
  pipelinesMetric: PipelinesMetricResponse
  pullRequestsMetric: PullRequestsMetricResponse
  returnRateMetric: ReturnRateMetricResponse
  returnTicketsMetric: ReturnTicketsMetricResponse
  speedingTicketsMetric: SpeedingTicketsMetricResponse
  speedingTransitionsRateMetric: SpeedingTransitionsRateMetricResponse
  sprintCompletion: SprintCompletionResponse
  sprintValuePerTeamSizeMetric: SprintValuePerTeamSizeMetricResponse
  ticketCommitHotspotsMetric: TicketCommitHotspotsMetricResponse
  ticketsMetric: TicketsMetricResponse
  ticketsScopeMetric: TicketsScopeMetricResponse
  ticketsTimelineMetricStatusBucket: TicketsTimelineMetricStatusBucketResponse
  ticketsTimelineMetricStatus: TicketsTimelineMetricStatusResponse
  treemap: TreemapResponse
  unresolvedTicketsMetric: UnresolvedTicketsMetricResponse
}

enum GenericMetricResponseType {
  CiMetricResponse
  CodeCycleTimeResponse
  CodeCycleTimeByStatusResponse
  CodeKnowledgeMetricResponse
  CommitPercentageMetricResponse
  CommitsMetricResponse
  DeliveryTimeByStatusBucketResponse
  DeliveryTimeByStatusResponse
  DeliveryTimeResponse
  FirstTimeYieldRateMetricResponse
  FirstTimeYieldTicketsMetricResponse
  FlowEfficiencyByStatusBucketResponse
  FlowEfficiencyResponse
  PipelinesMetricResponse
  PullRequestsMetricResponse
  ReturnRateMetricResponse
  ReturnTicketsMetricResponse
  SpeedingTicketsMetricResponse
  SpeedingTransitionsRateMetricResponse
  SprintCompletionResponse
  SprintValuePerTeamSizeMetricResponse
  TicketCommitHotspotsMetricResponse
  TicketsMetricResponse
  TicketsScopeMetricResponse
  TicketsTimelineMetricStatusBucketResponse
  TicketsTimelineMetricStatusResponse
  TreemapResponse
  UnresolvedTicketsMetricResponse
}

enum Granularity {
  day
  week
  month
  year
}

type HotspotSummary @notNormalised {
  """indicates if this represents a Folder, File or Repo"""
  hotspotType: HotspotType!
  """identification of the summary"""
  key: String!
  """
  label to show in the UI for this Summary bucket
    - repo: repo url
    - folder: folder name
    - file: filename
  """
  label: String!
  """
  path of the summary
    - repo: repo url
    - folder: full folder path from repo root
    - file: full file path from repo root
  """
  path: String!
  """value of the summary"""
  value: Float!
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

enum HotspotType {
  REPOSITORY
  FOLDER
  FILE
}

input InputWithFilters {
  filters: FiltersInput!
}

input InputWithFiltersOnlyDataSet {
  filters: FiltersOnlyDataSetInput!
}

enum IntegrationClass {
  CODE
  TICKETS
  BUILDS
  BUILD_COST
  DEPLOYMENTS
  TIME
}

"""object that represents the type of a ticket"""
type IssueType {
  id: ID!
  key: KeyString!
  label: String!
}

type ItemWithLink @notNormalised {
  """id of the hit that this item represents"""
  hitID: String!
  """identifier of the scatter item. Commit hash, ticket key, whatever it is. Not intended to be printed in the UI"""
  key: String!
  """whatever we show to the user about this scatter item. Could be the commit hash, or the ticket Key, etc."""
  label: String!
  """optional external link to the scatter item (commit in github, or ticket in jira, etc)"""
  link: String
}

"""Date in UTC, expected to be used as a js Date in the UI (as normal Dayjs in the server)"""
scalar JSDate

"""key-label pair"""
type KeyLabel @notNormalised {
  key: KeyString!
  label: String!
}

"""
returning as an object like this

type KeyLabelSet {
  type: KeySetType!
  elements: [KeyLabel!]
}

which should be parsed as a KeyLabelSet object of @eturino/key-set lib
"""
scalar KeyLabelSet

"""
returning as an object like this

type KeySet {
  type: KeySetType!
  elements: [String!]
}

which should be parsed as a KeySet object of @eturino/key-set lib
"""
scalar KeySet

enum KeySetType {
  ALL
  ALL_EXCEPT_SOME
  NONE
  SOME
}

"""string cleaned to act as a clean key, allowing only A-Z,a-z,0-9,-,_,. chars"""
scalar KeyString

enum MagicKeys {
  _general_
  _unknown_
  _deployment_
  _awaiting_deployment_
  _active_
  _inactive_
  _current_sprint_
  _last_sprint_
  _ignored_
  _data_set_
  _data_set_category_
  _unmapped_
}

type MetricCard {
  """cardListId--cardKey"""
  id: ID!
  cardListId: KeyString!
  cardKey: KeyString!
  """clientKey from dashboard"""
  clientKey: KeyString!
  """dataSetKey from dashboard"""
  dataSetKey: KeyString!
  """dashboardKey from dashboard"""
  dashboardKey: KeyString!
  """numeric position in the card list"""
  position: Int!
  """size of card to render"""
  size: CardSizeType
  enabledBreakdowns: KeySet!
  """settings: breakdown Configs of the enabled Breakdowns"""
  enabledBreakdownConfigs: [FilterConfig!]!
  chartType: KeyString
  chartTab: ChartTab!
  primaryMetricDataConfig: MetricDataConfig!
  overlayMetricDataConfigs: [MetricDataConfig!]
}

type MetricCardAverageTimeSeriesSettings @notNormalised {
  averageType: AverageType
}

input MetricCardAverageTimeSeriesSettingsInput {
  averageType: AverageType
}

type MetricCardCustomSettings @notNormalised {
  returns: TicketReturnSettings
  flowEfficiency: MetricCardFlowEfficiencySettings
  speeding: MetricCardSpeedingSettings
}

input MetricCardCustomSettingsInput {
  returns: TicketReturnSettingsInput
  flowEfficiency: MetricCardFlowEfficiencySettingsInput
  speeding: MetricCardSpeedingSettingsInput
}

type MetricCardFlowEfficiencySettings @notNormalised {
  """stored settings in the metric card for Flow Efficiency metrics -> list of statuses that we consider active"""
  activeStatusKeys: [KeyString!]!
}

input MetricCardFlowEfficiencySettingsInput {
  """stored settings in the metric card for Flow Efficiency metrics -> list of statuses that we consider active"""
  activeStatusKeys: [KeyString!]!
}

type MetricCardSpeedingSettings @notNormalised {
  """stored settings in the metric card for Speeding Tickets / Speeding Transitions metrics -> max amount of seconds that a transition can take to be considering to be speeding"""
  maxSeconds: Int!
}

input MetricCardSpeedingSettingsInput {
  """stored settings in the metric card for Speeding Tickets / Speeding Transitions metrics -> max amount of seconds that a transition can take to be considering to be speeding"""
  maxSeconds: Int!
}

type MetricDashboard {
  """clientKey--dataSetKey--dashboardKey"""
  id: ID!
  clientKey: KeyString!
  dataSetKey: KeyString!
  dashboardKey: KeyString!
  label: String!
  """determines if the dashboard is independent or if it is dependent of a Template"""
  type: MetricDashboardType!
  """determines if a dashboard is visible in the UI"""
  visibility: DashboardVisibility!
  """short description of the dashboard if present, or of the template otherwise"""
  shortDescription: String!
  """FK to the MetricDashboardTemplate, if it exists"""
  templateId: KeyString
  template: MetricDashboardTemplate
  """
  id of the cardList that this MetricDashboard has.
  It should be like `md--dashboardID`
  """
  cardListId: KeyString!
  """
  either cardList of the dashboard if exists,
  or the card list of the Template if it doesn't
  """
  cardList: CardList!
}

type MetricDashboardTemplate {
  """
  __general__--dashboardKey (if templateScope is GLOBAL)
  OR
  clientKey--dashboardKey (if templateScope is CLIENT)
  """
  id: ID!
  """specifies if the template is a Plandek Template (GLOBAL) or a client template"""
  templateScope: DashboardTemplateScope!
  """only available if templateScope is CLIENT"""
  clientKey: KeyString
  """dashboardKey that the dashboards created from this template would have"""
  dashboardKey: KeyString!
  """label that the dashboards created from this template would have"""
  label: String!
  """description of the template"""
  shortDescription: String!
  """
  FK of the card list that we have
  should be like `mdt--templateID`
  """
  cardListId: KeyString!
}

type MetricDashboardTemplateForDataSet {
  """clientKey--dataSetKey--dashboardKey"""
  id: ID!
  clientKey: KeyString!
  dataSetKey: KeyString!
  """dashboardKey that the dashboards created from this template would have"""
  dashboardKey: KeyString!
  """MetricDashboardTemplate.id"""
  templateId: KeyString!
  """MetricDashboardTemplate object"""
  template: MetricDashboardTemplate!
  """tells if dashboard is present"""
  hasDashboard: Boolean!
}

enum MetricDashboardType {
  FROM_TEMPLATE
  INDEPENDENT
}

type MetricDataConfig @notNormalised {
  """copied from metric card"""
  id: ID!
  """copied from metric card"""
  cardListId: KeyString!
  """copied from metric card"""
  cardKey: KeyString!
  """copied from metric card"""
  clientKey: KeyString!
  """copied from metric card"""
  dataSetKey: KeyString!
  """copied from metric card"""
  dashboardKey: KeyString!
  metricRole: MetricRole!
  """string to show as the metric label/title"""
  label: String!
  """list of entities for which this Data Set has a NONE"""
  dataSetForbiddenEntities: [EntityType!]!
  metricKey: MetricKey!
  metricConfig: ClientMetricConfig!
  """optional percentileRange to be applied to this metricCard only when calculating filters"""
  percentileRange: PercentileRange
  """
  optional sprintKey override to be applied to this metricCard only.
  It can accept the magic ones:
  - `_current_sprint_`
  - `_last_sprint_`
  """
  sprintKeyOverride: KeyString
  """optional SprintDateRangeField override to be applied to this metricCard only."""
  sprintDateRangeFieldOverride: SprintDateRangeField
  """optional extra set of entitySet filters to refine the dataSet's entitySet."""
  entitySetRefinement: EntitySet!
  """optional extra set of entitySet filters to refine the dataSet's entitySet."""
  entityLabelSetRefinement: EntityLabelSet!
  """Determines which field is used as basis for the calculation (key). If blank, it will use the default of the metric"""
  valueConfigKey: KeyString
  """ValueConfig of the `valueConfigKey`, or the default of the metric if that is blank."""
  valueConfig: ValueConfig!
  breakdown: KeyString
  """breakdownConfig of the given `breakdown`"""
  breakdownConfig: FilterConfig
  """breakdownConfig if present, otherwise use the metric-config's defaultBreakdownConfigForDetails"""
  breakdownConfigForDetails: FilterConfig
  """filters: list of custom filters (strings) to be applied in the requests"""
  multiFilters: [MultiFilter!]!
  """multiFilters with expansion to include the FilterConfig + the labels of the filtered keys"""
  expandedMultiFilters: [MultiFilterWithLabels!]!
  """filters: list of custom filters (boolean) to be applied in the requests"""
  boolFilters: [BoolFilter!]!
  """boolFilters with expansion to include the FilterConfig + the labels of the filtered keys"""
  expandedBoolFilters: [BoolFilterWithLabels!]!
  """list of extra range filters to apply when filtering the tickets to take into account"""
  rangeFilters: [RangeFilter!]!
  """rangeFilters with expansion to include the FilterConfig"""
  expandedRangeFilters: [RangeFilterWithLabels!]!
  customSettings: MetricCardCustomSettings
  averageTimeSeriesSettings: MetricCardAverageTimeSeriesSettings
  """response of this metric data config"""
  response(input: GenericMetricInputMin!): GenericMetricResponse
}

type MetricInsightDriver @notNormalised {
  metricKey: CiMetric!
  valueConfig: ValueConfig!
  breakdown: BreakdownInfo!
  deltasList: [NumberDeltaSummary!]!
}

type MetricInsightSummary @notNormalised {
  delta: NumberDeltaSummary!
  metricKey: CiMetric!
  valueConfig: ValueConfig!
}

"""enum of all possible Metrics, effectively a union of other enums"""
enum MetricKey {
  """of the sprints that take place on the given time range, sum the value (SP by default) of each completed ticket in the sprint and divide by the sprint.teamSize (0 if sprint.teamSize is 0)"""
  SPRINT_VALUE_PER_TEAM_SIZE
  """Ticket scope"""
  TICKET_SCOPE
  """ticket timeline -> value = sum of story points of the tickets existing"""
  TICKET_SCOPE_STORY_POINTS
  """Ticket timeline"""
  TICKET_TIMELINE
  """ticket timeline -> value = sum of story points of tickets existing"""
  TICKET_TIMELINE_STORY_POINTS
  """Build count"""
  BUILD_COUNT
  """
  Percentage of builds whose status is not "success"
  """
  BUILD_FAILURE_RATE
  """Total Build Cost"""
  TOTAL_BUILD_COST
  """Mean Build Cost"""
  MEAN_BUILD_COST
  """Mean Build duration in seconds (average of project build time)"""
  MEAN_BUILD_TIME_BY_PROJECT
  """Mean time between build failures (summary value is an average of project values)"""
  MEAN_TIME_BETWEEN_BUILD_FAILURES_BY_PROJECT
  """Mean time between build failures and first successful build (summary value is an average of project values)"""
  MEAN_TIME_TO_RECOVER_BUILD_FAILURES_BY_PROJECT
  """Return Tickets -> value is number of tickets that were returned"""
  RETURN_TICKETS_COUNT
  """Return Tickets -> value is sum of story points of tickets that were returned"""
  RETURN_TICKETS_STORY_POINTS
  """
  Return Rate ->
  Rate (%) = 100 * (RETURNED_TRANSITIONS / COMPLETED_TICKETS)
  (denominator) COMPLETED_TICKETS => Tickets that were completed in the given time range
  (numerator) RETURNED_TRANSITIONS => Transitions of the tickets in COMPLETED_TICKETS (at any point in time) that are considered Returns
  note: this rate is a division of TRANSITIONS over TICKETS, which means it can be more than 100%.
  """
  RETURN_RATE
  """First Time Yield Rate -> value is % of completed tickets that never were returned"""
  FIRST_TIME_YIELD_RATE
  """First Time Yield Tickets -> value is number of tickets that were returned"""
  FIRST_TIME_YIELD_TICKETS_COUNT
  """First Time Yield Tickets -> value is sum of story points of tickets that were completed and never returned"""
  FIRST_TIME_YIELD_TICKETS_STORY_POINTS
  """
  unresolved tickets -> (combined) -> value = depends on valueConfig (default ticketCount)
  
  - alphanumeric valueConfig => count distinct
  - numeric valueConfig => sum
  """
  UNRESOLVED_TICKETS
  """unresolved tickets -> value = count of tickets"""
  UNRESOLVED_TICKETS_COUNT
  """unresolved tickets -> value = sum of ticket's story points"""
  UNRESOLVED_TICKETS_STORY_POINTS
  """
  makes the Response about the OVERALL
  makes ticketValueDetail the percentage of completedTickets / allTickets
  makes storyPointValueDetail the percentage of completedStoryPoints / allStoryPoints
  """
  SPRINT_COMPLETION_OVERALL
  """
  makes the Response about the TARGET
  makes ticketValueDetail the percentage of completedTickets / allTickets
  makes storyPointValueDetail the percentage of completedStoryPoints / allStoryPoints
  """
  SPRINT_COMPLETION_TARGET
  """
  makes the Response about the SCOPE
  makes ticketValueDetail the percentage of completedTickets / allTickets
  makes storyPointValueDetail the percentage of completedStoryPoints / allStoryPoints
  """
  SPRINT_COMPLETION_ADDED
  """from CommitsMetric > COMMITS_COUNT"""
  COMMITS_COUNT
  """from PullRequestsMetric > CREATED_PULL_REQUESTS"""
  CREATED_PULL_REQUESTS
  """from FlowEfficiencyMetric > FLOW_EFFICIENCY"""
  FLOW_EFFICIENCY
  """from DeliveryTimeMetric > DELIVERY_TIME"""
  DELIVERY_TIME
  """from CodeCycleTimeMetric > CODE_CYCLE_TIME"""
  CODE_CYCLE_TIME
  """
  completed unreleased tickets -> (combined) -> value = depends on valueConfig (default ticketCount)
  
  - alphanumeric valueConfig => count distinct
  - numeric valueConfig => sum
  """
  COMPLETED_UNRELEASED_TICKETS
  """completed unreleased tickets -> value = count of tickets"""
  COMPLETED_UNRELEASED_TICKETS_COUNT
  """completed unreleased tickets -> value = sum of ticket's story points"""
  COMPLETED_UNRELEASED_TICKETS_STORY_POINTS
  """speeding tickets (tickets with at least one speeding transition) -> value = count of tickets"""
  SPEEDING_TICKETS_COUNT
  """speeding tickets (tickets with at least one speeding transition) -> value = sum of ticket's story points"""
  SPEEDING_TICKETS_STORY_POINTS
  """
  speeding transitions
  rate from
    the number of transitions from the input (in time range and with filters) that were speeding according to the input
      divided by
    the number of transitions from the input (in time range and with filters))
  -> value = rate of transitions (from the input) that were speeding among all the transisions (from the input)
  """
  SPEEDING_TRANSITIONS_RATE
  """
  from TicketsMetric > completed tickets -> (combined) -> value = depends on valueConfig (default ticketCount)
  
  - alphanumeric valueConfig => count distinct
  - numeric valueConfig => sum
  """
  COMPLETED_TICKETS
  """from TicketsMetric > completed tickets -> value = count of tickets"""
  COMPLETED_TICKETS_COUNT
  """from TicketsMetric > completed tickets -> value = sum of ticket's story points"""
  COMPLETED_TICKETS_STORY_POINTS
  """
  from TicketsMetric > created tickets -> (combined) -> value = depends on valueConfig (default ticketCount)
  
  - alphanumeric valueConfig => count distinct
  - numeric valueConfig => sum
  """
  CREATED_TICKETS
  """from TicketsMetric > created tickets -> value = count of tickets"""
  CREATED_TICKETS_COUNT
  """from TicketsMetric > created tickets -> value = sum of ticket's story points"""
  CREATED_TICKETS_STORY_POINTS
  """from TicketsMetric > time to value days -> value = time between ticket creation and deployment in days"""
  TIME_TO_VALUE_DAYS
  """from TicketsMetric > complexity of a ticket based on its commits"""
  TICKET_COMPLEXITY
  TICKET_COMMIT_HOTSPOTS
  """from CommitPercentageMetric > percentage of the commits without a pull request (100 * commitsWithoutPR / allCommits). Value is percentage of commits count"""
  COMMIT_WITHOUT_PULL_REQUEST_PERCENTAGE_COUNT
  """from CommitPercentageMetric > percentage of the commits without ticket reference (100 * commitsWithoutTR / allCommits). Value is percentage of commits count"""
  COMMIT_WITHOUT_TICKET_REF_PERCENTAGE_COUNT
  """Mean time between PR creation and closure (or current time)"""
  MEAN_PULL_REQUEST_TIME_TO_RESOLVE_BY_REPO
  """Files most often associated with commits linked to failed builds"""
  TREEMAP_FLAKIEST_FILES
  """build count -> value = number of builds"""
  PIPELINE_BUILD_COUNT
  """time between builds -> value = duration of elapsed time between builds (in days)"""
  PIPELINE_BUILD_INTERVAL
  """build frequency -> value = duration to show how often a single build is made (X times per day)"""
  PIPELINE_BUILD_FREQUENCY
  """average lead time -> value = duration to show how long builds take from start to finish"""
  PIPELINE_BUILD_LEAD_TIME_MEAN
  """Knowledge Graph Visualisation for PR reviews"""
  CODE_KNOWLEDGE
}

enum MetricRole {
  PRIMARY
  SECONDARY
}

type MonitoredService {
  id: ID!
  clientKey: KeyString!
  monitoredServiceUri: ResourceURI!
  name: String!
}

"""(TYPE) common type to represent a MultiFilter (key -> array of values)"""
type MultiFilter @notNormalised {
  """key (same in FilterConfig.key) of the filter to apply"""
  key: KeyString!
  """list of values to filter"""
  values: [String!]!
}

"""(INPUT) common type to represent a MultiFilter (key -> array of values)"""
input MultiFilterInput {
  """key (same in FilterConfig.key) of the filter to apply"""
  key: KeyString!
  """list of values to filter"""
  values: [String!]!
}

"""
common type to represent a MultiFilter where the values are k-label pairs)
key -> array of values (k-v pairs)
"""
type MultiFilterWithLabels @notNormalised {
  """key (same in FilterConfig.key) of the filter to apply"""
  key: KeyString!
  """label (same in FilterConfig.label) of the filter to apply"""
  label: String!
  """value type (same in FilterConfig.valueType) of the filter to apply"""
  valueType: FilterValueType!
  """filter type (same in FilterConfig.filterType) of the filter to apply"""
  filterType: FilterType!
  """list of values to filter, in key-label objects"""
  values: [KeyLabel!]!
}

type Mutation {
  """
  changes the average time series settings for the metric card
  - if the dashboard does not exist, errors
  - if the card does not exist, errors
  - if the dashboard is not independent and it depends on a template, it will fork it before applying the change
  - it will change the card from the dashboard and return the card
  """
  changeCardAverageTimeSeriesSettings(clientKey: KeyString!, dataSetKey: KeyString!, dashboardKey: KeyString!, cardKey: KeyString!, averageTimeSeriesSettings: MetricCardAverageTimeSeriesSettingsInput!): Card!
  """
  changes the custom settings for the metric card
  - if the dashboard does not exist, errors
  - if the card does not exist, errors
  - if the customSettings is not valid (as in, has settings for more than one metric, or the settings are not compatible with the card's metric), errors
  - if the dashboard is not independent and it depends on a template, it will fork it before applying the change
  - it will change the card from the dashboard and return the card
  """
  changeCardCustomSettings(clientKey: KeyString!, dataSetKey: KeyString!, dashboardKey: KeyString!, cardKey: KeyString!, customSettings: MetricCardCustomSettingsInput!): Card!
  """
  changes the chartType, chartTab and breakdown settings of the given card from the given dashboard.
  - if the dashboard does not exist, errors
  - if the card does not exist, errors
  - if the dashboard is not independent and it depends on a template, it will fork it before applying the change
  - it will change the card from the dashboard and return the card
  """
  changeCardDashboardSettings(clientKey: KeyString!, dataSetKey: KeyString!, dashboardKey: KeyString!, cardKey: KeyString!, cardDashboardSettingsInput: CardDashboardSettingsInput!): Card!
  """
  changes the enabledBreakdowns settings of the given card from the given dashboard.
  - if the dashboard does not exist, errors
  - if the card does not exist, errors
  - if the dashboard is not independent and it depends on a template, it will fork it before applying the change
  - it will change the card from the dashboard and return the card
  """
  changeCardEnabledBreakdowns(clientKey: KeyString!, dataSetKey: KeyString!, dashboardKey: KeyString!, cardKey: KeyString!, enabledBreakdowns: KeySet!): Card!
  """
  changes the entities keySets, multiFilters and boolFilters settings of the given card from the given dashboard.
  - if the dashboard does not exist, errors
  - if the card does not exist, errors
  - if the dashboard is not independent and it depends on a template, it will fork it before applying the change
  - it will change the card from the dashboard and return the card
  """
  changeCardFilterSettings(clientKey: KeyString!, dataSetKey: KeyString!, dashboardKey: KeyString!, cardKey: KeyString!, cardFiltersInput: CardFiltersInput!): Card!
  """
  changes the overlays of the given card from the given dashboard.
  - if the dashboard does not exist, errors
  - if the card does not exist, errors
  - if the dashboard is not independent and it depends on a template, it will fork it before applying the change
  - it will change the card from the dashboard and return the card
  """
  changeCardOverlays(clientKey: KeyString!, dataSetKey: KeyString!, dashboardKey: KeyString!, cardKey: KeyString!, overlays: [OverlayInput!]!): Card!
  """
  changes the percentileRange of the given card from the given dashboard.
  - if the dashboard does not exist, errors
  - if the card does not exist, errors
  - if the dashboard is not independent and it depends on a template, it will fork it before applying the change
  - it will change the card from the dashboard and return the card
  """
  changeCardPercentileRange(clientKey: KeyString!, dataSetKey: KeyString!, dashboardKey: KeyString!, cardKey: KeyString!, percentileRange: PercentileRangeInput!): Card!
  """
  changes the sprintKeyOverride of the given card from the given dashboard.
  - if the dashboard does not exist, errors
  - if the card does not exist, errors
  - if the dashboard is not independent and it depends on a template, it will fork it before applying the change
  - it will change the card from the dashboard and return the card
  """
  changeCardSprintKeyOverride(clientKey: KeyString!, dataSetKey: KeyString!, dashboardKey: KeyString!, cardKey: KeyString!, sprintKeyOverride: KeyString): Card!
  """update the field to use as `to` of the Sprints' DateRange"""
  changeSprintDateRangeField(clientKey: KeyString!, field: SprintDateRangeField!): Client!
  """changes the visibility of the metric dashboard"""
  changeVisibilityMetricDashboard(clientKey: KeyString!, dataSetKey: KeyString!, dashboardKey: KeyString!, visibility: DashboardVisibility!): MetricDashboard!
  """
  it will flush all the cache
  REQUIRES BASIC AUTH
  """
  clearCache: String!
  """
  it will flush a single client's cache if it exists
  REQUIRES BASIC AUTH
  """
  clearClientCache(clientKey: KeyString!): String!
  """
  it will flush all the cache of permissions
  REQUIRES BASIC AUTH
  """
  clearPermissionsCache: String!
  """
  it will flush a single users's permissions cache if it exists
  REQUIRES BASIC AUTH
  """
  clearUserPermissionsCache(email: String!): String!
  """
  it will create a dataSetKey and id from the label, ensuring that they are unique for the client, save it and return it
  note: if the given data set is a Super Data Set it will clear the abilities cache
  """
  createDataSet(clientKey: KeyString!, input: DataSetInput!): DataSet!
  """
  if there is a category with the same label already, return that existing label
  otherwise create a new one, with categoryKey and id computed, and return it
  """
  createDataSetCategory(clientKey: KeyString!, label: String!): DataSetCategory!
  """
  creates a new card in the given dashboard, using the given label and metric. It will be added at the end of the list, with a cardKey based on the label
  - if the dashboard does not exist, errors
  - if the dashboard is not independent and it depends on a template, it will fork it before applying the change
  - it will create the card and return the modified dashboard
  """
  createMetricCard(clientKey: KeyString!, dataSetKey: KeyString!, dashboardKey: KeyString!, label: String!, metricKey: MetricKey!, percentileRange: PercentileRangeInput): MetricDashboard!
  """creates a blank metric dashboard"""
  createMetricDashboard(clientKey: KeyString!, dataSetKey: KeyString!, label: String!, shortDescription: String!): MetricDashboard!
  """
  if the template does not exist, bangs
  if the metric dashboard already exists, it will return it
  """
  createMetricDashboardFromTemplate(
    clientKey: KeyString!
    dataSetKey: KeyString!
    templateId: KeyString!
    """if blank, use the one of the template"""
    label: String
    """if blank, use the one of the template"""
    shortDescription: String
  ): MetricDashboard!
  """
  If any of the templates don't exist, bangs
  For each template, it will check i fthe metric dashboard already exists:
  - if the metric dashboard already exists, it will return it
  - otherwise it will create it, using the default label and shortDescription
  It will return the list of dashboards
  """
  createMetricDashboardListFromTemplates(clientKey: KeyString!, dataSetKey: KeyString!, templateIds: [KeyString!]!): [MetricDashboard!]!
  """
  removes the given cardKey from the given dashboard.
  - if the dashboard does not exist, errors
  - if the card does not exist, returns the dashboard as is
  - if the dashboard is not independent and it depends on a template, it will fork it before applying the change
  - it will remove the card from the dashboard and return the dashboard
  """
  deleteCard(clientKey: KeyString!, dataSetKey: KeyString!, dashboardKey: KeyString!, cardKey: KeyString!): MetricDashboard!
  """
  If the data set does not exist, it will return false
  If the data set exists, it will remove it as well as all its dashboards
  note: the DataSet `all` cannot be deleted
  note: if the given data set existed and was a Super Data Set it will clear the abilities cache
  """
  deleteDataSet(clientKey: KeyString!, dataSetKey: KeyString!): Boolean!
  """
  If the data set category does not exist, it will return false
  If the data set category exists and has some data sets, it will error
  If the data set category exists and has no data sets (it's empty), it will delete it and return true
  """
  deleteDataSetCategory(clientKey: KeyString!, categoryKey: KeyString!): Boolean!
  """
  if the dashboard exists, deletes it and returns true
  otherwise it returns false
  """
  deleteMetricDashboard(clientKey: KeyString!, dataSetKey: KeyString!, dashboardKey: KeyString!): Boolean!
  """
  if the dashboard template exists and can be deleted, deletes it and returns true
  otherwise it returns false
  """
  deleteMetricDashboardTemplate(clientKey: KeyString!, templateId: KeyString!): Boolean!
  """it will remove the variants for the given board, effectively reverting back to the client level mappings for this board"""
  deleteStatusBucketVariants(clientKey: String!, boardKey: String!): Boolean!
  """
  if the template does not exist, does nothing and returns []
  if the template exists, but it is global or it is for a different client, then bangs
  if the template exists and it is for the given client, then:
  - clone the template cardList into each of the dashboards that depend on this template
  - unlink those dashboards with this template
  - delete this template
  - return the list of modified dashboards
  """
  demoteMetricDashboardTemplate(clientKey: KeyString!, templateId: KeyString!): [MetricDashboard!]!
  """returns the given value"""
  mutantPing(value: String!): String!
  """
  if the dashboard does not exist, bangs
  if the dashboard is already a template, then it return the template
  otherwise, it migrates the dashboard to a template and returns the new template
  """
  promoteMetricDashboardToTemplate(clientKey: KeyString!, dataSetKey: KeyString!, dashboardKey: KeyString!): MetricDashboardTemplate!
  """
  Sets the default data set (for the given client and logged user) to null
  
  It will error if there is no logged user or if the client does not exist
  """
  removeDefaultDataSetFor(clientKey: KeyString!): ClientDefaultDataSet!
  """
  renames the given card from the given dashboard.
  - if the dashboard does not exist, errors
  - if the card does not exist, errors
  - if the dashboard is not independent and it depends on a template, it will fork it before applying the change
  - it will rename the card from the dashboard and return the card
  """
  renameCard(clientKey: KeyString!, dataSetKey: KeyString!, dashboardKey: KeyString!, cardKey: KeyString!, newLabel: String!): Card!
  """
  If there is a category with the given client + categoryKey, it renames it with the new label.
  If the new label is already in use for that client, it will fail
  If the category does not exist, it will return null
  """
  renameDataSetCategory(clientKey: KeyString!, categoryKey: KeyString!, newLabel: String!): DataSetCategory
  """changes the label of the metric dashboard"""
  renameMetricDashboard(clientKey: KeyString!, dataSetKey: KeyString!, dashboardKey: KeyString!, newLabel: String!): MetricDashboard!
  """
  changes the order of the cards in the given dashboard following the order of the keys in the given array
  - it will return an error if the dashboard does not exist
  - if the dashboard is not independent and it depends on a template, it will fork it before applying the change
  - it will ignore any unknown card keys in the list
  - it will add any missing card keys at the end, sorted by createdAt ASC
  """
  reorderMetricDashboard(clientKey: KeyString!, dataSetKey: KeyString!, dashboardKey: KeyString!, orderedCardKeys: [KeyString!]!): MetricDashboard!
  """
  rensizes the given card from the given dashboard.
  - if the dashboard does not exist, errors
  - if the card does not exist, errors
  - if the dashboard is not independent and it depends on a template, it will fork it before applying the change
  - it will resize the card from the dashboard and return the card
  """
  resizeCard(clientKey: KeyString!, dataSetKey: KeyString!, dashboardKey: KeyString!, cardKey: KeyString!, newSize: CardSizeType!): Card!
  """
  Sets the default data set (for the given client and logged user) to the given DataSetKey
  
  It will error if there is no logged user or if the client does not exist or if the given DataSet does not exist
  """
  setDefaultDataSetFor(clientKey: KeyString!, dataSetKey: KeyString!): ClientDefaultDataSet!
  """
  if the dataSet does not exist, upsert it.
  note: the DataSet `all` cannot be updated
  note: if the given data set is a Super Data Set it will clear the abilities cache
  """
  updateDataSet(clientKey: KeyString!, dataSetKey: KeyString!, input: DataSetInput!): DataSet!
  """
  update metricCard SprintDateRangeFieldOverride of a given card from the given dashboard.
  - if the dashboard does not exist, errors
  - if the card does not exist, errors
  - if the dashboard is not independent and it depends on a template, it will fork it before applying the change
  - it will rename the valueConfig field from the dashboard and return the card
  """
  updateMetricCardSprintDateRangeFieldOverride(clientKey: KeyString!, dataSetKey: KeyString!, dashboardKey: KeyString!, cardKey: KeyString!, sprintDateRangeFieldOverride: SprintDateRangeField): Card!
  """
  update metricCard metricKey and valueConfigKey of a given card from the given dashboard.
  - if the dashboard does not exist, errors
  - if the card does not exist, errors
  - if the dashboard is not independent and it depends on a template, it will fork it before applying the change
  - it will change the metricKey and valueConfig field from the card and return it
  """
  updateMetricCardValueConfigKey(clientKey: KeyString!, dataSetKey: KeyString!, dashboardKey: KeyString!, cardKey: KeyString!, metricKey: MetricKey!, valueConfigKey: KeyString): Card!
  """update the list of StatusBuckets for the specified client"""
  updateStatusBuckets(input: UpdateStatusBucketsInput!): [StatusBucket!]!
  """update the list of StatusBuckets for the specified client"""
  upsertStatusBucketsForBuckets(input: UpdateStatusBucketsInputForBuckets!): [StatusBucket!]!
  """update the list of StatusBuckets for the specified client"""
  upsertStatusBucketsForStatuses(input: UpdateStatusBucketsInputForStatuses!): StatusBucketsForStatusesResponse!
  """update or create a variant of  the list of StatusBuckets for the specified client"""
  upsertStatusBucketsVariant(input: UpsertStatusBucketsVariantInput!): StatusBucketsVariantResponse!
}

type NumberDeltaSummary @notNormalised {
  """identification of the summary (breakdown's bucket key, or _general_)"""
  key: String!
  """optional label to show in the UI for this Summary bucket (blank for _general_)"""
  label: String!
  """delta value of the summary"""
  value: DeltaValue!
  """number of items that this point represents"""
  countItems: DeltaValue!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query) for present"""
  overrideFilters: OverrideFilters!
}

"""represents a single point of a TimeSeries with a single numeric value + size of the elements represented by this point"""
type NumberSizeTimePoint implements TimePoint @notNormalised {
  """date-time of the point"""
  x: JSDate!
  """date-time of the initial moment of the time range represented by this point"""
  fromDate: UTCStartOfDay!
  """date-time of the final moment of the time range represented by this point"""
  toDate: UTCEndOfDay!
  """numeric value of the point"""
  y: Float!
  """numeric value of the point"""
  size: Int!
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

type NumberSizeTimeSeries implements TimeSeries @notNormalised {
  """identification of the Time Series"""
  key: String!
  """optional label to show in the UI for this Time Series (it should override any regular translation-based label)"""
  label: String!
  """sorted list of points"""
  data: [NumberSizeTimePoint!]!
}

type NumberSprintSummary @notNormalised {
  key: String!
  label: String!
  sprint: Sprint!
  teamSize: Float!
  sum: Float!
  value: Float!
  countItems: Float!
  overrideFilters: OverrideFilters!
}

type NumberStatusBucketDeltaSummary @notNormalised {
  """identification of the summary (breakdown's bucket key, or _general_)"""
  key: String!
  """optional label to show in the UI for this Summary bucket (blank for _general_)"""
  label: String!
  """delta value of the summary"""
  value: DeltaValue!
  """number of items that this point represents"""
  countItems: DeltaValue!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

type NumberStatusBucketTimeSeries implements TimeSeries @notNormalised {
  """identification of the Time Series"""
  key: String!
  """optional label to show in the UI for this Time Series (it should override any regular translation-based label)"""
  label: String!
  """position of the status bucket (for ordering)"""
  position: Int!
  """sorted list of points"""
  data: [NumberTimePoint!]!
}

type NumberStatusDeltaSummary @notNormalised {
  """identification of the summary (breakdown's bucket key, or _general_)"""
  key: String!
  """optional label to show in the UI for this Summary bucket (blank for _general_)"""
  label: String!
  """status that this bucket represents"""
  status: Status!
  """delta value of the summary"""
  value: DeltaValue!
  """number of items that this point represents"""
  countItems: DeltaValue!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

type NumberStatusSummary @notNormalised {
  """identification of the summary (breakdown's bucket key, or _general_)"""
  key: String!
  """label to show in the UI for this Summary bucket (blank for _general_)"""
  label: String!
  """status that this bucket represents"""
  status: Status!
  """value of the summary"""
  value: Float!
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

"""represents a single point of a TimeSeries with a single numeric value"""
type NumberStatusTimePoint implements TimePoint @notNormalised {
  """date-time of the point"""
  x: JSDate!
  """date-time of the initial moment of the time range represented by this point"""
  fromDate: UTCStartOfDay!
  """date-time of the final moment of the time range represented by this point"""
  toDate: UTCEndOfDay!
  """numeric value of the point (average)"""
  y: Float!
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

type NumberStatusTimeSeries implements TimeSeries @notNormalised {
  """identification of the Time Series"""
  key: String!
  """label to show in the UI for this Time Series (it should override any regular translation-based label)"""
  label: String!
  """status that this time series references"""
  status: Status!
  """sorted list of points"""
  data: [NumberStatusTimePoint!]!
}

type NumberSummary @notNormalised {
  """identification of the summary (breakdown's bucket key, or _general_)"""
  key: String!
  """label to show in the UI for this Summary bucket (blank for _general_)"""
  label: String!
  """value of the summary"""
  value: Float!
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

"""represents a single point of a TimeSeries with a single numeric value"""
type NumberTimePoint implements TimePoint @notNormalised {
  """date-time of the point"""
  x: JSDate!
  """date-time of the initial moment of the time range represented by this point"""
  fromDate: UTCStartOfDay!
  """date-time of the final moment of the time range represented by this point"""
  toDate: UTCEndOfDay!
  """numeric value of the point"""
  y: FloatRounded2!
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

type NumberTimeSeries implements TimeSeries @notNormalised {
  """identification of the Time Series"""
  key: String!
  """optional label to show in the UI for this Time Series (it should override any regular translation-based label)"""
  label: String!
  """sorted list of points"""
  data: [NumberTimePoint!]!
}

type NumberTimeSeriesWithWindowSize implements TimeSeries @notNormalised {
  """identification of the Time Series"""
  key: String!
  """optional label to show in the UI for this Time Series (it should override any regular translation-based label)"""
  label: String!
  """sorted list of points"""
  data: [NumberTimePoint!]!
  """size of the window used for calculating the points' value"""
  windowSize: Int!
}

input OverlayInput {
  metricKey: MetricKey!
  label: String!
  filters: CardFiltersInput!
  valueConfigKey: KeyString
  customSettings: MetricCardCustomSettingsInput
  percentileRange: PercentileRangeInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
}

"""(INPUT) common set of filters for all metrics that serve to override another FiltersInput"""
type OverrideFilters @notNormalised {
  dateRange: DateRange
  sprintKey: KeyString
  pastSprintKey: KeyString
  dataSetKey: KeyString
  entitySetRefinement: EntitySet
  multiFilters: [MultiFilter!]
  boolFilters: [BoolFilter!]
  rangeFilters: [RangeFilter!]
  scatterMetricInput: ScatterMetricInputType
  """optionally state which items you want as a subset, identified with hitID. Each metric will know which type of items to load"""
  itemIDs: [KeyString!]
  excludeDataSetKeys: [KeyString!]
}

"""(INPUT) common set of filters for all metrics that serve to override another FiltersInput"""
input OverrideFiltersInput {
  dateRange: DateRangeInput
  sprintKey: KeyString
  pastSprintKey: KeyString
  dataSetKey: KeyString
  entitySetRefinement: EntitySetInput
  multiFilters: [MultiFilterInput!]
  boolFilters: [BoolFilterInput!]
  rangeFilters: [RangeFilterInput!]
  scatterMetricInput: ScatterMetricInput
  """optionally state which items you want as a subset, identified with hitID. Each metric will know which type of items to load"""
  itemIDs: [KeyString!]
  """optionally removing children data sets from the calculation (e.g. children of the filtered data set)"""
  excludeDataSetKeys: [KeyString!]
}

input PRDetailInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: CiMetric!
}

type PRStatusDeltaSummary @notNormalised {
  """identification of the summary (breakdown's bucket key, or _general_)"""
  key: KeyString!
  """optional label to show in the UI for this Summary bucket (blank for _general_)"""
  label: String!
  """status that this bucket represents"""
  status: PullRequestStatus!
  """delta value of the summary"""
  value: DeltaValue!
  """delta of numerator of the summary"""
  numerator: DeltaValue!
  """number of items that this point represents"""
  countItems: DeltaValue!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

type PRStatusSummary @notNormalised {
  """identification of the summary (breakdown's bucket key, or _general_)"""
  key: KeyString!
  """label to show in the UI for this Summary bucket (blank for _general_)"""
  label: String!
  """status that this bucket represents"""
  status: PullRequestStatus!
  """value of the summary"""
  value: FloatRounded2!
  """value taken into account (numerator to calculate the average)"""
  numerator: FloatRounded2!
  """optionally return the ids of the items of countItems, useful for the Detail."""
  itemIDs: [KeyString!]
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

"""percentage = 100 * numerator / denominator"""
type PercentageDetail @notNormalised {
  percentage: FloatRounded2
  numerator: Float!
  denominator: Float!
}

type PercentageSummary @notNormalised {
  """identification of the summary (breakdown's bucket key, or _global_)"""
  key: String!
  """optional label to show in the UI for this Summary bucket (blank for _global_)"""
  label: String!
  """files value of the summary, which depends on which Metric do we have"""
  value: FloatRounded2!
  """files value, with the detail (numerator, denominator, percentage)"""
  valueDetail: PercentageDetail!
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

"""like a NumberTimePoint but it has extra percentage time point value"""
type PercentageTimePoint implements TimePoint @notNormalised {
  """date-time of the point"""
  x: JSDate!
  """date-time of the initial moment of the time range represented by this point"""
  fromDate: UTCStartOfDay!
  """date-time of the final moment of the time range represented by this point"""
  toDate: UTCEndOfDay!
  """percentage time point value"""
  y: FloatRounded2!
  """numerator = size, denominator = all items in this bucket, not only the ones filtered by the metric"""
  percentageDetail: PercentageDetail!
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

"""like a ScatterTimeSeries but instead of ticket details it has CommitPercentage over time"""
type PercentageTimeSeries implements TimeSeries @notNormalised {
  """identification of the Time Series"""
  key: String!
  """optional label to show in the UI for this Time Series (it should override any regular translation-based label)"""
  label: String!
  """sorted list of points"""
  data: [PercentageTimePoint!]!
}

"""defines percentiles range from 0 to 100"""
type PercentileRange @notNormalised {
  from: Int!
  to: Int!
}

"""defines percentiles range from 0 to 100"""
input PercentileRangeInput {
  from: Int!
  to: Int!
}

type Person {
  id: ID!
  clientKey: KeyString!
  name: String!
  jobTitle: String
  employmentType: String
}

type PersonRelationshipSummary @notNormalised {
  """fromID--toID"""
  key: KeyString!
  """fromPerson.name -> toPerson.name"""
  label: String!
  """Participant ID who triggered certain event (We only have commented in he metric right now) in the PR that owned by fromID"""
  fromID: KeyString!
  """author's ID"""
  toID: KeyString!
  """Participant's name"""
  fromPerson: Person
  """Author's name"""
  toPerson: Person
  """Number of PRs based on certain event (We only have commented in he metric right now) that has been triggered"""
  value: Int!
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

"""list of available pipeline metric with a single value"""
enum PipelinesMetric {
  """build count -> value = number of builds"""
  PIPELINE_BUILD_COUNT
  """time between builds -> value = duration of elapsed time between builds (in days)"""
  PIPELINE_BUILD_INTERVAL
  """build frequency -> value = duration to show how often a single build is made (X times per day)"""
  PIPELINE_BUILD_FREQUENCY
  """average lead time -> value = duration to show how long builds take from start to finish"""
  PIPELINE_BUILD_LEAD_TIME_MEAN
}

input PipelinesMetricDetailInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: PipelinesMetric!
}

input PipelinesMetricFacetsInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: PipelinesMetric!
}

input PipelinesMetricInput {
  metricKey: PipelinesMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  """granularity for the time series (default to `day` if not specified)"""
  granularity: Granularity
  """
  can be null, to ask for the general result.
  
  If it is not null, it has to be one of the keys of the valid FilterConfig (see FilterConfig)
  
  note that this cannot be an enum because it will depend on the data (custom fields et al)
  """
  breakdown: KeyString
}

type PipelinesMetricInputType @notNormalised {
  metricKey: PipelinesMetric!
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  breakdown: KeyString
}

type PipelinesMetricResponse {
  id: ID!
  metricKey: PipelinesMetric!
  input: PipelinesMetricInputType!
  """summary with breakdown blank (key = _general_)"""
  generalSummary: NumberSummary
  """delta of the generalSummary"""
  generalDelta: NumberDeltaSummary
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one summary per breakdown bucket
  """
  summariesList: [NumberSummary!]!
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one delta summary per breakdown bucket
  if there are no deltas configured, it will be NULL
  """
  deltasList: [NumberDeltaSummary!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value, depending on `metric`
  """
  timeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates. The value of each point will be the cumulative average.
  
  e.g.
  
  ```
  TS0---TS1---TS2---TS3---TS4
  
  ATS0 = TS0
  ATS1 = (TS1 + TS0) / 2
  ATS2 = (TS2 + TS1 + TS0) / 3
  ATS3 = (TS3 + TS2 + TS1 + TS0) / 4
  ATS4 = (TS4 + TS3 + TS2 + TS1 + TS0) / 5
  ```
  """
  averageTimeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates.
  The value of each point will be the cumulative average taking into account only the current value + the previous `n` values (where n is the windowSize)
  It will load the TS of the past filters (same filters used for delta calculation) to get the "previous values" of the initial points
  
  e.g time series with 5 points (TS0 to TS4), windowSize = 2
  
  ```
  PTS0---PTS1---PTS2---PTS3---PTS4---TS0---TS1---TS2---TS3---TS4
  
  RATS0 = (TS0 + PTS4 + PTS3) / 3
  RATS1 = (TS1 + TS0 + PTS4) / 3
  RATS2 = (TS2 + TS1 + TS0) / 3
  RATS3 = (TS3 + TS2 + TS1) / 3
  RATS4 = (TS4 + TS3 + TS2) / 3
  ```
  """
  rollingAverageTimeSeriesList(windowSize: Int, windowSizePercentage: Float): [NumberTimeSeriesWithWindowSize!]!
  """
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value taken from each DeploymentDetail
  The `size` will be the number of builds that the point represents
  The `singleItem` is populated only when size is 1. It contains key, label, link
  """
  scatterTimeSeriesList: [ScatterTimeSeries!]!
}

enum PipelinesValueField {
  LEAD_TIME_SECONDS
}

type Programme {
  """clientKey--programmeKey"""
  id: ID!
  """client_key that this programme belongs to"""
  clientKey: KeyString!
  """programme_key of the Programme."""
  programmeKey: KeyString!
  name: String!
  """list of boardKeys of the boards that belong to this programme"""
  boardKeys: [KeyString!]!
  """list of boards that belong to this programme"""
  boards: [Board!]!
}

type PullRequestDetail {
  """system-wide ID for the commit detail"""
  id: ID!
  """determines the field used to calculate `value`"""
  valueConfigKey: KeyString!
  """value to be used"""
  value: Float
  """hit ID"""
  hitID: ID!
  """person ID who created the PR"""
  personId: ID
  """Author of the Pull Request"""
  person: Person
  """PR ID"""
  prID: KeyString!
  """Client key associated to the PR"""
  clientKey: KeyString!
  """number of comments"""
  commentsCount: Int!
  """SHA of each commit in the PR"""
  commitShas: [KeyString!]!
  """repository name the PR belongs to"""
  repoName: String
  """link to the PR url"""
  webUrl: String
  """state the PR is in"""
  state: String
  """title of the PR"""
  title: String
  """list of tickets associated to the PR"""
  matchedTicketKeys: [KeyString!]!
  """number of tickets associated with the PR"""
  matchedTicketKeysCount: Int!
  """number of inserted lines"""
  insertedLines: Int!
  """number of deleted lines"""
  deletedLines: Int!
  """number of changed files"""
  filesChanged: Int!
  """commit sha name"""
  mergeCommit: KeyString!
  """time when PR is created"""
  prCreatedAt: UTCDate!
  """time when PR is updated"""
  prUpdatedAt: UTCDate
  """time when PR is merged"""
  prMergedAt: UTCDate
  """time when PR is closed"""
  prClosedAt: UTCDate
  """number of seconds took to resolve issues on the PR"""
  timeToResolveSeconds: Int!
  """number of seconds took to finish the PR"""
  durationSeconds: Int!
  """number of seconds the PR stays unresolved"""
  unresolvedDurationSeconds: Int!
  """list of person_id of everyone involved in the PR (including the author)"""
  participantIds: [KeyString!]!
  participants: [Person!]!
  """list of person_id of everyone involved in the PR (excluding the author)"""
  participantNoAuthorIds: [KeyString!]!
  participantsNoAuthor: [Person!]!
}

type PullRequestDetailList {
  id: ID!
  """total quantity of details"""
  count: Int!
  """list of item details, all sharing the same valueConfig to calculate the `value` field"""
  details: [PullRequestDetail!]!
  """key of the valueConfig"""
  valueConfigKey: KeyString!
  """valueConfig used for the `value` field of each detail"""
  valueConfig: ValueConfig!
}

enum PullRequestDetailValueField {
  insertions
  deletions
  filesChanged
}

type PullRequestStatus {
  id: ID!
  statusKey: KeyString!
  label: String!
  position: Int!
}

"""represents a single point of a TimeSeries with a single numeric value"""
type PullRequestStatusTimePoint implements TimePoint @notNormalised {
  """date-time of the point"""
  x: JSDate!
  """date-time of the initial moment of the time range represented by this point"""
  fromDate: UTCStartOfDay!
  """date-time of the final moment of the time range represented by this point"""
  toDate: UTCEndOfDay!
  """numeric value of the point (average)"""
  y: FloatRounded2!
  """value taken into account (numerator to calculate the average)"""
  numerator: FloatRounded2!
  """optionally return the ids of the items of countItems, useful for the Detail."""
  itemIDs: [KeyString!]
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

type PullRequestStatusTimeSeries implements TimeSeries @notNormalised {
  """identification of the Time Series"""
  key: String!
  """label to show in the UI for this Time Series (it should override any regular translation-based label)"""
  label: String!
  """status that this time series references"""
  status: PullRequestStatus!
  """sorted list of points"""
  data: [PullRequestStatusTimePoint!]!
}

"""
represents a single point of a TimeSeries with a list of pull requests for each x + a single numeric value

It can be used for a scatter chart, when the pull requests are divided and each point represents `x` and `y` and the PRs that share the same x and y.
"""
type PullRequestWithKeysNumberTimePoint implements TimePoint @notNormalised {
  """date-time of the point"""
  x: JSDate!
  """date-time of the initial moment of the time range represented by this point"""
  fromDate: UTCStartOfDay!
  """date-time of the final moment of the time range represented by this point"""
  toDate: UTCEndOfDay!
  """numeric value of the point"""
  y: Float!
  """number of items that this point represents"""
  size: Int!
  """list of PR IDs that this point includes"""
  prIDs: [KeyString!]!
  """info about the single item if this point represents a single item (only 1 PR). If size > 1 singleItem will be blank."""
  singleItem: ItemWithLink
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

type PullRequestWithKeysNumberTimeSeries implements TimeSeries @notNormalised {
  """identification of the Time Series"""
  key: String!
  """optional label to show in the UI for this Time Series (it should override any regular translation-based label)"""
  label: String!
  """sorted list of points"""
  data: [PullRequestWithKeysNumberTimePoint!]!
}

"""list of available pull requests metric with a single value"""
enum PullRequestsMetric {
  """completed pull requests -> value = count of pull requests"""
  CREATED_PULL_REQUESTS
}

input PullRequestsMetricDetailInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: PullRequestsMetric!
}

input PullRequestsMetricFacetsInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: PullRequestsMetric!
}

input PullRequestsMetricInput {
  metricKey: PullRequestsMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  """granularity for the time series (default to `day` if not specified)"""
  granularity: Granularity
  """
  can be null, to ask for the general result.
  
  If it is not null, it has to be one of the keys of the valid FilterConfig (see FilterConfig)
  
  note that this cannot be an enum because it will depend on the data (custom fields et al)
  """
  breakdown: KeyString
}

type PullRequestsMetricInputType @notNormalised {
  metricKey: PullRequestsMetric!
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  breakdown: KeyString
}

type PullRequestsMetricResponse {
  id: ID!
  input: PullRequestsMetricInputType!
  metricKey: PullRequestsMetric!
  """summary with breakdown blank (key = _general_)"""
  generalSummary: NumberSummary
  """delta of the generalSummary"""
  generalDelta: NumberDeltaSummary
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one summary per breakdown bucket
  """
  summariesList: [NumberSummary!]!
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one delta summary per breakdown bucket
  if there are no deltas configured, it will be NULL
  """
  deltasList: [NumberDeltaSummary!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value, depending on `metric`
  """
  timeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates. The value of each point will be the cumulative average.
  
  e.g.
  
  ```
  TS0---TS1---TS2---TS3---TS4
  
  ATS0 = TS0
  ATS1 = (TS1 + TS0) / 2
  ATS2 = (TS2 + TS1 + TS0) / 3
  ATS3 = (TS3 + TS2 + TS1 + TS0) / 4
  ATS4 = (TS4 + TS3 + TS2 + TS1 + TS0) / 5
  ```
  """
  averageTimeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates.
  The value of each point will be the cumulative average taking into account only the current value + the previous `n` values (where n is the windowSize)
  It will load the TS of the past filters (same filters used for delta calculation) to get the "previous values" of the initial points
  
  e.g time series with 5 points (TS0 to TS4), windowSize = 2
  
  ```
  PTS0---PTS1---PTS2---PTS3---PTS4---TS0---TS1---TS2---TS3---TS4
  
  RATS0 = (TS0 + PTS4 + PTS3) / 3
  RATS1 = (TS1 + TS0 + PTS4) / 3
  RATS2 = (TS2 + TS1 + TS0) / 3
  RATS3 = (TS3 + TS2 + TS1) / 3
  RATS4 = (TS4 + TS3 + TS2) / 3
  ```
  """
  rollingAverageTimeSeriesList(windowSize: Int, windowSizePercentage: Float): [NumberTimeSeriesWithWindowSize!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value taken from each PullRequestDetail
  The `size` will be the number of pull requests that the point represents
  The `pull requests` list will be the PullRequestDetail of the pull requests that the point represents
  """
  scatterTimeSeriesList: [ScatterTimeSeries!]!
}

type Query {
  CiMetricDeltaInsight(input: CiMetricInsightInput!): CiMetricInsightResponse!
  """returns the list of key-label objects defining all boards available for the user in this client, for editing/creating a data set"""
  availableBoardsKeyLabels(clientKey: KeyString!): [KeyLabel!]!
  """returns the list of key-label objects defining all CI Pipelines available for the user in this client, for editing/creating a data set"""
  availableCIPipelinesKeyLabels(clientKey: KeyString!): [KeyLabel!]!
  """returns the list of key-label objects defining all Monitored Services available for the user in this client, for editing/creating a data set"""
  availableMonitoredServicesKeyLabels(clientKey: KeyString!): [KeyLabel!]!
  """returns the list of key-label objects defining all people available for the user in this client, for editing/creating a data set"""
  availablePeopleKeyLabels(clientKey: KeyString!): [KeyLabel!]!
  """returns the list of key-label objects defining all repos available for the user in this client, for editing/creating a data set"""
  availableRepositoriesKeyLabels(clientKey: KeyString!): [KeyLabel!]!
  """load all available sprints for a specific DataSet"""
  availableSprints(filters: FiltersOnlyDataSetAndEntitiesInput!, sprintDateRangeFieldOverride: SprintDateRangeField): [Sprint!]! @cacheControl(maxAge: 300, scope: PRIVATE)
  """get a single board by clientKey and boardKey"""
  board(clientKey: KeyString!, boardKey: KeyString!): Board @cacheControl(maxAge: 300, scope: PRIVATE)
  """get the list of available boards"""
  boards(clientKey: KeyString!): [Board!]! @cacheControl(maxAge: 300, scope: PRIVATE)
  """load the detail data (list of builds) for the given metric"""
  ciBuildsDetail(input: CiBuildsDetailInput!): CiBuildDetailList! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["ci-metrics-builds"])
  """load the data for the given Metric (summaries, deltas, time-series)"""
  ciMetric(input: CiMetricInput!): CiMetricResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["ci-metrics-pr", "ci-metrics-builds"])
  """load the allowed filter configs (for ExtraFilters) and their facet values."""
  ciMetricFacets(input: CiMetricFacetsInput!): [FilterConfigWithFacets!]! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["ci-metrics-pr", "ci-metrics-builds"])
  """get a list of ci-pipelines by clientKey"""
  ciPipelines(clientKey: KeyString!, uris: [ResourceURI!]): [CiPipeline!]! @cacheControl(maxAge: 300, scope: PRIVATE)
  """
  get a single client by id (clientKey)
  
  if forceFresh is true, it will ensure it doesn't get the cached value
  """
  client(clientKey: KeyString!, forceFresh: Boolean): Client
  """get the list of available clients"""
  clients: [Client!]! @cacheControl(maxAge: 300, scope: PRIVATE)
  codeCycleTimeDetails(input: CodeCycleTimeDetailInput!): FilteredPullRequestDetailWithStatusesList! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["code-cycle-time"])
  """facets for Code Cycle Time"""
  codeCycleTimeFacets(input: CodeCycleTimeFacetsInput!): [FilterConfigWithFacets!]! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["code-cycle-time"])
  """codeCycleTimeMetric version for no breakdown or for breakdown by a valueType !== PRStatus"""
  codeCycleTimeMetric(input: CodeCycleTimeInput!): CodeCycleTimeResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["code-cycle-time"])
  """codeCycleTimeMetric version for breakdown by a valueType === PRStatus"""
  codeCycleTimeMetricByStatus(input: CodeCycleTimeByStatusInput!): CodeCycleTimeByStatusResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["code-cycle-time"])
  """
  load the data for the given Metric (summaries, deltas)
  This metric will show us different interactions (events [comments, approvals, merges, etc....]) from different participants in PRs
  """
  codeKnowledgeMetric(input: CodeKnowledgeMetricInput!): CodeKnowledgeMetricResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["code-knowledge"])
  codeKnowledgeMetricDetails(input: CodeKnowledgeMetricDetailsInput!): PullRequestDetailList! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["code-knowledge"])
  """load the allowed filter configs (for ExtraFilters) and their facet values."""
  codeKnowledgeMetricFacets(input: CodeKnowledgeMetricFacetsInput!): [FilterConfigWithFacets!]! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["code-knowledge"])
  """load the data for the given Metric (summaries)"""
  commitPercentageMetric(input: CommitPercentageMetricInput!): CommitPercentageMetricResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["commit-percentage-metrics"])
  commitPercentageMetricDetails(input: CommitPercentageMetricDetailsInput!): CommitDetailList! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["commit-percentage-metrics"])
  commitPercentageMetricFacets(input: CommitPercentageMetricFacetsInput!): [FilterConfigWithFacets!]! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["commit-percentage-metrics"])
  """load the data for the given Metric (summaries, deltas, time-series)"""
  commitsMetric(input: CommitsMetricInput!): CommitsMetricResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["commits-metrics"])
  """load the detail data (list of commits) for the given metric"""
  commitsMetricDetails(input: CommitsMetricDetailInput!): CommitDetailList! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["commits-metrics"])
  """load the allowed filter configs (for ExtraFilters) and their facet values."""
  commitsMetricFacets(input: CommitsMetricFacetsInput!): [FilterConfigWithFacets!]! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["commits-metrics"])
  """Get current sprint by FiltersInput"""
  currentSprint(filters: FiltersInput!, sprintDateRangeFieldOverride: SprintDateRangeField): Sprint @cacheControl(maxAge: 300, scope: PRIVATE)
  """returns the dataset if it exists"""
  dataSet(clientKey: KeyString!, dataSetKey: KeyString!): DataSet
  """returns the list of categories with their data sets for a given client"""
  dataSetCategories(clientKey: KeyString!): [DataSetCategory!]!
  """returns an object with the `all` data set if available as well as the list of categories"""
  dataSetTree(clientKey: KeyString!): DataSetTree!
  """
  returns the an object describing the default DataSet to be used for the given client for the logged user
  
  It will error if there is no logged user or if the client does not exist
  """
  defaultDataSetFor(clientKey: KeyString!): ClientDefaultDataSet!
  deliveryTimeDetails(input: DeliveryTimeDetailInput!): FilteredTicketDetailWithStatusesList! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["delivery-time"])
  """facets for Delivery Time"""
  deliveryTimeFacets(input: DeliveryTimeFacetsInput!): [FilterConfigWithFacets!]! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["delivery-time"])
  """deliveryTimeMetric version for no breakdown or for breakdown by a valueType !== Status"""
  deliveryTimeMetric(input: DeliveryTimeInput!): DeliveryTimeResponse! @cacheControl(maxAge: 300, scope: PRIVATE)
  """deliveryTimeMetric version for breakdown by a valueType === Status / PlandekStatus"""
  deliveryTimeMetricByStatus(input: DeliveryTimeByStatusInput!): DeliveryTimeByStatusResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["delivery-time"])
  """deliveryTimeMetric version for breakdown by a valueType === StatusBucket"""
  deliveryTimeMetricByStatusBucket(input: DeliveryTimeByStatusBucketInput!): DeliveryTimeByStatusBucketResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["delivery-time"])
  fakeUser(id: ID!): UserProfile!
  """
  load the data for the given Metric (summaries, deltas, time-series)
  rate of FIRST_TIME_YIELD_TICKETS_COUNT / COMPLETED_TICKETS
  """
  firstTimeYieldRateMetric(input: FirstTimeYieldRateMetricInput!): FirstTimeYieldRateMetricResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["first-time-yield-rate"])
  firstTimeYieldRateMetricDetails(input: FirstTimeYieldRateMetricDetailsInput!): ReturnedTicketDetailList! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["first-time-yield-rate"])
  """facets for: tickets that were completed in the given filters and were never returned at any point in time (even outside of the given dateRange)"""
  firstTimeYieldTicketsFacets(input: FirstTimeYieldTicketsFacetsInput!): [FilterConfigWithFacets!]! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["first-time-yield-tickets"])
  firstTimeYieldTicketsMetric(input: FirstTimeYieldTicketsMetricInput!): FirstTimeYieldTicketsMetricResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["first-time-yield-tickets"])
  firstTimeYieldTicketsMetricDetails(input: FirstTimeYieldTicketsMetricDetailsInput!): TicketDetailList! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["first-time-yield-tickets"])
  """will use the facets for delivery time internally"""
  flowEfficiencyDetails(input: FlowEfficiencyDetailInput!): FilteredTicketDetailWithStatusesList! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["flow-efficiency"])
  """will use the facets for delivery time internally"""
  flowEfficiencyFacets(input: FlowEfficiencyFacetsInput!): [FilterConfigWithFacets!]! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["flow-efficiency"])
  flowEfficiencyMetric(input: FlowEfficiencyInput!): FlowEfficiencyResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["flow-efficiency"])
  flowEfficiencyMetricByStatusBucket(input: FlowEfficiencyInputByStatusBucket!): FlowEfficiencyByStatusBucketResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["flow-efficiency"])
  """TBC"""
  genericMetricResponse(input: GenericMetricInput!): GenericMetricResponse
  """get a list of the available integration classes for the client (looked up from the data)"""
  integrationClasses(clientKey: KeyString!): [IntegrationClass!]! @cacheControl(maxAge: 300, scope: PRIVATE)
  """get a list of issueTypes by clientKey and keys"""
  issueTypes(clientKey: KeyString!, keys: [KeyString!]): [IssueType!]! @cacheControl(maxAge: 300, scope: PRIVATE)
  """Get last sprint by FiltersInput"""
  lastSprint(filters: FiltersInput!, sprintDateRangeFieldOverride: SprintDateRangeField): Sprint @cacheControl(maxAge: 300, scope: PRIVATE)
  """get a single card of a single metric dashboard for the given data set if it exists"""
  metricCard(clientKey: KeyString!, dataSetKey: KeyString!, dashboardKey: KeyString!, cardKey: KeyString!): MetricCard
  """
  retrieves the metriConfig of the given metricKey for the given client and optionally also data set.
  - if the dataSetKey is sent but does not exist, it will return an error
  """
  metricConfig(clientKey: KeyString!, metricKey: MetricKey!, dataSetKey: KeyString!): ClientMetricConfig! @cacheControl(maxAge: 300, scope: PRIVATE)
  """
  retrieves the metriConfig of the given metricKey for the given client and optionally also data set.
  - if the dataSetKey is sent but does not exist, it will return an error
  """
  metricConfigs(clientKey: KeyString!, dataSetKey: KeyString!): [ClientMetricConfig!]! @cacheControl(maxAge: 300, scope: PRIVATE)
  """get a single metric dashboard for the given data set if it exists"""
  metricDashboard(clientKey: KeyString!, dataSetKey: KeyString!, dashboardKey: KeyString!): MetricDashboard
  """returns the list of templates for data set, encapsulated in an object with the nested template and actual dashboard if it exists"""
  metricDashboardTemplatesForDataSet(clientKey: KeyString!, dataSetKey: KeyString!, templateScope: DashboardTemplateScope!): [MetricDashboardTemplateForDataSet!]!
  """
  returns the list of templates for data set, encapsulated in an object with the nested template and actual dashboard if it exists, for the global templates and `all` data set
  
  note: it will create `all` data set if it does not exist
  """
  metricDashboardTemplatesGlobalForClient(clientKey: KeyString!): [MetricDashboardTemplateForDataSet!]!
  """returns the list of MetricDashboards for the given data set"""
  metricDashboardsForDataSet(clientKey: KeyString!, dataSetKey: KeyString!): [MetricDashboard!]!
  """
  Checks if the given metric is supported taking into account the client, the dataSetKey and the entitySetRefinement.
  - if the dataSetKey is sent but does not exist, it will return an error
  - if any of the entitySets (either the refinement or the dataSet)
    has a KeySetNone in an entity that is supported by the metric
    (there is a filterConfig with the relevant valueType)
    it returns false. Otherwise it returns true.
  """
  metricSupportedFor(metricKey: MetricKey!, clientKey: KeyString!, dataSetKey: KeyString!, entitySetRefinement: EntitySetInput): Boolean! @cacheControl(maxAge: 300, scope: PRIVATE)
  """get a list of services by clientKey, optionally filtered by uri"""
  monitoredServices(clientKey: KeyString!, uris: [ResourceURI!]): [MonitoredService!]! @cacheControl(maxAge: 300, scope: PRIVATE)
  """returns current time"""
  now: UTCDate!
  """get a list of people by clientKey"""
  people(clientKey: KeyString!): [Person!]! @cacheControl(maxAge: 300, scope: PRIVATE)
  """get a single person by clientKey and id"""
  person(clientKey: KeyString!, personId: ID!): Person @cacheControl(maxAge: 300, scope: PRIVATE)
  """returns ok"""
  ping: String!
  """returns en error"""
  pingError: String
  """load the data for the given Metric (summaries, deltas, time-series)"""
  pipelinesMetric(input: PipelinesMetricInput!): PipelinesMetricResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["pipelines-metrics"])
  """load the detail data (list of builds) for the given metric"""
  pipelinesMetricDetails(input: PipelinesMetricDetailInput!): DeploymentDetailList! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["pipelines-metrics"])
  """load the allowed filter configs (for ExtraFilters) and their facet values."""
  pipelinesMetricFacets(input: PipelinesMetricFacetsInput!): [FilterConfigWithFacets!]! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["pipelines-metrics"])
  """load the detail data (list of pull requests) for the given metric"""
  prDetail(input: PRDetailInput!): PullRequestDetailList! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["ci-metrics-pr"])
  """get a single programme by clientKey and programmeKey"""
  programme(clientKey: KeyString!, programmeKey: KeyString!): Programme @cacheControl(maxAge: 300, scope: PRIVATE)
  """get the list of available programmes"""
  programmes(clientKey: KeyString!): [Programme!]! @cacheControl(maxAge: 300, scope: PRIVATE)
  """load the data for the given Metric (summaries, deltas, time-series)"""
  pullRequestsMetric(input: PullRequestsMetricInput!): PullRequestsMetricResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["pull-requests-metrics"])
  """load the detail data (list of pull requests) for the given metric"""
  pullRequestsMetricDetails(input: PullRequestsMetricDetailInput!): PullRequestDetailList! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["pull-requests-metrics"])
  """load the allowed filter configs (for ExtraFilters) and their facet values."""
  pullRequestsMetricFacets(input: PullRequestsMetricFacetsInput!): [FilterConfigWithFacets!]! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["pull-requests-metrics"])
  queryMethodsFor(metricKey: MetricKey!): [QueryMethodDetail!]! @cacheControl(maxAge: 300, scope: PRIVATE)
  """get a list of repositories by clientKey, optionally filtered by uris"""
  repositories(clientKey: KeyString!, uris: [ResourceURI!]): [Repository!]! @cacheControl(maxAge: 300, scope: PRIVATE)
  """
  load the data for the given Metric (summaries, deltas, time-series)
  rate for Number of returned transitions (Does not respect Time Range) from completed tickets / Number of completed tickets (Respects Time Range)
  # not this -> rate of RETURN_TICKETS_COUNT / COMPLETED_TICKETS
  """
  returnRateMetric(input: ReturnRateMetricInput!): ReturnRateMetricResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["return-rate"])
  returnRateMetricDetails(input: ReturnRateMetricDetailsInput!): ReturnedTicketDetailList! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["return-rate"])
  """
  facets for: metric about the number of tickets (filtered by the given filters) that were returned
  - TO CONFIRM WITH LUKE: if the return event happened OUTSIDE of the date range, does it count as returned or not?
  """
  returnTicketsFacets(input: ReturnTicketsFacetsInput!): [FilterConfigWithFacets!]! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["return-tickets"])
  returnTicketsMetric(input: ReturnTicketsMetricInput!): ReturnTicketsMetricResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["return-tickets"])
  returnTicketsMetricDetails(input: ReturnTicketsMetricDetailsInput!): ReturnedTicketDetailList! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["return-tickets"])
  """load the detail data (list of tickets) for the given metric"""
  speedingTicketsDetails(input: SpeedingTicketsMetricDetailInput!): SpeedingTicketDetailList! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["speeding-tickets", "speeding-rate"])
  speedingTicketsFacets(input: SpeedingTicketsFacetsInput!): [FilterConfigWithFacets!]! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["speeding-tickets"])
  speedingTicketsMetric(input: SpeedingTicketsMetricInput!): SpeedingTicketsMetricResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["speeding-tickets"])
  """
  load the data for the given Metric (summaries, deltas, time-series)
  rate of NUMERATOR_TRANSITIONS / DENOMINATOR_TRANSITIONS
  SPEEDING TRANSITION RATE = 100 * (NUMERATOR_TRANSITIONS / DENOMINATOR_TRANSITIONS)
  NUMERATOR_TRANSITIONS: Number of Transitions that we care about (in time range and with filters) and were speeding
  DENOMINATOR_TRANSITIONS: Number of Transitions that we care about (in time range and with filters)
  """
  speedingTransitionsRateMetric(input: SpeedingTransitionsRateMetricInput!): SpeedingTransitionsRateMetricResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["speeding-rate"])
  """load the data for the given Metric (summaries and deltas)"""
  sprintCompletionMetric(input: SprintCompletionMetricInput!): SprintCompletionResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["sprint-completion"])
  """load the detail data (list of tickets) for the given metric"""
  sprintCompletionMetricDetails(input: SprintCompletionMetricDetailsInput!): TicketDetailWithSprintsList! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["sprint-completion"])
  """load the allowed filter configs (for ExtraFilters) and their facet values."""
  sprintCompletionMetricFacets(input: SprintCompletionMetricFacetsInput!): [FilterConfigWithFacets!]! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["sprint-completion"])
  """
  Get sprint by FiltersInput (recognises the CURRENT and LAST sprints via the MagicKeys)
  
  see MagicKeys
  """
  sprintForFilters(filters: FiltersInput!, sprintDateRangeFieldOverride: SprintDateRangeField): Sprint @cacheControl(maxAge: 300, scope: PRIVATE)
  """load the data for the given Metric (summaries, deltas, time-series)"""
  sprintValuePerTeamSize(input: SprintValuePerTeamSizeMetricInput!): SprintValuePerTeamSizeMetricResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["sprint-value-per-team-size"])
  """load the detail data (list of tickets) for the given metric"""
  sprintValuePerTeamSizeDetails(input: SprintValuePerTeamSizeMetricDetailInput!): TicketDetailList! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["sprint-value-per-team-size"])
  """load the allowed filter configs (for ExtraFilters) and their facet values."""
  sprintValuePerTeamSizeFacets(input: SprintValuePerTeamSizeMetricFacetsInput!): [FilterConfigWithFacets!]! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["sprint-value-per-team-size"])
  """returns the list of StatusBuckets for the given client (may include the variants)"""
  statusBuckets(clientKey: KeyString!): [StatusBucket!]! @cacheControl(maxAge: 300, scope: PRIVATE)
  """returns the list of StatusBuckets for the given client (may include the variants)"""
  statusBucketsIndex(clientKey: KeyString!): StatusBucketIndex! @cacheControl(maxAge: 300, scope: PRIVATE)
  """
  returns the list of buckets for the given board
  returns null if the board does not exist
  
  useGlobalIfMissing: if true and there are not variants for the given BoardKey, it will use the global mappings
  """
  statusBucketsVariant(clientKey: KeyString!, boardKey: KeyString!, useGlobalIfMissing: Boolean): StatusBucketVariantStatus @cacheControl(maxAge: 300, scope: PRIVATE)
  """get a list of statuses by clientKey"""
  statuses(clientKey: KeyString!): [Status!]! @cacheControl(maxAge: 300, scope: PRIVATE)
  """returns the list all of statuses present in any ticket using the given filters"""
  statusesFor(filters: FiltersInput!, sprintDateRangeFieldOverride: SprintDateRangeField, statusType: StatusType!): [Status!]! @cacheControl(maxAge: 300, scope: PRIVATE)
  """get a list of statuses by clientKey and boardKey"""
  statusesForBoard(clientKey: KeyString!, boardKey: KeyString!): [Status!]! @cacheControl(maxAge: 300, scope: PRIVATE)
  """
  returns the list all of transitions present in any ticket using the given filters
  note: it will ignore the sprint and date range
  """
  subtaskIssueTypesFor(filters: FiltersNoTimeInput!, sprintDateRangeFieldOverride: SprintDateRangeField): [IssueType!]! @cacheControl(maxAge: 300, scope: PRIVATE)
  """get a single person by clientKey and teamKey"""
  team(clientKey: KeyString!, teamKey: KeyString!): Team @cacheControl(maxAge: 300, scope: PRIVATE)
  """get a list of teams by clientKey"""
  teams(clientKey: KeyString!): [Team!]! @cacheControl(maxAge: 300, scope: PRIVATE)
  """checks feature toggles"""
  testFlag: String!
  """load the data for the given Metric (summaries, deltas, time-series)"""
  ticketCommitHotspotsMetric(input: TicketCommitHotspotsMetricInput!): TicketCommitHotspotsMetricResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["ticket-commit-hotspots"])
  """load the detail data (list of tickets) for the given metric"""
  ticketCommitHotspotsMetricDetails(input: TicketCommitHotspotsMetricDetailInput!): TicketDetailList! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["ticket-commit-hotspots"])
  """load the allowed filter configs (for ExtraFilters) and their facet values."""
  ticketCommitHotspotsMetricFacets(input: TicketCommitHotspotsMetricFacetsInput!): [FilterConfigWithFacets!]! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["ticket-commit-hotspots"])
  """load the data for the given Metric (summaries, deltas, time-series)"""
  ticketsMetric(input: TicketsMetricInput!): TicketsMetricResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["tickets-metrics"])
  """load the detail data (list of tickets) for the given metric"""
  ticketsMetricDetails(input: TicketsMetricDetailInput!): TicketDetailList! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["tickets-metrics"])
  """load the allowed filter configs (for ExtraFilters) and their facet values."""
  ticketsMetricFacets(input: TicketsMetricFacetsInput!): [FilterConfigWithFacets!]! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["tickets-metrics"])
  """load the data for the given Metric (summaries, deltas, time-series)"""
  ticketsScopeMetric(input: TicketsScopeMetricInput!): TicketsScopeMetricResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["tickets-scope"])
  """load the detail data (list of tickets) for the given metric"""
  ticketsScopeMetricDetails(input: TicketsScopeMetricDetailInput!): TicketDetailList! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["tickets-scope"])
  """load the allowed filter configs (for ExtraFilters) and their facet values."""
  ticketsScopeMetricFacets(input: TicketsScopeMetricFacetsInput!): [FilterConfigWithFacets!]! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["tickets-scope"])
  """load the data for the given Metric (summaries, deltas, time-series-by-status)"""
  ticketsTimelineMetric(input: TicketsTimelineMetricInput!): TicketsTimelineMetricStatusResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["tickets-timeline"])
  """load the data for the given Metric (summaries, deltas, time-series-by-status) where breakdown by valueType === StatusBucket"""
  ticketsTimelineMetricByStatusBucket(input: TicketsTimelineMetricInput!): TicketsTimelineMetricStatusBucketResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["tickets-timeline"])
  """load the detail data (list of tickets) for the given metric"""
  ticketsTimelineMetricDetails(input: TicketsTimelineMetricDetailInput!): TicketDetailList! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["tickets-timeline"])
  """load the allowed filter configs (for ExtraFilters) and their facet values."""
  ticketsTimelineMetricFacets(input: TicketsTimelineMetricFacetsInput!): [FilterConfigWithFacets!]! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["tickets-timeline"])
  """for temp checks"""
  tmp: String
  """get a list of transitions by clientKey and keys"""
  transitions(clientKey: KeyString!, transitionKeys: [KeyString!]): [Transition!]! @cacheControl(maxAge: 300, scope: PRIVATE)
  """
  returns the list all of transitions present in any ticket using the given filters
  note: it will ignore the sprint and date range
  """
  transitionsFor(filters: FiltersNoTimeInput!, sprintDateRangeFieldOverride: SprintDateRangeField): [Transition!]! @cacheControl(maxAge: 300, scope: PRIVATE)
  """load the data for the given input.Metric"""
  treemapMetric(input: TreemapInput!): TreemapResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["treemap-metrics"])
  """load the allowed filter configs (for ExtraFilters) and their facet values. Optionally, we can select which ones we want by passing the list of keys"""
  treemapMetricFacets(input: TreemapInput!): [FilterConfigWithFacets!]! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["treemap-metrics"])
  """returns a list of statuses that have not yet been assigned to a StatusBucket"""
  unmappedStatuses(clientKey: KeyString!): [Status!]! @cacheControl(maxAge: 300, scope: PRIVATE)
  """
  returns a list of statuses that have not yet been assigned to a StatusBucket
  
  useGlobalIfMissing: if true and there are not variants for the given BoardKey, it will use the global mappings
  """
  unmappedStatusesForVariant(clientKey: KeyString!, boardKey: KeyString!, useGlobalIfMissing: Boolean): [Status!]! @cacheControl(maxAge: 300, scope: PRIVATE)
  """load the data for the given Metric (summaries, deltas, time-series)"""
  unresolvedTicketsMetric(input: UnresolvedTicketsMetricInput!): UnresolvedTicketsMetricResponse! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["unresolved-tickets"])
  """load the detail data (list of tickets) for the given metric"""
  unresolvedTicketsMetricDetails(input: UnresolvedTicketsMetricDetailInput!): TicketDetailList! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["unresolved-tickets"])
  """load the allowed filter configs (for ExtraFilters) and their facet values."""
  unresolvedTicketsMetricFacets(input: UnresolvedTicketsMetricFacetsInput!): [FilterConfigWithFacets!]! @cacheControl(maxAge: 300, scope: PRIVATE) @queryMethodFor(metricGroups: ["unresolved-tickets"])
  """returns the UserInfo of the logged in user"""
  userInfo: UserInfo!
}

type QueryMethodDetail @notNormalised {
  methodName: String!
  description: String
}

"""(TYPE) common type to represent a RangeFilter (key -> from+to)"""
type RangeFilter @notNormalised {
  """
  key (same in FilterConfig.key) of the filter to apply"
  """
  key: KeyString!
  """lower limit of the range (could be null)"""
  from: Float
  """upper limit of the range (could be null)"""
  to: Float
}

"""(INPUT) common type to represent a RangeFilter (key -> from+to)"""
input RangeFilterInput {
  """key (same in FilterConfig.key) of the filter to apply"""
  key: KeyString!
  """lower limit of the range (could be null)"""
  from: Float
  """upper limit of the range (could be null)"""
  to: Float
}

"""common type to represent a RangeFilter with labels"""
type RangeFilterWithLabels @notNormalised {
  """key (same in FilterConfig.key) of the filter to apply"""
  key: KeyString!
  """label (same in FilterConfig.label) of the filter to apply"""
  label: String!
  """value type (same in FilterConfig.valueType) of the filter to apply"""
  valueType: FilterValueType!
  """filter type (same in FilterConfig.filterType) of the filter to apply"""
  filterType: FilterType!
  """lower limit of the range (could be null)"""
  from: Float
  """upper limit of the range (could be null)"""
  to: Float
}

type Repository {
  id: ID!
  clientKey: KeyString!
  repositoryUri: ResourceURI!
  name: String!
  filesPathPrefix: String!
}

"""string cleaned to act as a resource's URI, allowing only A-Z,a-z,0-9,-,_,.,/ chars"""
scalar ResourceURI

enum ReturnRateMetric {
  """
  Return Rate ->
  Rate (%) = 100 * (RETURN_INSTANCES / COMPLETED_TICKETS)
  (denominator) COMPLETED_TICKETS => Tickets that were completed in the given time range
  (numerator) RETURN_INSTANCES => RETURN_TRANSITION OR DEFECT_CREATED OR DEFECT_RETURN_TRANSITION
  
  RETURN_TRANSITION => Transitions of the tickets in COMPLETED_TICKETS (at any point in time) that are considered Returns
  DEFECT_CREATED => Sub-Task of the tickets in COMPLETED_TICKETS that are considered a Defect
  DEFECT_RETURN_TRANSITION => Transitions of the Defects (see above) of the tickets in COMPLETED_TICKETS (at any point in time) that are considered Returns
  
  note: this rate is a division of TRANSITIONS+SUBTASKS over TICKETS, which means it can be more than 100%.
  It is actually the number of returns on average per ticket, but represented as a %
  """
  RETURN_RATE
}

input ReturnRateMetricDetailsInput {
  metricKey: ReturnRateMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  """if not specified, we'll look into the stored Config for this client"""
  returnConfig: TicketReturnSettingsInput
}

input ReturnRateMetricInput {
  metricKey: ReturnRateMetric!
  """if not specified, we'll look into the stored Config for this client"""
  returnConfig: TicketReturnSettingsInput
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  """granularity for the time series (default to `day` if not specified)"""
  granularity: Granularity
  """
  can be null, to ask for the general result.
  
  If it is not null, it has to be one of the keys of the valid FilterConfig (see FilterConfig)
  
  note that this cannot be an enum because it will depend on the data (custom fields et al)
  """
  breakdown: KeyString
}

type ReturnRateMetricInputType @notNormalised {
  metricKey: ReturnRateMetric!
  returnConfig: TicketReturnSettings
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  breakdown: KeyString
}

type ReturnRateMetricResponse {
  id: ID!
  input: ReturnRateMetricInputType!
  metricKey: ReturnRateMetric!
  """summary with breakdown blank (key = _general_)"""
  generalSummary: PercentageSummary
  """delta of the generalSummary"""
  generalDelta: NumberDeltaSummary
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one summary per breakdown bucket
  """
  summariesList: [PercentageSummary!]!
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one delta summary per breakdown bucket
  if there are no deltas configured, it will be NULL
  """
  deltasList: [NumberDeltaSummary!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value, depending on `metric`
  """
  timeSeriesList: [PercentageTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates. The value of each point will be the cumulative average.
  
  e.g.
  
  ```
  TS0---TS1---TS2---TS3---TS4
  
  ATS0 = TS0
  ATS1 = (TS1 + TS0) / 2
  ATS2 = (TS2 + TS1 + TS0) / 3
  ATS3 = (TS3 + TS2 + TS1 + TS0) / 4
  ATS4 = (TS4 + TS3 + TS2 + TS1 + TS0) / 5
  ```
  """
  averageTimeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates.
  The value of each point will be the cumulative average taking into account only the current value + the previous `n` values (where n is the windowSize)
  It will load the TS of the past filters (same filters used for delta calculation) to get the "previous values" of the initial points
  
  e.g time series with 5 points (TS0 to TS4), windowSize = 2
  
  ```
  PTS0---PTS1---PTS2---PTS3---PTS4---TS0---TS1---TS2---TS3---TS4
  
  RATS0 = (TS0 + PTS4 + PTS3) / 3
  RATS1 = (TS1 + TS0 + PTS4) / 3
  RATS2 = (TS2 + TS1 + TS0) / 3
  RATS3 = (TS3 + TS2 + TS1) / 3
  RATS4 = (TS4 + TS3 + TS2) / 3
  ```
  """
  rollingAverageTimeSeriesList(windowSize: Int, windowSizePercentage: Float): [NumberTimeSeriesWithWindowSize!]!
}

input ReturnTicketsFacetsInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: ReturnTicketsMetric!
  """if not specified, we'll look into the stored Config for this client"""
  returnConfig: TicketReturnSettingsInput
}

enum ReturnTicketsMetric {
  """Return Tickets -> value is number of tickets that were returned"""
  RETURN_TICKETS_COUNT
  """Return Tickets -> value is sum of story points of tickets that were returned"""
  RETURN_TICKETS_STORY_POINTS
}

input ReturnTicketsMetricDetailsInput {
  metricKey: ReturnTicketsMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  """if not specified, we'll look into the stored Config for this client"""
  returnConfig: TicketReturnSettingsInput
}

input ReturnTicketsMetricInput {
  metricKey: ReturnTicketsMetric!
  """if not specified, we'll look into the stored Config for this client"""
  returnConfig: TicketReturnSettingsInput
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  """granularity for the time series (default to `day` if not specified)"""
  granularity: Granularity
  """
  can be null, to ask for the general result.
  
  If it is not null, it has to be one of the keys of the valid FilterConfig (see FilterConfig)
  
  note that this cannot be an enum because it will depend on the data (custom fields et al)
  """
  breakdown: KeyString
}

type ReturnTicketsMetricInputType @notNormalised {
  metricKey: ReturnTicketsMetric!
  returnConfig: TicketReturnSettings
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  breakdown: KeyString
}

type ReturnTicketsMetricResponse {
  id: ID!
  input: ReturnTicketsMetricInputType!
  metricKey: ReturnTicketsMetric!
  """summary with breakdown blank (key = _general_)"""
  generalSummary: NumberSummary
  """delta of the generalSummary"""
  generalDelta: NumberDeltaSummary
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one summary per breakdown bucket
  """
  summariesList: [NumberSummary!]!
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one delta summary per breakdown bucket
  if there are no deltas configured, it will be NULL
  """
  deltasList: [NumberDeltaSummary!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value, depending on `metric`
  """
  timeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates. The value of each point will be the cumulative average.
  
  e.g.
  
  ```
  TS0---TS1---TS2---TS3---TS4
  
  ATS0 = TS0
  ATS1 = (TS1 + TS0) / 2
  ATS2 = (TS2 + TS1 + TS0) / 3
  ATS3 = (TS3 + TS2 + TS1 + TS0) / 4
  ATS4 = (TS4 + TS3 + TS2 + TS1 + TS0) / 5
  ```
  """
  averageTimeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates.
  The value of each point will be the cumulative average taking into account only the current value + the previous `n` values (where n is the windowSize)
  It will load the TS of the past filters (same filters used for delta calculation) to get the "previous values" of the initial points
  
  e.g time series with 5 points (TS0 to TS4), windowSize = 2
  
  ```
  PTS0---PTS1---PTS2---PTS3---PTS4---TS0---TS1---TS2---TS3---TS4
  
  RATS0 = (TS0 + PTS4 + PTS3) / 3
  RATS1 = (TS1 + TS0 + PTS4) / 3
  RATS2 = (TS2 + TS1 + TS0) / 3
  RATS3 = (TS3 + TS2 + TS1) / 3
  RATS4 = (TS4 + TS3 + TS2) / 3
  ```
  """
  rollingAverageTimeSeriesList(windowSize: Int, windowSizePercentage: Float): [NumberTimeSeriesWithWindowSize!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value taken from each TicketDetail
  The `size` will be the number of tickets that the point represents
  The `singleItem` is populated only when size is 1. It contains key, label, link
  """
  scatterTimeSeriesList: [ScatterTimeSeries!]!
}

type ReturnedEvent @notNormalised {
  date: UTCDate!
  transitionKey: KeyString!
  fromStatus: Status
  toStatus: Status
}

type ReturnedTicketDetail {
  id: ID!
  ticketDetail: TicketDetail!
  returnsCount: Int!
  returnTransitionsCount: Int!
  defectsCount: Int!
  defectReturnTransitionsCount: Int!
  returnTransitions: [ReturnedEvent!]!
  defects: [DefectDetail!]!
}

type ReturnedTicketDetailList {
  id: ID!
  """total quantity of details"""
  count: Int!
  """list of ticket details, all sharing the same valueConfig to calculate the `value` field"""
  details: [ReturnedTicketDetail!]!
  """key of the valueConfig"""
  valueConfigKey: KeyString!
  """valueConfig used for the `value` field of each detail"""
  valueConfig: ValueConfig!
}

input ScatterMetricInput {
  """The value of the `y` axis in the scatter chart. It will be the value taken from each Detail item from a field determined by the MetricConfig"""
  value: Float!
}

type ScatterMetricInputType @notNormalised {
  """The value of the `y` axis in the scatter chart. It will be the value taken from each Detail item from a field determined by the MetricConfig"""
  value: Float!
}

"""
represents a single point of a TimeSeries with a list of tickets for each x + a single numeric value

It can be used for a scatter chart, when the tickets are divided and each point represents `x` and `y` and the tickets that share the same x and y.
"""
type ScatterTimePoint implements TimePoint @notNormalised {
  """date-time of the point"""
  x: JSDate!
  """date-time of the initial moment of the time range represented by this point"""
  fromDate: UTCStartOfDay!
  """date-time of the final moment of the time range represented by this point"""
  toDate: UTCEndOfDay!
  """numeric value of the point"""
  y: Float!
  """number of items that this point represents (tickets or commits)"""
  size: Int!
  """info about the single item if this point represents a single item (only 1 ticket). If size > 1 singleItem will be blank."""
  singleItem: ItemWithLink
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

type ScatterTimeSeries implements TimeSeries @notNormalised {
  """identification of the Time Series"""
  key: String!
  """optional label to show in the UI for this Time Series (it should override any regular translation-based label)"""
  label: String!
  """sorted list of points"""
  data: [ScatterTimePoint!]!
}

type SpeedingTicketDetail {
  id: ID!
  ticketDetail: TicketDetail!
  speedingTransitions: [TransitionEvent!]!
}

type SpeedingTicketDetailList {
  id: ID!
  """total quantity of details"""
  count: Int!
  """list of item details, all sharing the same valueConfig to calculate the `value` field"""
  details: [SpeedingTicketDetail!]!
  """key of the valueConfig"""
  valueConfigKey: KeyString!
  """valueConfig used for the `value` field of each detail"""
  valueConfig: ValueConfig!
}

input SpeedingTicketsFacetsInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: SpeedingTicketsMetric!
  """max duration in seconds of a transition to be considered speeding"""
  maxSeconds: Int!
}

"""list of available ticket metric with a single value"""
enum SpeedingTicketsMetric {
  """speeding tickets (tickets with at least one speeding transition) -> value = count of tickets"""
  SPEEDING_TICKETS_COUNT
  """speeding tickets (tickets with at least one speeding transition) -> value = sum of ticket's story points"""
  SPEEDING_TICKETS_STORY_POINTS
}

input SpeedingTicketsMetricDetailInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: SpeedingTicketsMetric!
  """max duration in seconds of a transition to be considered speeding"""
  maxSeconds: Int!
}

input SpeedingTicketsMetricInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: SpeedingTicketsMetric!
  """max duration in seconds of a transition to be considered speeding"""
  maxSeconds: Int!
  """granularity for the time series (default to `day` if not specified)"""
  granularity: Granularity
  """
  can be null, to ask for the general result.
  
  If it is not null, it has to be one of the keys of the valid FilterConfig (see FilterConfig)
  
  note that this cannot be an enum because it will depend on the data (custom fields et al)
  """
  breakdown: KeyString
}

type SpeedingTicketsMetricInputType @notNormalised {
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: SpeedingTicketsMetric!
  maxSeconds: Int!
  granularity: Granularity
  breakdown: KeyString
}

type SpeedingTicketsMetricResponse {
  id: ID!
  input: SpeedingTicketsMetricInputType!
  metricKey: SpeedingTicketsMetric!
  """summary with breakdown blank (key = _general_)"""
  generalSummary: NumberSummary
  """delta of the generalSummary"""
  generalDelta: NumberDeltaSummary
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one summary per breakdown bucket
  """
  summariesList: [NumberSummary!]!
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one delta summary per breakdown bucket
  if there are no deltas configured, it will be NULL
  """
  deltasList: [NumberDeltaSummary!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value, depending on `metric`
  """
  timeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates. The value of each point will be the cumulative average.
  
  e.g.
  
  ```
  TS0---TS1---TS2---TS3---TS4
  
  ATS0 = TS0
  ATS1 = (TS1 + TS0) / 2
  ATS2 = (TS2 + TS1 + TS0) / 3
  ATS3 = (TS3 + TS2 + TS1 + TS0) / 4
  ATS4 = (TS4 + TS3 + TS2 + TS1 + TS0) / 5
  ```
  """
  averageTimeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates.
  The value of each point will be the cumulative average taking into account only the current value + the previous `n` values (where n is the windowSize)
  It will load the TS of the past filters (same filters used for delta calculation) to get the "previous values" of the initial points
  
  e.g time series with 5 points (TS0 to TS4), windowSize = 2
  
  ```
  PTS0---PTS1---PTS2---PTS3---PTS4---TS0---TS1---TS2---TS3---TS4
  
  RATS0 = (TS0 + PTS4 + PTS3) / 3
  RATS1 = (TS1 + TS0 + PTS4) / 3
  RATS2 = (TS2 + TS1 + TS0) / 3
  RATS3 = (TS3 + TS2 + TS1) / 3
  RATS4 = (TS4 + TS3 + TS2) / 3
  ```
  """
  rollingAverageTimeSeriesList(windowSize: Int, windowSizePercentage: Float): [NumberTimeSeriesWithWindowSize!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value taken from each TicketDetail
  The `size` will be the number of tickets that the point represents
  The `singleItem` is populated only when size is 1. It contains key, label, link
  """
  scatterTimeSeriesList: [ScatterTimeSeries!]!
}

enum SpeedingTransitionsRateMetric {
  """
  speeding transitions
  rate from
    the number of transitions from the input (in time range and with filters) that were speeding according to the input
      divided by
    the number of transitions from the input (in time range and with filters))
  -> value = rate of transitions (from the input) that were speeding among all the transisions (from the input)
  """
  SPEEDING_TRANSITIONS_RATE
}

input SpeedingTransitionsRateMetricInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: SpeedingTransitionsRateMetric!
  """max duration in seconds of a transition to be considered speeding"""
  maxSeconds: Int!
  """granularity for the time series (default to `day` if not specified)"""
  granularity: Granularity
  """
  can be null, to ask for the general result.
  
  If it is not null, it has to be one of the keys of the valid FilterConfig (see FilterConfig)
  
  note that this cannot be an enum because it will depend on the data (custom fields et al)
  """
  breakdown: KeyString
}

type SpeedingTransitionsRateMetricInputType @notNormalised {
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: SpeedingTransitionsRateMetric!
  maxSeconds: Int!
  granularity: Granularity
  breakdown: KeyString
}

type SpeedingTransitionsRateMetricResponse {
  id: ID!
  input: SpeedingTransitionsRateMetricInputType!
  metricKey: SpeedingTransitionsRateMetric!
  """summary with breakdown blank (key = _general_)"""
  generalSummary: PercentageSummary
  """delta of the generalSummary"""
  generalDelta: NumberDeltaSummary
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one summary per breakdown bucket
  """
  summariesList: [PercentageSummary!]!
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one delta summary per breakdown bucket
  if there are no deltas configured, it will be NULL
  """
  deltasList: [NumberDeltaSummary!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value, depending on `metric`
  """
  timeSeriesList: [PercentageTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates. The value of each point will be the cumulative average.
  
  e.g.
  
  ```
  TS0---TS1---TS2---TS3---TS4
  
  ATS0 = TS0
  ATS1 = (TS1 + TS0) / 2
  ATS2 = (TS2 + TS1 + TS0) / 3
  ATS3 = (TS3 + TS2 + TS1 + TS0) / 4
  ATS4 = (TS4 + TS3 + TS2 + TS1 + TS0) / 5
  ```
  """
  averageTimeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates.
  The value of each point will be the cumulative average taking into account only the current value + the previous `n` values (where n is the windowSize)
  It will load the TS of the past filters (same filters used for delta calculation) to get the "previous values" of the initial points
  
  e.g time series with 5 points (TS0 to TS4), windowSize = 2
  
  ```
  PTS0---PTS1---PTS2---PTS3---PTS4---TS0---TS1---TS2---TS3---TS4
  
  RATS0 = (TS0 + PTS4 + PTS3) / 3
  RATS1 = (TS1 + TS0 + PTS4) / 3
  RATS2 = (TS2 + TS1 + TS0) / 3
  RATS3 = (TS3 + TS2 + TS1) / 3
  RATS4 = (TS4 + TS3 + TS2) / 3
  ```
  """
  rollingAverageTimeSeriesList(windowSize: Int, windowSizePercentage: Float): [NumberTimeSeriesWithWindowSize!]!
}

type Sprint {
  id: ID!
  clientKey: KeyString!
  sprintKey: KeyString!
  sprintLabel: String!
  dateRange: DateRange!
  sprintDateRangeField: SprintDateRangeField!
  rawStartDate: UTCDate
  rawEndDate: UTCDate
  rawCompletedDate: UTCDate
}

type SprintCompletionBucket @notNormalised {
  seriesKey: KeyString!
  seriesLabel: String!
  """Sprint key"""
  key: KeyString!
  """Sprint Lebel"""
  label: String!
  """the number of tickets for sprint based on SprintCompletionMetric"""
  ticketCount: Int!
  """DateTime when the sprints starts"""
  startDate: UTCDate!
  """DateTime when the sprints end"""
  endDate: UTCDate!
  """
  the number of story points for the bucket
  state (discuss it with Luke or Eddie)
  """
  value: FloatRounded2!
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

type SprintCompletionBucketsSeries @notNormalised {
  key: KeyString!
  label: String!
  data: [SprintCompletionBucket!]!
  """the field that this series represents"""
  sprintCompletionSummaryField: SprintCompletionSummaryField!
}

type SprintCompletionDelta @notNormalised {
  """identification of the summary (breakdown's bucket key, or _general_)"""
  key: KeyString!
  """label to show in the UI for this Summary bucket (blank for _general_)"""
  label: String!
  """list of sprint keys involved"""
  sprintKeys: [String!]!
  ticketValue: DeltaValue!
  value: DeltaValue!
  total: SprintCompletionDeltaResult!
  completed: SprintCompletionDeltaResult!
  incomplete: SprintCompletionDeltaResult!
  descoped: SprintCompletionDeltaResult!
  removed: SprintCompletionDeltaResult!
}

type SprintCompletionDeltaResult @notNormalised {
  tickets: DeltaValue!
  value: DeltaValue!
  ticketKeys: [String!]!
  """number of items that this point represents"""
  countItems: DeltaValue!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
  sprintCompletionSummaryField: SprintCompletionSummaryField!
}

enum SprintCompletionMetric {
  """
  makes the Response about the OVERALL
  makes ticketValueDetail the percentage of completedTickets / totalTicketCount
  makes valueDetail the percentage of completedValue / totalValue
  """
  SPRINT_COMPLETION_OVERALL
  """
  makes the Response about the TARGET
  makes ticketValueDetail the percentage of completedTickets / totalTicketCount
  makes valueDetail the percentage of completedValue / totalValue
  """
  SPRINT_COMPLETION_TARGET
  """
  makes the Response about the SCOPE
  makes ticketValueDetail the percentage of completedTickets / totalTicketCount
  makes valueDetail the percentage of completedValue / totalValue
  """
  SPRINT_COMPLETION_ADDED
}

input SprintCompletionMetricDetailsInput {
  metricKey: SprintCompletionMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  sprintCompletionSummaryField: SprintCompletionSummaryField!
  valueConfigKey: KeyString
}

input SprintCompletionMetricFacetsInput {
  metricKey: SprintCompletionMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
}

input SprintCompletionMetricInput {
  metricKey: SprintCompletionMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  """
  optional. Override of the sprint date range field to use, which will also be used
  It will default to the client's SprintDateRangeField.
  """
  sprintDateRangeFieldOverride: SprintDateRangeField
  """count of tickets by default"""
  valueConfigKey: KeyString
  """
  can be null, to ask for the general result.
  
  If it is not null, it has to be one of the keys of the valid FilterConfig (see FilterConfig)
  
  note that this cannot be an enum because it will depend on the data (custom fields et al)
  """
  breakdown: KeyString
}

type SprintCompletionMetricInputType @notNormalised {
  metricKey: SprintCompletionMetric!
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
  valueConfigKey: KeyString
  breakdown: KeyString
}

type SprintCompletionResponse {
  id: ID!
  input: SprintCompletionMetricInputType!
  metricKey: SprintCompletionMetric!
  """used value config"""
  valueConfig: ValueConfig!
  """summary with breakdown blank (key = _general_)"""
  generalSummary: SprintCompletionSummary
  """delta of the generalSummary"""
  generalDelta: SprintCompletionDelta
  """
  if breakdown is blank, only one with GENERAL_KEY
  otherwise, one per element of the breakdown
  """
  summariesList: [SprintCompletionSummary!]!
  """This Field is instead of breaking down by sprint_name"""
  sprintCompletionPerSprint: [SprintCompletionSummary!]!
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one delta summary per breakdown bucket
  if there are no deltas configured, it will be NULL
  """
  deltasList: [SprintCompletionDelta!]!
}

type SprintCompletionSummary {
  id: ID!
  """identification of the summary (breakdown's bucket key, or _general_)"""
  key: KeyString!
  """label to show in the UI for this Summary bucket (blank for _general_)"""
  label: String!
  """list of sprint keys involved"""
  sprintKeys: [String!]!
  """DateTime when the sprints starts (ONLY IF THE BREAKDOWN IS sprint_name OTHERWISE IT'S NULL)"""
  startDate: UTCDate
  """DateTime when the sprints end (ONLY IF THE BREAKDOWN IS sprint_name OTHERWISE IT'S NULL)"""
  endDate: UTCDate
  """valueDetail.percentage || 0"""
  ticketValue: FloatRounded2!
  """valueDetail.percentage || 0"""
  value: FloatRounded2!
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
  """files percentage value, depends on which metric is it"""
  ticketValueDetail: PercentageDetail!
  valueDetail: PercentageDetail!
  """TODO RENAME THIS"""
  sprintCompletionBucketsSeriesLists: [SprintCompletionBucketsSeries]
  total: SprintCompletionSummaryResult!
  completed: SprintCompletionSummaryResult!
  incomplete: SprintCompletionSummaryResult!
  descoped: SprintCompletionSummaryResult!
  removed: SprintCompletionSummaryResult!
}

enum SprintCompletionSummaryField {
  """
  All completed tickets regarding a specific sprint and the sprint that
  are associated with each ticket
  """
  COMPLETED_TICKETS
  """
  All close incomplete tickets regarding a specific sprint and the sprint that
  are associated with each ticket
  """
  INCOMPLETE_TICKETS
  """
  All removed tickets regarding a specific sprint and the sprint that
  are associated with each ticket
  """
  REMOVED_TICKETS
  """
  All descoped tickets regarding a specific sprint and the sprint that
  are associated with each ticket
  """
  DESCOPED_TICKETS
  """All of the tickets involved"""
  TOTAL_TICKETS
}

type SprintCompletionSummaryResult @notNormalised {
  tickets: Int!
  value: FloatRounded2!
  ticketKeys: [String!]!
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
  """the field that this data represents"""
  sprintCompletionSummaryField: SprintCompletionSummaryField!
}

enum SprintDateRangeField {
  END_DATE
  COMPLETED_DATE
}

type SprintKeyLabel @notNormalised {
  sprintKey: KeyString!
  sprintLabel: String!
}

"""list of available ticket metric with a single value"""
enum SprintValuePerTeamSizeMetric {
  """of the sprints that take place on the given time range, sum the value (SP by default) of each completed ticket in the sprint and divide by the sprint.teamSize (0 if sprint.teamSize is 0)"""
  SPRINT_VALUE_PER_TEAM_SIZE
}

input SprintValuePerTeamSizeMetricDetailInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: SprintValuePerTeamSizeMetric!
  valueConfigKey: KeyString
}

input SprintValuePerTeamSizeMetricFacetsInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: SprintValuePerTeamSizeMetric!
}

input SprintValuePerTeamSizeMetricInput {
  metricKey: SprintValuePerTeamSizeMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  valueConfigKey: KeyString
}

type SprintValuePerTeamSizeMetricInputType @notNormalised {
  metricKey: SprintValuePerTeamSizeMetric!
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
  valueConfigKey: KeyString
}

type SprintValuePerTeamSizeMetricResponse {
  id: ID!
  input: SprintValuePerTeamSizeMetricInputType!
  metricKey: SprintValuePerTeamSizeMetric!
  valueConfig: ValueConfig
  sprintSummariesList: [NumberSprintSummary!]!
  summary: NumberSummary!
  delta: NumberDeltaSummary
  teamSizeSeries: [NumberSummary!]!
  valueSeries: [NumberSummary!]!
}

"""object that represents a status of a ticket"""
type Status {
  id: ID!
  type: StatusType!
  statusKey: KeyString!
  label: String!
  """calculated order of this status in the usual life of a ticket"""
  assumedOrder: Int!
  """
  When StatusBuckets feature is enabled, a Status gets its position
  assigned from the StatusBucket that contains it.
  """
  position: Int!
}

type StatusBucket {
  id: ID!
  clientKey: KeyString!
  statusBucketKey: KeyString!
  label: String!
  description: String!
  position: Int!
  statuses: [Status!]!
  variants: [StatusBucketVariant!]!
}

type StatusBucketIndex {
  id: ID!
  clientKey: KeyString!
  """client level mapping"""
  global: StatusBucketStats!
  """only the variants that exist"""
  variants: [StatusBucketVariantStats!]!
}

input StatusBucketItemInput {
  """Must be unique to identify a patch"""
  statusBucketKey: KeyString!
  """The label for the StatusBucket. If not supplied, then this field won't be updated."""
  label: String
  """The description for the StatusBucket. If not supplied, then this field won't be updated."""
  description: String
  """
  The list of StatusKeys that should be mapped to this StatusBucket
  If the same StatusKey appears in two (or more) StatusBuckets, the mutation will fail.
  This field can be included in all StatusBucketItemPatches, or omitted from all StatusBucketItemPatches
  (If this field is provided for some and omitted for other StatusBucketItemPatches, the mutation will fail)
  """
  statusKeyList: [KeyString!]
}

input StatusBucketItemInputForBucket {
  """Must be unique to identify a patch"""
  statusBucketKey: KeyString!
  label: String!
  description: String!
}

input StatusBucketItemInputForStatuses {
  """Must be unique to identify a patch"""
  statusBucketKey: KeyString!
  """
  The list of StatusKeys that should be mapped to this StatusBucket
  If the same StatusKey appears in two (or more) StatusBuckets, the mutation will fail.
  This field can be included in all StatusBucketItemPatches, or omitted from all StatusBucketItemPatches
  """
  statusKeyList: [KeyString!]!
}

type StatusBucketNumberDeltaSummary @notNormalised {
  """identification of the summary (breakdown's bucket key, or _general_)"""
  key: String!
  """optional label to show in the UI for this Summary bucket (blank for _general_)"""
  label: String!
  """delta value of the summary"""
  value: DeltaValue!
  """position of the status bucket (for ordering)"""
  position: Int!
  """number of items that this point represents"""
  countItems: DeltaValue!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

type StatusBucketPercentageSummary @notNormalised {
  """identification of the summary (breakdown's bucket key, or _global_)"""
  key: String!
  """optional label to show in the UI for this Summary bucket (blank for _global_)"""
  label: String!
  """files value of the summary, which depends on which Metric do we have"""
  value: FloatRounded2!
  """files value, with the detail (numerator, denominator, percentage)"""
  valueDetail: PercentageDetail!
  """position of the status bucket (for ordering)"""
  position: Int!
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

"""like a ScatterTimeSeries but instead of ticket details it has CommitPercentage over time"""
type StatusBucketPercentageTimeSeries implements TimeSeries @notNormalised {
  """identification of the Time Series"""
  key: String!
  """optional label to show in the UI for this Time Series (it should override any regular translation-based label)"""
  label: String!
  """sorted list of points"""
  data: [PercentageTimePoint!]!
  """position of the status bucket (for ordering)"""
  position: Int!
}

type StatusBucketStats {
  """clientKey"""
  id: ID!
  totalStatuses: Int!
  unmappedStatuses: Int!
}

type StatusBucketSummary @notNormalised {
  """identification of the summary (breakdown's bucket key, or _general_)"""
  key: String!
  """label to show in the UI for this Summary bucket (blank for _general_)"""
  label: String!
  """value of the summary (average -> numerator / countItems)"""
  value: Float!
  """value taken into account (numerator to calculate the average)"""
  numerator: Float!
  """optionally return the ids of the items of countItems, useful for the Detail."""
  itemIDs: [KeyString!]
  """position of the status bucket (for ordering)"""
  position: Int!
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

type StatusBucketTimeSeries implements TimeSeries @notNormalised {
  """identification of the Time Series"""
  key: String!
  """optional label to show in the UI for this Time Series (it should override any regular translation-based label)"""
  label: String!
  """position of the status bucket (for ordering)"""
  position: Int!
  """sorted list of points"""
  data: [AverageNumberTimePoint!]!
}

type StatusBucketVariant {
  id: ID!
  clientKey: KeyString!
  boardKey: KeyString!
  statusBucketKey: KeyString!
  statuses: [Status!]!
  label: String!
  description: String!
  position: Int!
}

type StatusBucketVariantStats {
  """clientKey---boardKey"""
  id: ID!
  boardKey: KeyString!
  board: Board!
  """only the statuses that are used in that board"""
  totalStatuses: Int!
  """amount of statuses from totalStatuses that are not mapped in any bucket"""
  unmappedStatuses: Int!
}

type StatusBucketVariantStatus {
  id: ID!
  clientKey: KeyString!
  boardKey: KeyString!
  board: Board!
  statuses: [Status!]!
  created: Boolean!
  shallowCopyFromGlobal: Boolean!
  variants: [StatusBucketVariant!]!
}

type StatusBucketsForStatusesResponse {
  id: ID!
  statusBuckets: [StatusBucket!]!
  unmappedStatuses: [Status!]!
}

type StatusBucketsVariantResponse {
  id: ID!
  statusBucketsVariant: StatusBucketVariantStatus!
  unmappedStatuses: [Status!]!
  board: Board!
}

type StatusDeltaSummary @notNormalised {
  """identification of the summary (breakdown's bucket key, or _general_)"""
  key: KeyString!
  """optional label to show in the UI for this Summary bucket (blank for _general_)"""
  label: String!
  """status that this bucket represents"""
  status: Status!
  """delta value of the summary"""
  value: DeltaValue!
  """delta of numerator of the summary"""
  numerator: DeltaValue!
  """number of items that this point represents"""
  countItems: DeltaValue!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

type StatusSummary @notNormalised {
  """identification of the summary (breakdown's bucket key, or _general_)"""
  key: KeyString!
  """label to show in the UI for this Summary bucket (blank for _general_)"""
  label: String!
  """status that this bucket represents"""
  status: Status!
  """value of the summary"""
  value: FloatRounded1!
  """value taken into account (numerator to calculate the average)"""
  numerator: FloatRounded1!
  """optionally return the ids of the items of countItems, useful for the Detail."""
  itemIDs: [KeyString!]
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

"""represents a single point of a TimeSeries with a single numeric value"""
type StatusTimePoint implements TimePoint @notNormalised {
  """date-time of the point"""
  x: JSDate!
  """date-time of the initial moment of the time range represented by this point"""
  fromDate: UTCStartOfDay!
  """date-time of the final moment of the time range represented by this point"""
  toDate: UTCEndOfDay!
  """numeric value of the point (average)"""
  y: FloatRounded1!
  """value taken into account (numerator to calculate the average)"""
  numerator: FloatRounded1!
  """optionally return the ids of the items of countItems, useful for the Detail."""
  itemIDs: [KeyString!]
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

type StatusTimeSeries implements TimeSeries @notNormalised {
  """identification of the Time Series"""
  key: String!
  """label to show in the UI for this Time Series (it should override any regular translation-based label)"""
  label: String!
  """status that this time series references"""
  status: Status!
  """sorted list of points"""
  data: [StatusTimePoint!]!
}

enum StatusType {
  ORIGINAL_STATUS
  PLANDEK_STATUS
}

type Team {
  id: ID!
  clientKey: KeyString!
  teamKey: KeyString!
  name: String!
  peopleIds: [ID!]!
  people: [Person!]!
}

type TextCard {
  """cardListId--cardKey"""
  id: ID!
  cardListId: KeyString!
  cardKey: KeyString!
  """numeric position in the card list"""
  position: Int!
  """string to show as the card label/title"""
  label: String!
  textBody: String!
}

enum TicketCommitHotspotsMetric {
  TICKET_COMMIT_HOTSPOTS
}

input TicketCommitHotspotsMetricDetailInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: TicketCommitHotspotsMetric!
}

input TicketCommitHotspotsMetricFacetsInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: TicketCommitHotspotsMetric!
}

input TicketCommitHotspotsMetricInput {
  metricKey: TicketCommitHotspotsMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
}

type TicketCommitHotspotsMetricInputType @notNormalised {
  metricKey: TicketCommitHotspotsMetric!
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
}

type TicketCommitHotspotsMetricResponse {
  id: ID!
  input: TicketCommitHotspotsMetricInputType!
  metricKey: TicketCommitHotspotsMetric!
  """filter config to be used as breakdownConfig when calling for a details endpoint for a given summary"""
  filterConfigForFileDetails: FilterConfig!
  """filter config to be used as breakdownConfig when calling for a details endpoint for a given folderSummary"""
  filterConfigForFolderDetails: FilterConfig!
  """filter config to be used as breakdownConfig when calling for a details endpoint for a given repoSummary"""
  filterConfigForRepoDetails: FilterConfig!
  """
  list of summaries that describes the direct children of the filtered level: either repos or folders + files
  - key =  repo: repoKey; folder or file: path from repo root
  - label = repo: repoLabel; folder or file: filename
  - value = number of tickets
  """
  singleLevelSummariesList: [HotspotSummary!]!
  """
  list of summaries -> one per file
   - key =  file path (with repo prefix)
   - label = file path (with repo prefix)
   - value = number of tickets
  """
  fileSummariesList: [HotspotSummary!]!
  """
  list of summaries -> one per folder
  - key =  folder path (from repo root)
  - label = folder path (from repo root)
  - value = number of tickets
  """
  folderSummariesList: [HotspotSummary!]!
  """
  list of summaries -> one per repo
  - key =  repoKey
  - label = repoLabel
  - value = number of tickets
  """
  repoSummariesList: [HotspotSummary!]!
}

type TicketDetail {
  """clientKey--ticketKey--valueConfigKey"""
  id: ID!
  """determines the field used to calculate `value`"""
  valueConfigKey: KeyString!
  """value to be used. By default Story Points"""
  value: Float
  hitID: KeyString!
  clientKey: KeyString!
  ticketKey: KeyString!
  boardKeys: [KeyString!]!
  storyPoints: Float
  issueType: String
  summary: String
  status: String
  plandekStatus: String!
  filesChanged: Int
  insertedLines: Int
  deletedLines: Int
  timeToValueDays: Float
  extraFields: [TicketField!]!
  completedAt: UTCDate
  completedByPersonId: KeyString
  completedBy: Person
  createdAt: UTCDate
  createdByPersonId: KeyString
  createdBy: Person
  committerPersonIds: [KeyString!]!
  committers: [Person!]
  committersCount: Int!
  commits: [String!]!
  commitsCount: Int!
  plandekPriorityNumber: Int
  link: String!
  """number of seconds the ticket stays unresolved / incomplete"""
  unresolvedDurationSeconds: Int!
}

type TicketDetailList {
  id: ID!
  """total quantity of details"""
  count: Int!
  """list of ticket details, all sharing the same valueConfig to calculate the `value` field"""
  details: [TicketDetail!]!
  """key of the valueConfig"""
  valueConfigKey: KeyString!
  """valueConfig used for the `value` field of each detail"""
  valueConfig: ValueConfig!
}

"""numeric values of a TicketDetail, which can be used as `y` to plot a scatter chart"""
enum TicketDetailValueField {
  storyPoints
  insertedLines
  deletedLines
  filesChanged
  timeToValueDays
  committersCount
  commitsCount
  plandekPriorityNumber
}

type TicketDetailWithSprints @notNormalised {
  ticketDetail: TicketDetail!
  sprints: [SprintKeyLabel!]!
}

type TicketDetailWithSprintsList {
  id: ID!
  """total quantity of details"""
  count: Int!
  """list of ticket details, all sharing the same valueConfig to calculate the `value` field"""
  details: [TicketDetailWithSprints!]!
  """key of the valueConfig"""
  valueConfigKey: KeyString!
  """valueConfig used for the `value` field of each detail"""
  valueConfig: ValueConfig!
}

type TicketDetailWithTransitionEvents {
  id: ID!
  ticketDetail: TicketDetail!
  transitionEvents: [TransitionEvent!]!
}

type TicketField @notNormalised {
  key: KeyString!
  value: String
}

"""This is exacly like ScatterTimePoint but instead we need items as we will change it to single items later"""
type TicketNumberTimePointForReturns implements TimePoint @notNormalised {
  """date-time of the point"""
  x: JSDate!
  """date-time of the initial moment of the time range represented by this point"""
  fromDate: UTCStartOfDay!
  """date-time of the final moment of the time range represented by this point"""
  toDate: UTCEndOfDay!
  """numeric value of the point"""
  y: Float!
  """number of items that this point represents (tickets or commits)"""
  size: Int!
  """ticket details associated with this time point"""
  items: [TicketDetail!]
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

type TicketNumberTimeSeriesForReturns implements TimeSeries @notNormalised {
  """identification of the Time Series"""
  key: String!
  """optional label to show in the UI for this Time Series (it should override any regular translation-based label)"""
  label: String!
  """sorted list of points"""
  data: [TicketNumberTimePointForReturns!]!
}

type TicketReturnSettings @notNormalised {
  """
  stored settings in the metric card for Returns metrics ->
  - list of transition keys considered return transitions
  - list of issue types that we considered a defect
  """
  returnTransitionKeys: [KeyString!]!
  defectIssueTypes: [KeyString!]!
}

input TicketReturnSettingsInput {
  """stored settings in the metric card for Returns metrics -> list of transition keys considered return transitions"""
  returnTransitionKeys: [KeyString!]!
  defectIssueTypes: [KeyString!]!
}

"""
represents a single point of a TimeSeries with a list of tickets for each x + a single numeric value

It can be used for a scatter chart, when the tickets are divided and each point represents `x` and `y` and the tickets that share the same x and y.
"""
type TicketWithKeysNumberTimePoint implements TimePoint @notNormalised {
  """date-time of the point"""
  x: JSDate!
  """date-time of the initial moment of the time range represented by this point"""
  fromDate: UTCStartOfDay!
  """date-time of the final moment of the time range represented by this point"""
  toDate: UTCEndOfDay!
  """numeric value of the point"""
  y: Float!
  """number of items that this point represents (tickets or commits)"""
  size: Int!
  """list of ticket keys that this point includes"""
  ticketKeys: [KeyString!]!
  """info about the single item if this point represents a single item (only 1 ticket). If size > 1 singleItem will be blank."""
  singleItem: ItemWithLink
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

type TicketWithKeysNumberTimeSeries implements TimeSeries @notNormalised {
  """identification of the Time Series"""
  key: String!
  """optional label to show in the UI for this Time Series (it should override any regular translation-based label)"""
  label: String!
  """sorted list of points"""
  data: [TicketWithKeysNumberTimePoint!]!
}

"""list of available ticket metric with a single value"""
enum TicketsMetric {
  """
  completed unreleased tickets (combined) -> value = depends on valueConfig (default ticketCount)
  
  - alphanumeric valueConfig => count distinct
  - numeric valueConfig => sum
  """
  COMPLETED_UNRELEASED_TICKETS
  """completed unreleased tickets -> value = count of tickets"""
  COMPLETED_UNRELEASED_TICKETS_COUNT
  """completed unreleased tickets -> value = sum of ticket's story points"""
  COMPLETED_UNRELEASED_TICKETS_STORY_POINTS
  """
  completed tickets (combined) -> value = depends on valueConfig (default ticketCount)
  
  - alphanumeric valueConfig => count distinct
  - numeric valueConfig => sum
  """
  COMPLETED_TICKETS
  """completed tickets -> value = count of tickets"""
  COMPLETED_TICKETS_COUNT
  """completed tickets -> value = sum of ticket's story points"""
  COMPLETED_TICKETS_STORY_POINTS
  """
  created tickets (combined) -> value = depends on valueConfig (default ticketCount)
  
  - alphanumeric valueConfig => count distinct
  - numeric valueConfig => sum
  """
  CREATED_TICKETS
  """created tickets -> value = count of tickets"""
  CREATED_TICKETS_COUNT
  """created tickets -> value = sum of ticket's story points"""
  CREATED_TICKETS_STORY_POINTS
  """time to value days -> value = time between ticket creation and deployment in days"""
  TIME_TO_VALUE_DAYS
  """complexity of a ticket based on its commits"""
  TICKET_COMPLEXITY
}

input TicketsMetricDetailInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: TicketsMetric!
  valueConfigKey: KeyString
}

input TicketsMetricFacetsInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: TicketsMetric!
}

input TicketsMetricInput {
  metricKey: TicketsMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  """granularity for the time series (default to `day` if not specified)"""
  granularity: Granularity
  valueConfigKey: KeyString
  """
  can be null, to ask for the general result.
  
  If it is not null, it has to be one of the keys of the valid FilterConfig (see FilterConfig)
  
  note that this cannot be an enum because it will depend on the data (custom fields et al)
  """
  breakdown: KeyString
}

type TicketsMetricInputType @notNormalised {
  metricKey: TicketsMetric!
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  valueConfigKey: KeyString
  breakdown: KeyString
}

type TicketsMetricResponse {
  id: ID!
  input: TicketsMetricInputType!
  metricKey: TicketsMetric!
  valueConfig: ValueConfig
  """summary with breakdown blank (key = _general_)"""
  generalSummary: NumberSummary
  """delta of the generalSummary"""
  generalDelta: NumberDeltaSummary
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one summary per breakdown bucket
  """
  summariesList: [NumberSummary!]!
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one delta summary per breakdown bucket
  if there are no deltas configured, it will be NULL
  """
  deltasList: [NumberDeltaSummary!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value, depending on `metric`
  """
  timeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates. The value of each point will be the cumulative average.
  
  e.g.
  
  ```
  TS0---TS1---TS2---TS3---TS4
  
  ATS0 = TS0
  ATS1 = (TS1 + TS0) / 2
  ATS2 = (TS2 + TS1 + TS0) / 3
  ATS3 = (TS3 + TS2 + TS1 + TS0) / 4
  ATS4 = (TS4 + TS3 + TS2 + TS1 + TS0) / 5
  ```
  """
  averageTimeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates.
  The value of each point will be the cumulative average taking into account only the current value + the previous `n` values (where n is the windowSize)
  It will load the TS of the past filters (same filters used for delta calculation) to get the "previous values" of the initial points
  
  e.g time series with 5 points (TS0 to TS4), windowSize = 2
  
  ```
  PTS0---PTS1---PTS2---PTS3---PTS4---TS0---TS1---TS2---TS3---TS4
  
  RATS0 = (TS0 + PTS4 + PTS3) / 3
  RATS1 = (TS1 + TS0 + PTS4) / 3
  RATS2 = (TS2 + TS1 + TS0) / 3
  RATS3 = (TS3 + TS2 + TS1) / 3
  RATS4 = (TS4 + TS3 + TS2) / 3
  ```
  """
  rollingAverageTimeSeriesList(windowSize: Int, windowSizePercentage: Float): [NumberTimeSeriesWithWindowSize!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value taken from each TicketDetail
  The `size` will be the number of tickets that the point represents
  The `singleItem` is populated only when size is 1. It contains key, label, link
  """
  scatterTimeSeriesList: [ScatterTimeSeries!]!
}

"""list of available ticket metric with a single value"""
enum TicketsScopeMetric {
  """ticket timeline -> value = count of tickets existing"""
  TICKET_SCOPE
  """ticket timeline -> value = sum of story points of the tickets existing"""
  TICKET_SCOPE_STORY_POINTS
}

input TicketsScopeMetricDetailInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: TicketsScopeMetric!
  ticketsScopeType: TicketsScopeType!
}

input TicketsScopeMetricFacetsInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: TicketsScopeMetric!
}

input TicketsScopeMetricInput {
  metricKey: TicketsScopeMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  """granularity for the time series (default to `day` if not specified)"""
  granularity: Granularity
  """breakdown, taken from the input (see ticketsTimelineMetricInput)"""
  breakdown: KeyString
}

type TicketsScopeMetricInputType @notNormalised {
  metricKey: TicketsScopeMetric!
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  breakdown: KeyString
}

type TicketsScopeMetricPastResponse {
  id: ID!
  filters: Filters!
  scopeTimeSeriesList: [TicketsScopeNumberTimeSeries!]!
  addedTimeSeriesList: [TicketsScopeNumberTimeSeries!]!
  removedTimeSeriesList: [TicketsScopeNumberTimeSeries!]!
  summariesList: [TicketsScopeNumberSummary!]!
}

type TicketsScopeMetricResponse {
  id: ID!
  input: TicketsScopeMetricInputType!
  metricKey: TicketsScopeMetric!
  """summary with breakdown blank (key = _general_)"""
  generalSummary: TicketsScopeNumberSummary
  """delta of the generalSummary"""
  generalDelta: TicketsScopeNumberDeltaSummary
  summariesList: [TicketsScopeNumberSummary!]!
  """changes since the previous period"""
  deltasList: [TicketsScopeNumberDeltaSummary!]!
  """AUX to be used internally"""
  pastResponse: TicketsScopeMetricPastResponse
  """Count of tickets / sum of story points in sprint at each time point (depending on which metric)"""
  scopeTimeSeriesList: [TicketsScopeNumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates. The value of each point will be the cumulative average.
  
  e.g.
  
  ```
  TS0---TS1---TS2---TS3---TS4
  
  ATS0 = TS0
  ATS1 = (TS1 + TS0) / 2
  ATS2 = (TS2 + TS1 + TS0) / 3
  ATS3 = (TS3 + TS2 + TS1 + TS0) / 4
  ATS4 = (TS4 + TS3 + TS2 + TS1 + TS0) / 5
  ```
  """
  scopeAverageTimeSeriesList: [TicketsScopeNumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates.
  The value of each point will be the cumulative average taking into account only the current value + the previous `n` values (where n is the windowSize)
  It will load the TS of the past filters (same filters used for delta calculation) to get the "previous values" of the initial points
  
  e.g time series with 5 points (TS0 to TS4), windowSize = 2
  
  ```
  PTS0---PTS1---PTS2---PTS3---PTS4---TS0---TS1---TS2---TS3---TS4
  
  RATS0 = (TS0 + PTS4 + PTS3) / 3
  RATS1 = (TS1 + TS0 + PTS4) / 3
  RATS2 = (TS2 + TS1 + TS0) / 3
  RATS3 = (TS3 + TS2 + TS1) / 3
  RATS4 = (TS4 + TS3 + TS2) / 3
  ```
  """
  scopeRollingAverageTimeSeriesList(windowSize: Int, windowSizePercentage: Float): [TicketsScopeNumberTimeSeriesWithWindowSize!]!
  """
  if NOT filtered by sprint => this is created tickets
  if filtered by sprint => this is ??? (created OR added to the sprint)
  """
  addedTimeSeriesList: [TicketsScopeNumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates. The value of each point will be the cumulative average.
  
  e.g.
  
  ```
  TS0---TS1---TS2---TS3---TS4
  
  ATS0 = TS0
  ATS1 = (TS1 + TS0) / 2
  ATS2 = (TS2 + TS1 + TS0) / 3
  ATS3 = (TS3 + TS2 + TS1 + TS0) / 4
  ATS4 = (TS4 + TS3 + TS2 + TS1 + TS0) / 5
  ```
  """
  addedAverageTimeSeriesList: [TicketsScopeNumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates.
  The value of each point will be the cumulative average taking into account only the current value + the previous `n` values (where n is the windowSize)
  It will load the TS of the past filters (same filters used for delta calculation) to get the "previous values" of the initial points
  
  e.g time series with 5 points (TS0 to TS4), windowSize = 2
  
  ```
  PTS0---PTS1---PTS2---PTS3---PTS4---TS0---TS1---TS2---TS3---TS4
  
  RATS0 = (TS0 + PTS4 + PTS3) / 3
  RATS1 = (TS1 + TS0 + PTS4) / 3
  RATS2 = (TS2 + TS1 + TS0) / 3
  RATS3 = (TS3 + TS2 + TS1) / 3
  RATS4 = (TS4 + TS3 + TS2) / 3
  ```
  """
  addedRollingAverageTimeSeriesList(windowSize: Int, windowSizePercentage: Float): [TicketsScopeNumberTimeSeriesWithWindowSize!]!
  """
  if NOT filtered by sprint => this is tickets with plandek_resolution_time (i.e. max(competed_time, closed_incomplete_time))
  if filtered by sprint => this is ??? (completed OR taken out of the sprint)
  """
  removedTimeSeriesList: [TicketsScopeNumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates. The value of each point will be the cumulative average.
  
  e.g.
  
  ```
  TS0---TS1---TS2---TS3---TS4
  
  ATS0 = TS0
  ATS1 = (TS1 + TS0) / 2
  ATS2 = (TS2 + TS1 + TS0) / 3
  ATS3 = (TS3 + TS2 + TS1 + TS0) / 4
  ATS4 = (TS4 + TS3 + TS2 + TS1 + TS0) / 5
  ```
  """
  removedAverageTimeSeriesList: [TicketsScopeNumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates.
  The value of each point will be the cumulative average taking into account only the current value + the previous `n` values (where n is the windowSize)
  It will load the TS of the past filters (same filters used for delta calculation) to get the "previous values" of the initial points
  
  e.g time series with 5 points (TS0 to TS4), windowSize = 2
  
  ```
  PTS0---PTS1---PTS2---PTS3---PTS4---TS0---TS1---TS2---TS3---TS4
  
  RATS0 = (TS0 + PTS4 + PTS3) / 3
  RATS1 = (TS1 + TS0 + PTS4) / 3
  RATS2 = (TS2 + TS1 + TS0) / 3
  RATS3 = (TS3 + TS2 + TS1) / 3
  RATS4 = (TS4 + TS3 + TS2) / 3
  ```
  """
  removedRollingAverageTimeSeriesList(windowSize: Int, windowSizePercentage: Float): [TicketsScopeNumberTimeSeriesWithWindowSize!]!
}

type TicketsScopeNumberDeltaSummary @notNormalised {
  """determines which TicketsScopeType to use for details"""
  ticketsScopeType: TicketsScopeType!
  """identification of the summary (breakdown's bucket key, or _general_)"""
  key: String!
  """optional label to show in the UI for this Summary bucket (blank for _general_)"""
  label: String!
  """delta value of the summary"""
  value: DeltaValue!
  """number of items that this point represents"""
  countItems: DeltaValue!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query) for present"""
  overrideFilters: OverrideFilters!
}

type TicketsScopeNumberSummary @notNormalised {
  """determines which TicketsScopeType to use for details"""
  ticketsScopeType: TicketsScopeType!
  """identification of the summary (breakdown's bucket key, or _general_)"""
  key: String!
  """label to show in the UI for this Summary bucket (blank for _general_)"""
  label: String!
  """value of the summary"""
  value: Float!
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

"""represents a single point of a TimeSeries with a single numeric value"""
type TicketsScopeNumberTimePoint implements TimePoint @notNormalised {
  """determines which TicketsScopeType to use for details"""
  ticketsScopeType: TicketsScopeType!
  """date-time of the point"""
  x: JSDate!
  """date-time of the initial moment of the time range represented by this point"""
  fromDate: UTCStartOfDay!
  """date-time of the final moment of the time range represented by this point"""
  toDate: UTCEndOfDay!
  """numeric value of the point"""
  y: Float!
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

type TicketsScopeNumberTimeSeries implements TimeSeries @notNormalised {
  """determines which TicketsScopeType to use for details"""
  ticketsScopeType: TicketsScopeType!
  """identification of the Time Series"""
  key: String!
  """optional label to show in the UI for this Time Series (it should override any regular translation-based label)"""
  label: String!
  """sorted list of points"""
  data: [TicketsScopeNumberTimePoint!]!
}

type TicketsScopeNumberTimeSeriesWithWindowSize implements TimeSeries @notNormalised {
  """identification of the Time Series"""
  key: String!
  """optional label to show in the UI for this Time Series (it should override any regular translation-based label)"""
  label: String!
  """sorted list of points"""
  data: [TicketsScopeNumberTimePoint!]!
  """size of the window used for calculating the points' value"""
  windowSize: Int!
}

enum TicketsScopeType {
  SCOPE
  ADDED
  REMOVED
}

"""list of available ticket metric with a single value"""
enum TicketsTimelineMetric {
  """ticket timeline -> value = count of tickets existing"""
  TICKET_TIMELINE
  """ticket timeline -> value = sum of story points of tickets existing"""
  TICKET_TIMELINE_STORY_POINTS
}

input TicketsTimelineMetricDetailInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: TicketsTimelineMetric!
  ignoreFromDate: Boolean
}

input TicketsTimelineMetricFacetsInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: TicketsTimelineMetric!
  ignoreFromDate: Boolean
}

input TicketsTimelineMetricInput {
  metricKey: TicketsTimelineMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  """granularity for the time series (default to `day` if not specified)"""
  granularity: Granularity
}

type TicketsTimelineMetricInputType @notNormalised {
  metricKey: TicketsTimelineMetric!
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
}

type TicketsTimelineMetricStatusBucketResponse {
  id: ID!
  input: TicketsTimelineMetricInputType!
  metricKey: TicketsTimelineMetric!
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one summary per breakdown bucket
  """
  summariesList: [StatusBucketSummary!]!
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one delta summary per breakdown bucket
  if there are no deltas configured, it will be NULL
  """
  deltasList: [NumberStatusBucketDeltaSummary!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value, depending on `metric`
  """
  timeSeriesList: [NumberStatusBucketTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates. The value of each point will be the cumulative average.
  
  e.g.
  
  ```
  TS0---TS1---TS2---TS3---TS4
  
  ATS0 = TS0
  ATS1 = (TS1 + TS0) / 2
  ATS2 = (TS2 + TS1 + TS0) / 3
  ATS3 = (TS3 + TS2 + TS1 + TS0) / 4
  ATS4 = (TS4 + TS3 + TS2 + TS1 + TS0) / 5
  ```
  """
  averageTimeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates.
  The value of each point will be the cumulative average taking into account only the current value + the previous `n` values (where n is the windowSize)
  It will load the TS of the past filters (same filters used for delta calculation) to get the "previous values" of the initial points
  
  e.g time series with 5 points (TS0 to TS4), windowSize = 2
  
  ```
  PTS0---PTS1---PTS2---PTS3---PTS4---TS0---TS1---TS2---TS3---TS4
  
  RATS0 = (TS0 + PTS4 + PTS3) / 3
  RATS1 = (TS1 + TS0 + PTS4) / 3
  RATS2 = (TS2 + TS1 + TS0) / 3
  RATS3 = (TS3 + TS2 + TS1) / 3
  RATS4 = (TS4 + TS3 + TS2) / 3
  ```
  """
  rollingAverageTimeSeriesList(windowSize: Int, windowSizePercentage: Float): [NumberTimeSeriesWithWindowSize!]!
}

type TicketsTimelineMetricStatusResponse {
  id: ID!
  input: TicketsTimelineMetricInputType!
  metricKey: TicketsTimelineMetric!
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one summary per breakdown bucket
  """
  summariesList: [NumberStatusSummary!]!
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one delta summary per breakdown bucket
  if there are no deltas configured, it will be NULL
  """
  deltasList: [NumberStatusDeltaSummary!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value, depending on `metric`
  """
  timeSeriesList: [NumberStatusTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates. The value of each point will be the cumulative average.
  
  e.g.
  
  ```
  TS0---TS1---TS2---TS3---TS4
  
  ATS0 = TS0
  ATS1 = (TS1 + TS0) / 2
  ATS2 = (TS2 + TS1 + TS0) / 3
  ATS3 = (TS3 + TS2 + TS1 + TS0) / 4
  ATS4 = (TS4 + TS3 + TS2 + TS1 + TS0) / 5
  ```
  """
  averageTimeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates.
  The value of each point will be the cumulative average taking into account only the current value + the previous `n` values (where n is the windowSize)
  It will load the TS of the past filters (same filters used for delta calculation) to get the "previous values" of the initial points
  
  e.g time series with 5 points (TS0 to TS4), windowSize = 2
  
  ```
  PTS0---PTS1---PTS2---PTS3---PTS4---TS0---TS1---TS2---TS3---TS4
  
  RATS0 = (TS0 + PTS4 + PTS3) / 3
  RATS1 = (TS1 + TS0 + PTS4) / 3
  RATS2 = (TS2 + TS1 + TS0) / 3
  RATS3 = (TS3 + TS2 + TS1) / 3
  RATS4 = (TS4 + TS3 + TS2) / 3
  ```
  """
  rollingAverageTimeSeriesList(windowSize: Int, windowSizePercentage: Float): [NumberTimeSeriesWithWindowSize!]!
}

interface TimePoint {
  """date-time of the point"""
  x: JSDate!
  """date-time of the initial moment of the time range represented by this point"""
  fromDate: UTCStartOfDay!
  """date-time of the final moment of the time range represented by this point"""
  toDate: UTCEndOfDay!
  """number of items that this point represents"""
  countItems: Float!
  """Override Filters to be applied on any subsequent query to get the data of this point (e.g. for a Details query)"""
  overrideFilters: OverrideFilters!
}

"""represents a sorted list of points, identified with a key and a label"""
interface TimeSeries {
  """identification of the Time Series"""
  key: String!
  """optional label to show in the UI for this Time Series (it should override any regular translation-based label)"""
  label: String!
  """sorted list of points"""
  data: [TimePoint!]!
}

"""object that represents a transition from one status to the next"""
type Transition {
  id: ID!
  transitionKey: KeyString!
  label: String!
  plandekStatus: Status!
  status: Status!
  nextStatus: Status
}

type TransitionEvent {
  id: ID!
  transition: Transition!
  time: UTCDate!
  durationSeconds: FloatRounded1!
  durationDays: FloatRounded2!
}

type TransitionEventWithTicket {
  id: ID!
  ticketDetail: TicketDetail!
  transitionEvent: TransitionEvent!
}

type Treemap @notNormalised {
  """unique key"""
  key: KeyString!
  """Display label"""
  label: String!
  """list containing the total for each variable in data"""
  totals: [TreemapValue!]!
  """list containing the data for each breakdown"""
  data: [TreemapData!]!
}

type TreemapData @notNormalised {
  """Display name"""
  key: String!
  """Label"""
  label: String!
  """List of values for this key"""
  values: [TreemapValue!]!
}

input TreemapInput {
  metricKey: TreemapMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
}

type TreemapInputType @notNormalised {
  metricKey: TreemapMetric!
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
}

enum TreemapMetric {
  """Files most frequently associated with commits from failed builds"""
  TREEMAP_FLAKIEST_FILES
}

type TreemapResponse {
  id: ID!
  metricKey: TreemapMetric!
  input: TreemapInputType!
  """response data"""
  treemapSeriesList: [Treemap!]!
}

type TreemapValue @notNormalised {
  """Key"""
  key: String!
  """Label"""
  label: String!
  """Value"""
  value: Float!
}

"""Date in UTC"""
scalar UTCDate

"""Date in UTC that when parsing, if it does not have time already, it will be parsed at end of UTC day"""
scalar UTCEndOfDay

"""Date in UTC that when parsing, if it does not have time already, it will be parsed at start of UTC day"""
scalar UTCStartOfDay

"""list of available ticket metric with a single value"""
enum UnresolvedTicketsMetric {
  """
  unresolved tickets -> (combined) -> value = depends on valueConfig (default ticketCount)
  
  - alphanumeric valueConfig => count distinct
  - numeric valueConfig => sum
  """
  UNRESOLVED_TICKETS
  """unresolved tickets -> value = count of tickets"""
  UNRESOLVED_TICKETS_COUNT
  """unresolved tickets -> value = sum of ticket's story points"""
  UNRESOLVED_TICKETS_STORY_POINTS
}

input UnresolvedTicketsMetricDetailInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: UnresolvedTicketsMetric!
  valueConfigKey: KeyString
}

input UnresolvedTicketsMetricFacetsInput {
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  metricKey: UnresolvedTicketsMetric!
}

input UnresolvedTicketsMetricInput {
  metricKey: UnresolvedTicketsMetric!
  filters: FiltersInput!
  overrideFilters: OverrideFiltersInput!
  sprintDateRangeFieldOverride: SprintDateRangeField
  """granularity for the time series (default to `day` if not specified)"""
  granularity: Granularity
  """
  can be null, to ask for the general result.
  
  If it is not null, it has to be one of the keys of the valid FilterConfig (see FilterConfig)
  
  note that this cannot be an enum because it will depend on the data (custom fields et al)
  """
  breakdown: KeyString
  """valueConfigKey from the input"""
  valueConfigKey: KeyString
}

type UnresolvedTicketsMetricInputType @notNormalised {
  metricKey: UnresolvedTicketsMetric!
  filters: Filters!
  overrideFilters: OverrideFilters!
  sprintDateRangeFieldOverride: SprintDateRangeField
  granularity: Granularity
  breakdown: KeyString
  valueConfigKey: KeyString
}

type UnresolvedTicketsMetricResponse {
  id: ID!
  input: UnresolvedTicketsMetricInputType!
  metricKey: UnresolvedTicketsMetric!
  valueConfig: ValueConfig!
  """summary with breakdown blank (key = _general_)"""
  generalSummary: NumberSummary
  """delta of the generalSummary"""
  generalDelta: NumberDeltaSummary
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one summary per breakdown bucket
  """
  summariesList: [NumberSummary!]!
  """
  if breakdown is blank, it will have a single element with the general summary (key = _general_)
  otherwise, it will have one delta summary per breakdown bucket
  if there are no deltas configured, it will be NULL
  """
  deltasList: [NumberDeltaSummary!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  The `x` will be a date, depending on `granularity`.
  The `fromDate` and `toDate` are dates that mark the range of time that this point represents
  The `y` will be the value, depending on `metric`
  """
  timeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates. The value of each point will be the cumulative average.
  
  e.g.
  
  ```
  TS0---TS1---TS2---TS3---TS4
  
  ATS0 = TS0
  ATS1 = (TS1 + TS0) / 2
  ATS2 = (TS2 + TS1 + TS0) / 3
  ATS3 = (TS3 + TS2 + TS1 + TS0) / 4
  ATS4 = (TS4 + TS3 + TS2 + TS1 + TS0) / 5
  ```
  """
  averageTimeSeriesList: [NumberTimeSeries!]!
  """
  if breakdown is blank, it will have a single element with the general timeSeries (key = _general_)
  otherwise, it will have one time series per breakdown bucket
  
  returns one TS per TS in the timeSeriesList. Same key and label, and points with the same dates.
  The value of each point will be the cumulative average taking into account only the current value + the previous `n` values (where n is the windowSize)
  It will load the TS of the past filters (same filters used for delta calculation) to get the "previous values" of the initial points
  
  e.g time series with 5 points (TS0 to TS4), windowSize = 2
  
  ```
  PTS0---PTS1---PTS2---PTS3---PTS4---TS0---TS1---TS2---TS3---TS4
  
  RATS0 = (TS0 + PTS4 + PTS3) / 3
  RATS1 = (TS1 + TS0 + PTS4) / 3
  RATS2 = (TS2 + TS1 + TS0) / 3
  RATS3 = (TS3 + TS2 + TS1) / 3
  RATS4 = (TS4 + TS3 + TS2) / 3
  ```
  """
  rollingAverageTimeSeriesList(windowSize: Int, windowSizePercentage: Float): [NumberTimeSeriesWithWindowSize!]!
}

input UpdateStatusBucketsInput {
  clientKey: KeyString!
  buckets: [StatusBucketItemInput!]!
}

input UpdateStatusBucketsInputForBuckets {
  clientKey: KeyString!
  buckets: [StatusBucketItemInputForBucket!]!
}

input UpdateStatusBucketsInputForStatuses {
  clientKey: KeyString!
  buckets: [StatusBucketItemInputForStatuses!]!
}

input UpsertStatusBucketsVariantInput {
  clientKey: KeyString!
  boardKey: KeyString!
  buckets: [StatusBucketItemInputForStatuses!]!
}

enum UserAuthType {
  AUTH
  M2M
  FAKE
}

type UserInfo {
  """ID of the user (Auth0)"""
  id: ID!
  """describes the method used for authentication"""
  authType: UserAuthType!
  """email that serves as identifier for this user (both Auth0 and PermissionSet)"""
  email: String!
  """list of claims that are allowed for this user in the form `read:something` or `admin:something.or.other` (in strings) (PermissionSet)"""
  permittedClaims: [String!]!
  """list of claims that are explicitly forbidden for this user in the form `read:something` or `admin:something.or.other` (in strings) (PermissionSet)"""
  prohibitedClaims: [String!]!
  """flag to mark the user as a plandek internal user (PermissionSet)"""
  internalUser: Boolean!
  """string to mark the user type for analytical purposes (PermissionSet)"""
  userType: String
  """string to mark the type of Auth0 that the user (Auth0)"""
  connectionType: String
  """flag to mark the user as internal or external (PermissionSet)"""
  elevioHash: String
  """name of the user (Auth0)"""
  name: String
  """nickname of the user (Auth0)"""
  nickname: String
  """url of the picture of the user (Auth0)"""
  picture: String
  """marks the time when the UserInfo was collected from Auth0"""
  time: String
  """date the user was created"""
  createdAt: UTCDate
  """json to be sent to LaunchDarkly as identifier of the user"""
  encodedLDUser: String!
  """json with the API toggles for the user"""
  encodedLDUserToggles: String!
  """secure hash to be used by LaunchDarkly with the user"""
  encodedLDUserHash: String
}

type UserProfile {
  """ID of the user (Auth0)"""
  id: ID!
  """email that serves as identifier for this user (both Auth0 and PermissionSet)"""
  email: String!
  """name of the user (Auth0)"""
  name: String
  """nickname of the user (Auth0)"""
  nickname: String
  """url of the picture of the user (Auth0)"""
  picture: String
}

type ValueConfig {
  """for general configs = key, for custom configs = `clientKey--key`"""
  id: ID!
  """ID of the valueConfig to use in the metric call"""
  key: KeyString!
  """how to call this thing in the UI"""
  label: String!
  """how to call this thing in the UI (each unit)"""
  unitSingular: String!
  """how to call this thing in the UI (each unit)"""
  unitPlural: String!
  """which kind of values do we have on this one (e.g. we can only do Math on NUMERIC stuff, but we can do cardinality on anything)"""
  valueConfigType: ValueConfigTypes!
  """has support for outliers. Note that for the outliers to be supported, it has to be supported by FeatureToggle + Metric + ValueConfig"""
  supportsOutliers: Boolean!
}

enum ValueConfigTypes {
  INVALID
  NUMERIC
  ALPHANUMERIC
}
